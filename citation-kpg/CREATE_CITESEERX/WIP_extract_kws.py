import re
import spacy
import nltk
from collections import defaultdict
from spacy import Language

@Language.component('merge_compounds')
def merge_compounds(d):
    """ Merge compounds to be one token

    A compound is two tokens separated by a hyphen when the tokens are right
    next to the hyphen

    d (spacy.Doc): Document
    Returns: spacy.Doc

    > [t.text for t in nlp('peer-to-peer-to-peer')]
    ['peer', '-', 'to', '-', 'peer', 'to', '-', 'peer']
    > [t.text for t in merge_compounds(nlp('peer-to-peer-to-peer'))]
    ['peer-to-peer-to-peer']
    """
    # Returns beginning and end offset of spacy.Token
    offsets = lambda t: (t.idx, t.idx + len(t))

    # Identify the hyphens : for each token is it a hyphen and the next and
    #  preceding token are right next to the hyphen
    spans = [(i - 1, i + 1) for i in range(len(d))
             if i != 0 and i < len(d) - 1 and d[i].text == '-'
                       and offsets(d[i - 1])[1] == offsets(d[i])[0]
                       and offsets(d[i + 1])[0] == offsets(d[i])[1]
            ]
    # merging spans to account for multi-compound terms
    merged = []
    for i, (b, e) in enumerate(spans):
        # if the last spans ends when the current span begins,
        # merge those
        if merged and b == merged[-1][1]:
            merged[-1] = (merged[-1][0], e)
        else:
            merged.append((b, e))

    # Merge the spacy Doc compute the span beforehand as merging changes
    #  the indexation
    to_merge = [d[b:e + 1] for b, e in merged]
    with d.retokenize() as retokenizer:
        for span in to_merge:
            # also computing the lemma (but it will be overwritten)
            retokenizer.merge(span, attrs={"LEMMA":''.join(t.lemma_ for t in span)})
    return d


def grammar_selection(doc, grammar):
    # initialize chunker
    chunker = nltk.RegexpParser(grammar)
    out = []
    # loop through the sentences
    for i, sentence in enumerate(doc.sents):
        # convert sentence as list of (offset, pos) tuples
        tuples = [(str(j), w.pos_) for j, w in enumerate(sentence)]
        # parse sentence
        tree = chunker.parse(tuples)
        # find candidates
        for subtree in tree.subtrees():
            if subtree.label() == 'NP':
                leaves = subtree.leaves()
                # get the first and last offset of the current candidate
                first = int(leaves[0][0])
                last = int(leaves[-1][0])
                out.append(sentence[first:last + 1])
    return out


et_al_p = re.compile(r'\bet al\.')
def pretreat_ctxt(ctxt):
    # Preprocess, remove markup and return sentence containing the citation markup.
    #ctxt = re.sub(r'\d+(\s*,\s*\d+)*', '<D>', ctxt[1])
    ctxt = et_al_p.sub(r'et al', ctxt)
    ctxt = ctxt.replace('[', ' [').replace(']', '] ')

    # Serching for first marker
    marker_offset = ctxt.find('=-=')
    if marker_offset < 0:
        #print(ctxt)
        #input()
        return None
    # Searching for second marker after first marker
    if ctxt[marker_offset:].find('-=-') < 0:
        #print(ctxt)
        #input()
        return None

    # removing markers
    ctxt = ctxt.replace('=-=', '').replace('-=-', '')

    return ctxt, marker_offset


txt = """een cells. Wei et al. [97] use a technique based on the CA model called a Lattice Boltzmann Model (LBM) to animate bubbles and feathers drifting in the air. A LBM approach is used again by Wei et al. =-=[96]-=- to simulate the microscopic behavior of fluid so that the averaged macroscopic movement obeys the Navier-Stokes equations. They also model the buoyancy forces of gas with a temperature field. Wrennin
dering systems have been introduced. In some cases emphasis was put on both of the stages; the simulation was based on the Navier-Stokes equations for incompressible flow [1], Lattice Boltzmann Model =-=[2]-=- or cellular automaton [3]. In other cases the first stage was omitted [4] and emphasis was put only on realistic cloud rendering. It is obvious that the last method is computationally least expensive
equations for an incompressible fluid. The Lattice Boltzmann model has been used quite frequently in computer graphics research. Wei et al. use it to simulate smoke, fire and other gaseous phenomena =-=[42, 43]-=-. Much of the recent interest is generated by the fact that the algorithm is well suited for acceleration in hardware on modern GPUs. 2.5 Programmable Graphics Hardware During the last couple of years
rule was selected from a lookuptable. Similarly, Miyazaki et al. [7] use a set of rules similar to cellular automata, where each cell contains floating values, to simulate cloud formation. Wei et al. =-=[8]-=- simulate the motion of particles in fluids applying a local set of rules over a grid of cells. In previous work [9], we created a system that specified behavior by using XML scripting and images. Whi
nd are computationally less intensive A principal contribution of this work is the vorticity confinement correction, which adds back fine-scale flow detail that is damped out by coarse-grid solvers. =-=[WLMK04]-=- use a lattice-Boltzmann model to simulate the dynamics of steam. They model air flow and then attach textures, constructed from photographs of steam, to particles in the flow. Attaching textures in t
e-dependent physics into account and share data together. Recently, LBM [36] has been introduced to the graphics community for modeling various flow phenomena including wind, smoke, fire, and melting =-=[38, 39, 41, 45]-=-. Although LBM is a relatively new CFD procedure, it has the advantages of being simple to implement, parallelizable, and can accommodate complex boundaries. It can also be extended to model thermal e
on a sparse matrix representation which they applied to the Navier Stokes equations. In our previous work, Li et al. [29] have accelerated single-relaxation-time LBM computation on GPU and Wei et al. =-=[38, 39, 41]-=- have used the accelerated LBM to model gaseous phenomena, such as fire, smoke and wind. (More examples can be found on the web site of GPGPU (generalpurpose computation using graphics hardware) [1]).
algorithm for participating media [22]. However, although ray tracing and photon mapping methods can render high-quality images, they are extremely slow, usually several minutes per frame. Wei et al. =-=[39]-=- have rendered smoke with textured splats, which was proposed by Crawfis and Max [11] and used by King et al. [23] to render fire. This method can render smoke in real-time but doesn’t incorporate glo
tribution of momentum toward equilibrium. The model has one free parameter - the relaxation time, which controls the viscosity of the fluid. We refer the readers to a book [36] and our previous works =-=[38, 39, 41, 45]-=- for details on SRTLBM. MRTLBM uses a more general collision model in which many of the hydrodynamic moments relax toward their equilibria independently [12]. The additional freedom afforded by the de
ques. A GPU cluster based on previous generation of NVIDIA cards has already been used successfully in applying a parallel Lattice Boltzmann Method to several problems in computational fluid dynamics =-=[17]-=-. We plan to investigate the efficiency of mapping particle-in-cell algorithms for plasma dynamics to scalable high-performance distributed GPU architectures. Concurrent visualization is another direc
r graphics processing. It was difficult to use the languages for solving Navier-Stokes equations. Hence, only a few attempts were made by experts in graphics (Ho et al. 2008; Scheidegger et al. 2005; =-=Wei et al. 2004-=-). In 2006, NVIDIA (NVIDIA 2007) provided a new GPU programming environment, named computer unified data architecture (CUDA). CUDA is an extended C language. Like other advanced programming languages,"""

c = txt.split('\n')

nlp = spacy.load('en_core_web_sm')
nlp.add_pipe('merge_compounds')
stem = nltk.stem.porter.PorterStemmer().stem


# Version fréquence
np_grammar = r"""
    NBAR:
        {<NOUN|PROPN|ADJ>*<NOUN|PROPN>} 
        
    NP:
        {<NBAR>}
        {<NBAR><ADP><NBAR>}
"""
voc = defaultdict(lambda: [[0]*len(c), [[] for _ in range(len(c))] ])
for i, cc in enumerate(c):
    cc, center = pretreat_ctxt(cc)
    doc = merge_compounds(nlp(cc))
    #center = (cc.index('=-=') + 3 + cc.index('-=-')) / 2
    for w in grammar_selection(doc, np_grammar):
        s = ' '.join([stem(e.text).lower() for e in w])
        voc[s][0][i] += 1
        voc[s][1][i].append(abs(w[0].idx - center))

# pour chaque candidats
# document frequency
# sum of tf
# minimal distance to citation
a = sorted([(k, (sum(map(bool,v[0])), sum(v[0]), min(1/(p or 1) for d in v[1] for p in d))) for k, v in voc.items()],key=lambda x: x[1], reverse=True)[:10]
b = sorted([(k, sum(map(bool,v[0]))) for k, v in voc.items()],key=lambda x: x[1], reverse=True)[:10]
# Selection des candidats avec grammaire
# Pondération en fonction du nombre d'apparition


# Version MPrank
voc = defaultdict(lambda: [[0]*len(c), [[] for _ in range(len(c))]])
for i, cc in enumerate(c):
    cc, center = pretreat_ctxt(cc)
    #doc = merge_compounds(nlp(cc))
    e = pke.unsupervised.MultipartiteRank()
    e.load_document(cc, spacy_model=nlp)
    e.candidate_selection()
    e.candidate_weighting()
    kws = e.get_n_best(10, stemming=True)
    for k in kws:
        voc[k[0]][0][i] = k[1]
        voc[k[0]][1][i] = e.candidates[k[0]].offsets
        # TODO: really compute the distance to center !
ga = sorted([(k, (sum(map(bool,v[0])), sum(v[0]), min(1/(p or 1) for d in v[1] for p in d))) for k, v in voc.items()], key=lambda x: x[1], reverse=True)[:10]
bu = sorted([(k, sum(v[0])) for k, v in voc.items()],key=lambda x: x[1], reverse=True)[:10]


# TODO: calculer Tf-Idf avec 1 doc = ensemble des contextes
# TODO: calculer Tf-Idf avec 1 doc = 1 contexte
import pke
import spacy

with open('../jsonl/data.json') as f:
    data = list(map(json.loads, f))

nlp = spacy.load('en_core_web_sm',
                 disable=['ner', 'textcat', 'parser'])
if int(spacy.__version__.split('.')[0]) < 3:
    sentencizer = nlp.create_pipe('sentencizer')
else:
    sentencizer = 'sentencizer'
nlp.add_pipe(sentencizer)

def extractt(context, df):
    e = pke.unsupervised.TfIdf()
    e.load_document(context, spacy_model=nlp)
    e.grammar_selection()
    e.candidate_weighting(df=df)
    return e.get_n_best(10, redundancy_removal=True)

ctxts = [(d['id'], '. '.join(pretreat_ctxt(c[1])[0] for c in d['ctxt'])) for d in data]
pke.utils.compute_document_frequency(input_dir=tqdm([c[1] for c in ctxts]), output_file='tfidf_agg.gz')
df = pke.utils.load_document_frequency_file('tfidf_agg.gz')
res = [(c[0], extractt(c[1], df)) for c in ctxts]


# How many min context a document should have
# 5 or more because going up to 6 makes us lose ~50000 documents
# They go up to 18972
# We keep the one that have lots of citation because they will be better represtened


## Essayer en faisant la moyenne des scores si != 0 !!!!! voir si ça change un truc beaucoup ou pas

# On pondère soit par la fréquence soit par une méthode MPRank ou YAKE (moyenne ou somme)
# Il faut virer les et al ! comment ? post traitement