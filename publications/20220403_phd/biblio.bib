
@article{ercan_using_2007,
	title = {Using lexical chains for keyword extraction},
	volume = {43},
	issn = {03064573},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0306457307000398},
	doi = {10.1016/j.ipm.2007.01.015},
	abstract = {Silber and McCoy (2002) describe another eﬃcient method that evaluates lexical chains by word sense disambiguation. Silber builds metachains for a text, by ﬁnding the relationships between words, but not building the lexical chains. For each word, the word sense that has more than one relationship with other words is selected and the other senses of the word are removed. After disambiguating all the words in the text, the lexical chain is built. In both of Barzilay’s and Silber’s algorithms, lexical chains in two diﬀerent segments are merged if there is a word re-iteration or a synonym relation. If there is only one lexical chain for a text, this means that all the words in the text are related. In general, a text can be associated with more than one lexical chain. In this case, the words of the text are grouped according to relatedness into subsets where each subset is indicated by a lexical chain. We have used the approach described in (Silber \& McCoy, 2002) with minor changes. In (Silber \& McCoy, 2002), the WordNet database is re-indexed to access it more eﬃciently. We have not done this re-indexing, instead we created ﬂat relationship lists for each word (up to 3 levels of depth). This technique enabled us to check for relations in linear time and enabled us to ﬁnd a relation between two word-senses faster. Before building lexical chains we have to identify nouns in the text. We have used only the relations between nouns, since they provide more information about the subject, and keywords usually appear in text as nouns. The part of speech (POS) tagger, that is used in order to identify nouns in the text, is Maxent Tagger (Toutanova \& Manning, 2000; Toutanova, Klein, Manning, \& Singer, 2003). The WordNet relations used in our version of lexical chain builder are synonym, hypernym/hyponym and meronym. The algorithm in (Silber \& McCoy, 2002) uses the same relations except meronym, and they also consider hypernym siblings in the construction of the lexical chains. The system in (Barzilay \& Elhadad, 1997) uses exactly the same features that we use.},
	language = {en},
	number = {6},
	urldate = {2018-10-04},
	journal = {Information Processing \& Management},
	author = {Ercan, Gonenc and Cicekli, Ilyas},
	month = nov,
	year = {2007},
	keywords = {decision tree, supervised, kpe},
	pages = {1705--1714},
	file = {Ercan et Cicekli - 2007 - Using lexical chains for keyword extraction.pdf:/home/gallina-y/Zotero/storage/YPJAZ2AR/Ercan et Cicekli - 2007 - Using lexical chains for keyword extraction.pdf:application/pdf},
}

@article{barzilay_lexical_1997,
	title = {Lexical {Chains} for {Summarization}},
	language = {en},
	author = {Barzilay, Regina},
	year = {1997},
	keywords = {lexical chain},
	pages = {95},
	file = {Barzilay - Lexical Chains for Summarization.pdf:/home/gallina-y/Zotero/storage/T6M8Q6HH/Barzilay - Lexical Chains for Summarization.pdf:application/pdf},
}

@article{matsuo_keyword_2004,
	title = {Keyword {Extraction} from a {Single} {Document} {Using} {Word} {Co}-occurrence {Statistical} {Information}},
	volume = {13},
	issn = {0218-2130, 1793-6349},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213004001466},
	doi = {10.1142/S0218213004001466},
	abstract = {We present a new keyword extraction algorithm that applies to a single document without using a corpus. Frequent terms are extracted ﬁrst, then a set of cooccurrence between each term and the frequent terms, i.e., occurrences in the same sentences, is generated. Co-occurrence distribution shows importance of a term in the document as follows. If probability distribution of co-occurrence between term a and the frequent terms is biased to a particular subset of frequent terms, then term a is likely to be a keyword. The degree of biases of distribution is measured by the χ2-measure. Our algorithm shows comparable performance to tﬁdf without using a corpus.},
	language = {en},
	number = {01},
	urldate = {2018-10-04},
	journal = {International Journal on Artificial Intelligence Tools},
	author = {Matsuo, Y. and Ishizuka, M.},
	month = mar,
	year = {2004},
	keywords = {kpe},
	pages = {157--169},
	file = {Matsuo et Ishizuka - 2004 - KEYWORD EXTRACTION FROM A SINGLE DOCUMENT USING WO.pdf:/home/gallina-y/Zotero/storage/6LA543DA/Matsuo et Ishizuka - 2004 - KEYWORD EXTRACTION FROM A SINGLE DOCUMENT USING WO.pdf:application/pdf},
}

@inproceedings{liu_clustering_2009,
	address = {Singapore},
	title = {Clustering to find exemplar terms for keyphrase extraction},
	volume = {1},
	isbn = {978-1-932432-59-6},
	url = {http://portal.acm.org/citation.cfm?doid=1699510.1699544},
	doi = {10.3115/1699510.1699544},
	abstract = {Keyphrases are widely used as a brief summary of documents. Since manual assignment is time-consuming, various unsupervised ranking methods based on importance scores are proposed for keyphrase extraction. In practice, the keyphrases of a document should not only be statistically important in the document, but also have a good coverage of the document. Based on this observation, we propose an unsupervised method for keyphrase extraction. Firstly, the method ﬁnds exemplar terms by leveraging clustering techniques, which guarantees the document to be semantically covered by these exemplar terms. Then the keyphrases are extracted from the document using the exemplar terms. Our method outperforms sate-of-the-art graphbased ranking methods (TextRank) by 9.5\% in F1-measure.},
	language = {en},
	urldate = {2018-10-04},
	booktitle = {Proceedings of the 2009 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} {Volume} 1 - {EMNLP} '09},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Zhiyuan and Li, Peng and Zheng, Yabin and Sun, Maosong},
	year = {2009},
	keywords = {kpe},
	pages = {257},
	file = {Liu et al. - 2009 - Clustering to find exemplar terms for keyphrase ex.pdf:/home/gallina-y/Zotero/storage/TPAVD8SL/Liu et al. - 2009 - Clustering to find exemplar terms for keyphrase ex.pdf:application/pdf},
}

@inproceedings{jiang_ranking_2009,
	address = {Boston, MA, USA},
	title = {A ranking approach to keyphrase extraction},
	isbn = {978-1-60558-483-6},
	url = {http://portal.acm.org/citation.cfm?doid=1571941.1572113},
	doi = {10.1145/1571941.1572113},
	abstract = {This paper addresses the issue of automatically extracting keyphrases from a document. Previously, this problem was formalized as classiﬁcation and learning methods for classiﬁcation were utilized. This paper points out that it is more essential to cast the problem as ranking and employ a learning to rank method to perform the task. Speciﬁcally, it employs Ranking SVM, a state-of-art method of learning to rank, in keyphrase extraction. Experimental results on three datasets show that Ranking SVM signiﬁcantly outperforms the baseline methods of SVM and Naive Bayes, indicating that it is better to exploit learning to rank techniques in keyphrase extraction.},
	language = {en},
	urldate = {2018-10-04},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '09},
	publisher = {ACM Press},
	author = {Jiang, Xin and Hu, Yunhua and Li, Hang},
	year = {2009},
	keywords = {tagging, kpe},
	pages = {756},
	file = {Jiang et al. - 2009 - A ranking approach to keyphrase extraction.pdf:/home/gallina-y/Zotero/storage/W38A3GQA/Jiang et al. - 2009 - A ranking approach to keyphrase extraction.pdf:application/pdf},
}

@inproceedings{mihalcea_textrank:_2004,
	address = {Barcelona, Spain},
	title = {{TextRank}: {Bringing} {Order} into {Texts}},
	abstract = {In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.},
	language = {en},
	booktitle = {Proceedings of \{{EMNLP}-04\}and the 2004 {Conference} on {Empirical} {Methods} 	in {Natural} {Language} {Processing}},
	author = {Mihalcea, Rada and Tarau, Paul},
	year = {2004},
	keywords = {graph, kpe},
	pages = {8},
	file = {Mihalcea et Tarau - TextRank Bringing Order into Texts.pdf:/home/gallina-y/Zotero/storage/J98CQEYN/Mihalcea et Tarau - TextRank Bringing Order into Texts.pdf:application/pdf},
}

@inproceedings{hasan_automatic_2014,
	address = {Baltimore, Maryland},
	title = {Automatic {Keyphrase} {Extraction}: {A} {Survey} of the {State} of the {Art}},
	shorttitle = {Automatic {Keyphrase} {Extraction}},
	url = {http://aclweb.org/anthology/P14-1119},
	doi = {10.3115/v1/P14-1119},
	abstract = {While automatic keyphrase extraction has been examined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.},
	language = {en},
	urldate = {2018-10-10},
	booktitle = {Proceedings of the 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hasan, Kazi Saidul and Ng, Vincent},
	year = {2014},
	keywords = {meta},
	pages = {1262--1273},
	file = {Hasan et Ng - 2014 - Automatic Keyphrase Extraction A Survey of the St.pdf:/home/gallina-y/Zotero/storage/DAYUHHFT/Hasan et Ng - 2014 - Automatic Keyphrase Extraction A Survey of the St.pdf:application/pdf},
}

@article{harastani_identification_2013,
	title = {Identification, alignement, et traductions des adjectifs relationnels en corpus comparables},
	abstract = {In this paper, we extract French relational adjectives and automatically align them with the nouns they are derived from by using a monolingual corpus. The obtained adjective-noun alignments are then used in the compositional translation of compound nouns of the form [N ADJR] with a French-English comparable corpora. A new term [N N�] (eg, cancer du poumon) is obtained by replacing the relational adjective Ad jR (eg, pulmonaire) in [N AdjR] (eg, cancer pulmonaire) by its corresponding N� (eg, poumon). If no translation(s) are obtained for [N AdjR], we consider the one(s) obtained for its paraphrase [N N�]. We experiment with a comparable corpora in the field of breast cancer, and we get adjective-noun alignments that help in translating French compound nouns of the form [N AdjR] to English with a precision of 86\%. MOTS-CLÉS : Adjectifs relationnels, Corpus comparables, Méthode compositionnelle, Termes complexes.},
	language = {fr},
	author = {Harastani, Rima and Daille, Beatrice and Morin, Emmanuel},
	year = {2013},
	pages = {15},
	file = {Harastani et al. - 2013 - Identification, alignement, et traductions des adj.pdf:/home/gallina-y/Zotero/storage/KJ8X4HQN/Harastani et al. - 2013 - Identification, alignement, et traductions des adj.pdf:application/pdf},
}

@inproceedings{hulth_improved_2003,
	address = {Not Known},
	title = {Improved automatic keyword extraction given more linguistic knowledge},
	volume = {10},
	url = {http://portal.acm.org/citation.cfm?doid=1119355.1119383},
	doi = {10.3115/1119355.1119383},
	abstract = {In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and ngrams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the POS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.},
	language = {en},
	urldate = {2018-10-11},
	booktitle = {Proceedings of the 2003 conference on {Empirical} methods in natural language processing  -},
	publisher = {Association for Computational Linguistics},
	author = {Hulth, Anette},
	year = {2003},
	keywords = {kpe},
	pages = {216--223},
	file = {Hulth - 2003 - Improved automatic keyword extraction given more l.pdf:/home/gallina-y/Zotero/storage/953FT537/Hulth - 2003 - Improved automatic keyword extraction given more l.pdf:application/pdf},
}

@inproceedings{tomokiyo_language_2003,
	address = {Not Known},
	title = {A language model approach to keyphrase extraction},
	volume = {18},
	url = {http://portal.acm.org/citation.cfm?doid=1119282.1119287},
	doi = {10.3115/1119282.1119287},
	abstract = {We present a new approach to extracting keyphrases based on statistical language models. Our approach is to use pointwise KL-divergence between multiple language models for scoring both phraseness and informativeness, which can be uniﬁed into a single score to rank extracted phrases.},
	language = {en},
	urldate = {2018-10-12},
	booktitle = {Proceedings of the {ACL} 2003 workshop on {Multiword} expressions analysis, acquisition and treatment -},
	publisher = {Association for Computational Linguistics},
	author = {Tomokiyo, Takashi and Hurst, Matthew},
	year = {2003},
	keywords = {language model, kpe},
	pages = {33--40},
	file = {Tomokiyo et Hurst - 2003 - A language model approach to keyphrase extraction.pdf:/home/gallina-y/Zotero/storage/2XD47MIT/Tomokiyo et Hurst - 2003 - A language model approach to keyphrase extraction.pdf:application/pdf},
}

@article{zhang_wordtopic-multirank:_2013,
	title = {{WordTopic}-{MultiRank}: {A} {New} {Method} for {Automatic} {Keyphrase} {Extraction}},
	abstract = {Automatic keyphrase extraction aims to pick out a set of terms as a representation of a document without manual assignment efforts. Supervised and unsupervised graph-based ranking methods have been studied for this task. However, previous methods usually computed importance scores of words under the assumption of single relation between words. In this work, we propose WordTopic-MultiRank as a new method for keyphrase extraction, based on the idea that words relate with each other via multiple relations. First we treat various latent topics in documents as heterogeneous relations between words and construct a multi-relational word network. Then, a novel ranking algorithm, named Biased-MultiRank, is applied to score the importance of words and topics simultaneously, as words and topics are considered to have mutual inﬂuence on each other. Experimental results on two different data sets show the outstanding performance and robustness of our proposed approach in automatic keyphrase extraction task.},
	language = {en},
	author = {Zhang, Fan and Huang, Lian'en and Peng, Bo},
	year = {2013},
	keywords = {lda, graph, kpe},
	pages = {9},
	file = {Zhang et al. - WordTopic-MultiRank A New Method for Automatic Ke.pdf:/home/gallina-y/Zotero/storage/QGMBNF48/Zhang et al. - WordTopic-MultiRank A New Method for Automatic Ke.pdf:application/pdf},
}

@inproceedings{gillick_multilingual_2016,
	address = {San Diego, California},
	title = {Multilingual {Language} {Processing} {From} {Bytes}},
	url = {http://aclweb.org/anthology/N16-1155},
	doi = {10.18653/v1/N16-1155},
	abstract = {We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than languagespeciﬁc words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-ofthe-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning “from scratch” in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text.},
	language = {en},
	urldate = {2018-10-16},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Gillick, Dan and Brunk, Cliff and Vinyals, Oriol and Subramanya, Amarnag},
	year = {2016},
	pages = {1296--1306},
	file = {Gillick et al. - 2016 - Multilingual Language Processing From Bytes.pdf:/home/gallina-y/Zotero/storage/VSVWF6I3/Gillick et al. - 2016 - Multilingual Language Processing From Bytes.pdf:application/pdf},
}

@article{ding_keyphrase_2011,
	title = {Keyphrase {Extraction} from {Online} {News} {Using} {Binary} {Integer} {Programming}},
	abstract = {In recent years, keyphrase extraction has received great attention, and been successfully employed by various applications. Keyphrases extracted from news articles can be used to concisely represent main contents of news events. Keyphrases can help users to speed up browsing and ﬁnd the desired contents more quickly. In this paper, we ﬁrst present several criteria of high-quality news keyphrases. After that, in order to integrate those criteria into the keyphrase extraction task, we propose a novel formulation which converts the task to a binary integer programming problem. The formulation cannot only encode the prior knowledge as constraints, but also learn constraints from data. We evaluate the proposed approach on a manually labeled corpus. Experimental results demonstrate that our approach achieves better performances compared with the state-of-the-art methods.},
	language = {en},
	author = {Ding, Zhuoye and Zhang, Qi and Huang, Xuanjing},
	year = {2011},
	keywords = {lda, kpe},
	pages = {9},
	file = {Ding et al. - Keyphrase Extraction from Online News Using Binary.pdf:/home/gallina-y/Zotero/storage/DQVBERJ7/Ding et al. - Keyphrase Extraction from Online News Using Binary.pdf:application/pdf},
}

@inproceedings{chen_keyphrase_2018,
	address = {Brussels, Belgium},
	title = {Keyphrase {Generation} with {Correlation} {Constraints}},
	url = {http://arxiv.org/abs/1808.07185},
	abstract = {In this paper, we study automatic keyphrase generation. Although conventional approaches to this task show promising results, they neglect correlation among keyphrases, resulting in duplication and coverage issues. To solve these problems, we propose a new sequence-to-sequence architecture for keyphrase generation named CorrRNN, which captures correlation among multiple keyphrases in two ways. First, we employ a coverage vector to indicate whether the word in the source document has been summarized by previous phrases to improve the coverage for keyphrases. Second, preceding phrases are taken into account to eliminate duplicate phrases and improve result coherence. Experiment results show that our model signiﬁcantly outperforms the state-of-the-art method on benchmark datasets in terms of both accuracy and diversity.},
	language = {en},
	urldate = {2018-10-25},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Jun and Zhang, Xiaoming and Wu, Yu and Yan, Zhao and Li, Zhoujun},
	month = aug,
	year = {2018},
	keywords = {code, generation, o2m},
	file = {Chen et al. - 2018 - Keyphrase Generation with Correlation Constraints.pdf:/home/gallina-y/Zotero/storage/5NA2BQMR/Chen et al. - 2018 - Keyphrase Generation with Correlation Constraints.pdf:application/pdf},
}

@article{papagiannopoulou_local_2018,
	title = {Local word vectors guiding keyphrase extraction},
	volume = {54},
	issn = {0306-4573},
	url = {http://www.sciencedirect.com/science/article/pii/S0306457317308427},
	doi = {10.1016/j.ipm.2018.06.004},
	abstract = {Automated keyphrase extraction is a fundamental textual information processing task concerned with the selection of representative phrases from a document that summarize its content. This work presents a novel unsupervised method for keyphrase extraction, whose main innovation is the use of local word embeddings (in particular GloVe vectors), i.e., embeddings trained from the single document under consideration. We argue that such local representation of words and keyphrases are able to accurately capture their semantics in the context of the document they are part of, and therefore can help in improving keyphrase extraction quality. Empirical results offer evidence that indeed local representations lead to better keyphrase extraction results compared to both embeddings trained on very large third corpora or larger corpora consisting of several documents of the same scientific field and to other state-of-the-art unsupervised keyphrase extraction methods.},
	number = {6},
	urldate = {2018-10-25},
	journal = {Information Processing \& Management},
	author = {Papagiannopoulou, Eirini and Tsoumakas, Grigorios},
	month = nov,
	year = {2018},
	keywords = {kpe},
	pages = {888--902},
	file = {ScienceDirect Full Text PDF:/home/gallina-y/Zotero/storage/EN7NQ3EV/Papagiannopoulou et Tsoumakas - 2018 - Local word vectors guiding keyphrase extraction.pdf:application/pdf;ScienceDirect Snapshot:/home/gallina-y/Zotero/storage/4HIGG9B8/S0306457317308427.html:text/html},
}

@article{habibi_diverse_2013,
	title = {Diverse {Keyword} {Extraction} from {Conversations}},
	abstract = {A new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. Inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. The method is evaluated on excerpts of the Fisher and AMI corpora, using a crowdsourcing platform to elicit comparative relevance judgments. The results demonstrate that the method outperforms two competitive baselines.},
	language = {en},
	author = {Habibi, Maryam and Popescu-Belis, Andrei},
	year = {2013},
	keywords = {kpe},
	pages = {7},
	file = {Habibi et Popescu-Belis - Diverse Keyword Extraction from Conversations.pdf:/home/gallina-y/Zotero/storage/R6JN8PLP/Habibi et Popescu-Belis - Diverse Keyword Extraction from Conversations.pdf:application/pdf},
}

@inproceedings{wan_collabrank:_2008,
	address = {Manchester, United Kingdom},
	title = {{CollabRank}: towards a collaborative approach to single-document keyphrase extraction},
	volume = {1},
	isbn = {978-1-905593-44-6},
	shorttitle = {{CollabRank}},
	url = {http://portal.acm.org/citation.cfm?doid=1599081.1599203},
	doi = {10.3115/1599081.1599203},
	abstract = {Previous methods usually conduct the keyphrase extraction task for single documents separately without interactions for each document, under the assumption that the documents are considered independent of each other. This paper proposes a novel approach named CollabRank to collaborative single-document keyphrase extraction by making use of mutual influences of multiple documents within a cluster context. CollabRank is implemented by first employing the clustering algorithm to obtain appropriate document clusters, and then using the graph-based ranking algorithm for collaborative single-document keyphrase extraction within each cluster. Experimental results demonstrate the encouraging performance of the proposed approach. Different clustering algorithms have been investigated and we find that the system performance relies positively on the quality of document clusters.},
	language = {en},
	urldate = {2018-11-03},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Computational} {Linguistics} - {COLING} '08},
	publisher = {Association for Computational Linguistics},
	author = {Wan, Xiaojun and Xiao, Jianguo},
	year = {2008},
	keywords = {graph, kpe},
	pages = {969--976},
	file = {Wan and Xiao - 2008 - CollabRank towards a collaborative approach to si.pdf:/home/gallina-y/Zotero/storage/9Q5NB4MF/Wan and Xiao - 2008 - CollabRank towards a collaborative approach to si.pdf:application/pdf},
}

@inproceedings{narayan_dont_2018,
	address = {Brussels, Belgium},
	title = {Don't {Give} {Me} the {Details}, {Just} the {Summary}! {Topic}-{Aware} {Convolutional} {Neural} {Networks} for {Extreme} {Summarization}},
	url = {http://www.aclweb.org/anthology/D18-1206},
	urldate = {2018-11-03},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	month = nov,
	year = {2018},
	pages = {1797--1807},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/8W9BXJTJ/Narayan et al. - 2018 - Don't Give Me the Details, Just the Summary! Topic.pdf:application/pdf},
}

@inproceedings{wang_neural_2018,
	address = {Brussels, Belgium},
	title = {Neural {Related} {Work} {Summarization} with a {Joint} {Context}-driven {Attention} {Mechanism}},
	url = {http://www.aclweb.org/anthology/D18-1204},
	urldate = {2018-11-03},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yongzhen and Liu, Xiaozhong and Gao, Zheng},
	month = nov,
	year = {2018},
	pages = {1776--1786},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/VNWJCCKQ/Wang et al. - 2018 - Neural Related Work Summarization with a Joint Con.pdf:application/pdf},
}

@inproceedings{boudin_unsupervised_2018,
	title = {Unsupervised {Keyphrase} {Extraction} with {Multipartite} {Graphs}},
	url = {http://arxiv.org/abs/1803.08721},
	abstract = {We propose an unsupervised keyphrase extraction model that encodes topical information within a multipartite graph structure. Our model represents keyphrase candidates and topics in a single graph and exploits their mutually reinforcing relationship to improve candidate ranking. We further introduce a novel mechanism to incorporate keyphrase selection preferences into the model. Experiments conducted on three widely used datasets show significant improvements over state-of-the-art graph-based models.},
	urldate = {2018-11-03},
	booktitle = {Proceedings of {NAACL}-{HLT} 2018},
	publisher = {Association for Computational Linguistics},
	author = {Boudin, Florian},
	month = mar,
	year = {2018},
	keywords = {graph, kpe, unsupervised},
	file = {arXiv\:1803.08721 PDF:/home/gallina-y/Zotero/storage/RYN5H7G5/Boudin - 2018 - Unsupervised Keyphrase Extraction with Multipartit.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/2888DTVY/1803.html:text/html},
}

@inproceedings{bennani-smires_simple_2018,
	address = {Brussels, Belgium},
	title = {Simple {Unsupervised} {Keyphrase} {Extraction} using {Sentence} {Embeddings}},
	url = {http://www.aclweb.org/anthology/K18-1022},
	abstract = {Keyphrase extraction is the task of automatically selecting a small set of phrases that best describe a given free text document. Supervised keyphrase extraction requires large amounts of labeled training data and generalizes very poorly outside the domain of the training data. At the same time, unsupervised systems have poor accuracy, and often do not generalize well, as they require the input document to belong to a larger corpus also given as input.},
	urldate = {2018-10-31},
	booktitle = {Proceedings of the 22nd {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Bennani-Smires, Kamil and Musat, Claudiu and Hossmann, Andreea and Baeriswyl, Michael and Jaggi, Martin},
	month = oct,
	year = {2018},
	keywords = {embedding, unsupervised, kpe},
	pages = {221--229},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/HPT5JHLG/Bennani-Smires et al. - 2018 - Simple Unsupervised Keyphrase Extraction using Sen.pdf:application/pdf},
}

@inproceedings{sterckx_topical_2015,
	address = {Florence, Italy},
	title = {Topical {Word} {Importance} for {Fast} {Keyphrase} {Extraction}},
	isbn = {978-1-4503-3473-0},
	url = {http://dl.acm.org/citation.cfm?doid=2740908.2742730},
	doi = {10.1145/2740908.2742730},
	abstract = {We propose an improvement on a state-of-the-art keyphrase extraction algorithm, Topical PageRank (TPR), incorporating topical information from topic models. While the original algorithm requires a random walk for each topic in the topic model being used, ours is independent of the topic model, computing but a single PageRank for each text regardless of the amount of topics in the model. This increases the speed drastically and enables it for use on large collections of text using vast topic models, while not altering performance of the original algorithm.},
	language = {en},
	urldate = {2018-11-08},
	booktitle = {Proceedings of the 24th {International} {Conference} on {World} {Wide} {Web} - {WWW} '15 {Companion}},
	publisher = {ACM Press},
	author = {Sterckx, Lucas and Demeester, Thomas and Deleu, Johannes and Develder, Chris},
	year = {2015},
	keywords = {lda, kpe},
	pages = {121--122},
	file = {Sterckx et al. - 2015 - Topical Word Importance for Fast Keyphrase Extract.pdf:/home/gallina-y/Zotero/storage/QU7GFX8L/Sterckx et al. - 2015 - Topical Word Importance for Fast Keyphrase Extract.pdf:application/pdf},
}

@inproceedings{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2018-11-19},
	booktitle = {Proceedings of the 2015 {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	file = {Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf:/home/gallina-y/Zotero/storage/RZZEGD62/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf},
}

@inproceedings{witten_kea:_1999,
	address = {Berkeley, California, United States},
	title = {{KEA}: practical automatic keyphrase extraction},
	isbn = {978-1-58113-145-1},
	shorttitle = {{KEA}},
	url = {http://portal.acm.org/citation.cfm?doid=313238.313437},
	doi = {10.1145/313238.313437},
	abstract = {Keyphrases provide semantic metadata that summarize and characterize documents. This paper describes Kea, an algorithm for automatically extracting keyphrases from text. Kea identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machine-learning algorithm to predict which candidates are good keyphrases. The machine learning scheme first builds a prediction model using training documents with known keyphrases, and then uses the model to find keyphrases in new documents. We use a large test corpus to evaluate Kea’s effectiveness in terms of how many author-assigned keyphrases are correctly identified. The system is simple, robust, and publicly available.},
	language = {en},
	urldate = {2018-12-03},
	booktitle = {Proceedings of the fourth {ACM} conference on {Digital} libraries  - {DL} '99},
	publisher = {ACM Press},
	author = {Witten, Ian H. and Paynter, Gordon W. and Frank, Eibe and Gutwin, Carl and Nevill-Manning, Craig G.},
	year = {1999},
	keywords = {supervised, kpe},
	pages = {254--255},
	file = {Witten et al. - 1999 - KEA practical automatic keyphrase extraction.pdf:/home/gallina-y/Zotero/storage/GQCQP7M4/Witten et al. - 1999 - KEA practical automatic keyphrase extraction.pdf:application/pdf},
}

@inproceedings{zhang_keyphrase_2016,
	address = {Austin, Texas},
	title = {Keyphrase {Extraction} {Using} {Deep} {Recurrent} {Neural} {Networks} on {Twitter}},
	url = {http://aclweb.org/anthology/D16-1080},
	doi = {10.18653/v1/D16-1080},
	abstract = {Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas. The task of automatically extracting them have received considerable attention in recent decades. Different from previous studies, which are usually focused on automatically extracting keyphrases from documents or articles, in this study, we considered the problem of automatically extracting keyphrases from tweets. Because of the length limitations of Twitter-like sites, the performances of existing methods usually drop sharply. We proposed a novel deep recurrent neural network (RNN) model to combine keywords and context information to perform this problem. To evaluate the proposed method, we also constructed a large-scale dataset collected from Twitter. The experimental results showed that the proposed method performs signiﬁcantly better than previous methods.},
	language = {en},
	urldate = {2018-12-03},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Qi and Wang, Yang and Gong, Yeyun and Huang, Xuanjing},
	year = {2016},
	keywords = {extraction},
	pages = {836--845},
	file = {Zhang et al. - 2016 - Keyphrase Extraction Using Deep Recurrent Neural N.pdf:/home/gallina-y/Zotero/storage/GL6QZV5Y/Zhang et al. - 2016 - Keyphrase Extraction Using Deep Recurrent Neural N.pdf:application/pdf},
}

@article{hasan_conundrums_2010,
	title = {Conundrums in {Unsupervised} {Keyphrase} {Extraction}: {Making} {Sense} of the {State}-of-the-{Art}},
	abstract = {State-of-the-art approaches for unsupervised keyphrase extraction are typically evaluated on a single dataset with a single parameter setting. Consequently, it is unclear how effective these approaches are on a new dataset from a different domain, and how sensitive they are to changes in parameter settings. To gain a better understanding of state-of-the-art unsupervised keyphrase extraction algorithms, we conduct a systematic evaluation and analysis of these algorithms on a variety of standard evaluation datasets.},
	language = {en},
	author = {Hasan, Kazi Saidul and Ng, Vincent},
	year = {2010},
	keywords = {meta},
	pages = {9},
	file = {Hasan et Ng - Conundrums in Unsupervised Keyphrase Extraction M.pdf:/home/gallina-y/Zotero/storage/XL6NGR3W/Hasan et Ng - Conundrums in Unsupervised Keyphrase Extraction M.pdf:application/pdf},
}

@article{schuster_bidirectional_1997,
	title = {Bidirectional recurrent neural networks},
	volume = {45},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/650093/},
	doi = {10.1109/78.650093},
	abstract = {In the ﬁrst part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classiﬁcation experiments on artiﬁcial data, the proposed structure gives better results than other approaches. For real data, classiﬁcation experiments for phonemes from the TIMIT database show the same tendency.},
	language = {en},
	number = {11},
	urldate = {2018-12-05},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schuster, M. and Paliwal, K.K.},
	month = nov,
	year = {1997},
	pages = {2673--2681},
	file = {Schuster et Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:/home/gallina-y/Zotero/storage/UT3FXTFM/Schuster et Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:application/pdf},
}

@article{salton_vector_1975,
	title = {A vector space model for automatic indexing},
	volume = {18},
	issn = {0001-0782},
	url = {http://dl.acm.org/citation.cfm?id=361219.361220},
	doi = {10.1145/361219.361220},
	number = {11},
	urldate = {2018-12-06},
	journal = {Communications of the ACM},
	author = {Salton, G. and Wong, A. and Yang, C. S.},
	month = jan,
	year = {1975},
	pages = {613--620},
	file = {Version soumise:/home/gallina-y/Zotero/storage/PKBMAT9X/Salton et al. - 1975 - A vector space model for automatic indexing.pdf:application/pdf},
}

@inproceedings{boudin_comparison_2013,
	title = {A {Comparison} of {Centrality} {Measures} for {Graph}-{Based} {Keyphrase} {Extraction}},
	url = {https://hal.archives-ouvertes.fr/hal-00850187/document},
	abstract = {In this paper, we present and compare various centrality measures for graph-based keyphrase extraction. Through experiments carried out on three standard datasets of different languages and domains, we show that simple degree centrality achieve results comparable to the widely used TextRank algorithm, and that closeness centrality obtains the best results on short documents.},
	language = {en},
	urldate = {2018-12-06},
	author = {Boudin, Florian},
	month = oct,
	year = {2013},
	keywords = {graph, meta},
	pages = {834--838},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/NUGPHDLR/Boudin - 2013 - A Comparison of Centrality Measures for Graph-Base.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/VZJSRAAA/hal-00850187.html:text/html},
}

@inproceedings{bougouin_topicrank:_2013,
	address = {Nagoya, Japan},
	title = {{TopicRank}: {Graph}-{Based} {Topic} {Ranking} for {Keyphrase} {Extraction}},
	shorttitle = {{TopicRank}},
	url = {https://hal.archives-ouvertes.fr/hal-00917969},
	abstract = {Keyphrase extraction is the task of iden- tifying single or multi-word expressions that represent the main topics of a doc- ument. In this paper we present TopicRank, a graph-based keyphrase extraction method that relies on a topical representation of the document. Candidate keyphrases are clustered into topics and used as vertices in a complete graph. A graph-based ranking model is applied to assign a significance score to each topic. Keyphrases are then generated by selecting a candidate from each of the top-ranked topics. We conducted experiments on four evaluation datasets of different languages and domains. Results show that TopicRank significantly outperforms state-of-the-art methods on three datasets.},
	urldate = {2018-12-06},
	booktitle = {International {Joint} {Conference} on {Natural} {Language} {Processing} ({IJCNLP})},
	author = {Bougouin, Adrien and Boudin, Florian and Daille, Béatrice},
	month = oct,
	year = {2013},
	keywords = {graph, kpe},
	pages = {543--551},
	file = {HAL PDF Full Text:/home/gallina-y/Zotero/storage/KH624DRJ/Bougouin et al. - 2013 - TopicRank Graph-Based Topic Ranking for Keyphrase.pdf:application/pdf},
}

@inproceedings{augenstein_semeval_2017,
	address = {Vancouver, Canada},
	title = {{SemEval} 2017 {Task} 10: {ScienceIE} - {Extracting} {Keyphrases} and {Relations} from {Scientific} {Publications}},
	shorttitle = {{SemEval} 2017 {Task} 10},
	url = {http://arxiv.org/abs/1704.02853},
	abstract = {We describe the SemEval task of extracting keyphrases and relations between them from scientiﬁc documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the ﬁndings reported in this paper to be relevant for researchers working on understanding scientiﬁc content, as well as the broader knowledge base population and information extraction communities.},
	language = {en},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Semantic} {Evaluations} ({SemEval}-2017)},
	publisher = {Association for Computational Linguistics},
	author = {Augenstein, Isabelle and Das, Mrinal and Riedel, Sebastian and Vikraman, Lakshmi and McCallum, Andrew},
	month = apr,
	year = {2017},
	keywords = {kpe},
	file = {Augenstein et al. - 2017 - SemEval 2017 Task 10 ScienceIE - Extracting Keyph.pdf:/home/gallina-y/Zotero/storage/HMGMMEL3/Augenstein et al. - 2017 - SemEval 2017 Task 10 ScienceIE - Extracting Keyph.pdf:application/pdf},
}

@inproceedings{kim_semeval-2010_2010,
	address = {Stroudsburg, PA, USA},
	series = {{SemEval} '10},
	title = {{SemEval}-2010 {Task} 5: {Automatic} {Keyphrase} {Extraction} from {Scientific} {Articles}},
	shorttitle = {{SemEval}-2010 {Task} 5},
	url = {http://dl.acm.org/citation.cfm?id=1859664.1859668},
	abstract = {This paper describes Task 5 of the Workshop on Semantic Evaluation 2010 (SemEval-2010). Systems are to automatically assign keyphrases or keywords to given scientific articles. The participating systems were evaluated by matching their extracted keyphrases against manually assigned ones. We present the overall ranking of the submitted systems and discuss our findings to suggest future directions for this task.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 5th {International} {Workshop} on {Semantic} {Evaluation}},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Su Nam and Medelyan, Olena and Kan, Min-Yen and Baldwin, Timothy},
	year = {2010},
	keywords = {dataset},
	pages = {21--26},
	file = {ACM Full Text PDF:/home/gallina-y/Zotero/storage/K3YTHHD2/Kim et al. - 2010 - SemEval-2010 Task 5 Automatic Keyphrase Extractio.pdf:application/pdf},
}

@inproceedings{liu_automatic_2011,
	address = {Stroudsburg, PA, USA},
	series = {{CoNLL} '11},
	title = {Automatic {Keyphrase} {Extraction} by {Bridging} {Vocabulary} {Gap}},
	isbn = {978-1-932432-92-3},
	url = {http://dl.acm.org/citation.cfm?id=2018936.2018952},
	abstract = {Keyphrase extraction aims to select a set of terms from a document as a short summary of the document. Most methods extract keyphrases according to their statistical properties in the given document. Appropriate keyphrases, however, are not always statistically significant or even do not appear in the given document. This makes a large vocabulary gap between a document and its keyphrases. In this paper, we consider that a document and its keyphrases both describe the same object but are written in two different languages. By regarding keyphrase extraction as a problem of translating from the language of documents to the language of keyphrases, we use word alignment models in statistical machine translation to learn translation probabilities between the words in documents and the words in keyphrases. According to the translation model, we suggest keyphrases given a new document. The suggested keyphrases are not necessarily statistically frequent in the document, which indicates that our method is more flexible and reliable. Experiments on news articles demonstrate that our method outperforms existing unsupervised methods on precision, recall and F-measure.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the {Fifteenth} {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Zhiyuan and Chen, Xinxiong and Zheng, Yabin and Sun, Maosong},
	year = {2011},
	keywords = {supervised, kpe},
	pages = {135--144},
	file = {ACM Full Text PDF:/home/gallina-y/Zotero/storage/B8ULKJ9L/Liu et al. - 2011 - Automatic Keyphrase Extraction by Bridging Vocabul.pdf:application/pdf},
}

@inproceedings{liu_automatic_2010,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '10},
	title = {Automatic {Keyphrase} {Extraction} via {Topic} {Decomposition}},
	url = {http://dl.acm.org/citation.cfm?id=1870658.1870694},
	abstract = {Existing graph-based ranking methods for keyphrase extraction compute a single importance score for each word via a single random walk. Motivated by the fact that both documents and words can be represented by a mixture of semantic topics, we propose to decompose traditional random walk into multiple random walks specific to various topics. We thus build a Topical PageRank (TPR) on word graph to measure word importance with respect to different topics. After that, given the topic distribution of the document, we further calculate the ranking scores of words and extract the top ranked ones as keyphrases. Experimental results show that TPR outperforms state-of-the-art keyphrase extraction methods on two datasets under various evaluation metrics.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2010 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Zhiyuan and Huang, Wenyi and Zheng, Yabin and Sun, Maosong},
	year = {2010},
	keywords = {lda, graph, kpe},
	pages = {366--376},
	file = {ACM Full Text PDF:/home/gallina-y/Zotero/storage/UJ8IH4JT/Liu et al. - 2010 - Automatic Keyphrase Extraction via Topic Decomposi.pdf:application/pdf},
}

@article{yao_graph_2018,
	title = {Graph {Convolutional} {Networks} for {Text} {Classification}},
	url = {https://arxiv.org/abs/1809.05679},
	language = {en},
	urldate = {2018-12-20},
	author = {Yao, Liang and Mao, Chengsheng and Luo, Yuan},
	month = sep,
	year = {2018},
	keywords = {gcn},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/WUAFJAZF/Yao et al. - 2018 - Graph Convolutional Networks for Text Classificati.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/LMSGGQ4I/1809.html:text/html},
}

@incollection{vinyals_pointer_2015,
	title = {Pointer {Networks}},
	url = {http://papers.nips.cc/paper/5866-pointer-networks.pdf},
	urldate = {2018-12-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2692--2700},
	file = {NIPS Full Text PDF:/home/gallina-y/Zotero/storage/GUHABSXL/Vinyals et al. - 2015 - Pointer Networks.pdf:application/pdf;NIPS Snapshot:/home/gallina-y/Zotero/storage/JJ2E22YY/5866-pointer-networks.html:text/html},
}

@inproceedings{basaldella_evaluating_2016,
	address = {Osaka, Japan},
	title = {Evaluating anaphora and coreference resolution to improve automatic keyphrase extraction},
	abstract = {In this paper we analyze the effectiveness of using linguistic knowledge from coreference and anaphora resolution for improving the performance for supervised keyphrase extraction. In order to verify the impact of these features, we deﬁne a baseline keyphrase extraction system and evaluate its performance on a standard dataset using different machine learning algorithms. Then, we consider new sets of features by adding combinations of the linguistic features we propose and we evaluate the new performance of the system. We also use anaphora and coreference resolution to transform the documents, trying to simulate the cohesion process performed by the human mind. We found that our approach has a slightly positive impact on the performance of automatic keyphrase extraction, in particular when considering the ranking of the results.},
	language = {en},
	booktitle = {In {Proceedings} of {COLING} 2016, 26th {International} {Conference} on {Computational} {Linguistics}},
	author = {Basaldella, Marco and Chiaradia, Giorgia and Tasso, Carlo},
	year = {2016},
	keywords = {kpe},
	pages = {11},
	file = {Basaldella et al. - 2016 - Evaluating anaphora and coreference resolution to .pdf:/home/gallina-y/Zotero/storage/T3UAFIVG/Basaldella et al. - 2016 - Evaluating anaphora and coreference resolution to .pdf:application/pdf},
}

@article{lopez_humb:_2010,
	title = {{HUMB}: {Automatic} key term extraction from scientific articles in {GROBID}},
	abstract = {The Semeval task 5 was an opportunity for experimenting with the key term extraction module of GROBID, a system for extracting and generating bibliographical information from technical and scientiﬁc documents. The tool ﬁrst uses GROBID’s facilities for analyzing the structure of scientiﬁc articles, resulting in a ﬁrst set of structural features. A second set of features captures content properties based on phraseness, informativeness and keywordness measures. Two knowledge bases, GRISP and Wikipedia, are then exploited for producing a last set of lexical/semantic features. Bagged decision trees appeared to be the most efﬁcient machine learning algorithm for generating a list of ranked key term candidates. Finally a post ranking was realized based on statistics of cousage of keywords in HAL, a large Open Access publication repository.},
	language = {en},
	author = {Lopez, Patrice and Romary, Laurent},
	year = {2010},
	keywords = {kpe},
	pages = {4},
	file = {Lopez et Romary - HUMB Automatic key term extraction from scientifi.pdf:/home/gallina-y/Zotero/storage/MCYYBUN5/Lopez et Romary - HUMB Automatic key term extraction from scientifi.pdf:application/pdf},
}

@inproceedings{medelyan_human-competitive_2009,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '09},
	title = {Human-competitive {Tagging} {Using} {Automatic} {Keyphrase} {Extraction}},
	isbn = {978-1-932432-63-3},
	url = {http://dl.acm.org/citation.cfm?id=1699648.1699678},
	abstract = {This paper connects two research areas: automatic tagging on the web and statistical keyphrase extraction. First, we analyze the quality of tags in a collaboratively created folksonomy using traditional evaluation techniques. Next, we demonstrate how documents can be tagged automatically with a state-of-the-art keyphrase extraction algorithm, and further improve performance in this new domain using a new algorithm, "Maui", that utilizes semantic information extracted from Wikipedia. Maui outperforms existing approaches and extracts tags that are competitive with those assigned by the best performing human taggers.},
	urldate = {2019-01-23},
	booktitle = {Proceedings of the 2009 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Volume} 3 - {Volume} 3},
	publisher = {Association for Computational Linguistics},
	author = {Medelyan, Olena and Frank, Eibe and Witten, Ian H.},
	year = {2009},
	keywords = {kpe},
	pages = {1318--1327},
	file = {ACM Full Text PDF:/home/gallina-y/Zotero/storage/JR86YU2L/Medelyan et al. - 2009 - Human-competitive Tagging Using Automatic Keyphras.pdf:application/pdf},
}

@article{salton_term-weighting_1988,
	title = {Term-weighting approaches in automatic text retrieval},
	volume = {24},
	issn = {0306-4573},
	url = {http://www.sciencedirect.com/science/article/pii/0306457388900210},
	doi = {10.1016/0306-4573(88)90021-0},
	abstract = {The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.},
	number = {5},
	urldate = {2019-01-24},
	journal = {Information Processing \& Management},
	author = {Salton, Gerard and Buckley, Christopher},
	month = jan,
	year = {1988},
	pages = {513--523},
	file = {ScienceDirect Snapshot:/home/gallina-y/Zotero/storage/35IMPLET/0306457388900210.html:text/html;Submitted Version:/home/gallina-y/Zotero/storage/32VNQSKC/Salton and Buckley - 1988 - Term-weighting approaches in automatic text retrie.pdf:application/pdf},
}

@inproceedings{bougouin_termith-eval:_2016,
	address = {Potoroz, Slovenia},
	title = {{TermITH}-{Eval}: a {French} {Standard}-{Based} {Resource} for {Keyphrase} {Extraction} {Evaluation}},
	shorttitle = {{TermITH}-{Eval}},
	url = {https://hal.archives-ouvertes.fr/hal-01693805},
	abstract = {Keyphrase extraction is the task of finding phrases that represent the important content of a document. The main aim of keyphrase extraction is to propose textual units that represent the most important topics developed in a document. The output keyphrases of automatic keyphrase extraction methods for test documents are typically evaluated by comparing them to manually assigned reference keyphrases. Each output keyphrase is considered correct if it matches one of the reference keyphrases. However, the choice of the appropriate textual unit (keyphrase) for a topic is sometimes subjective and evaluating by exact matching underestimates the performance. This paper presents a dataset of evaluation scores assigned to automatically extracted keyphrases by human evaluators. Along with the reference keyphrases, the manual evaluations can be used to validate new evaluation measures. Indeed, an evaluation measure that is highly correlated to the manual evaluation is appropriate for the evaluation of automatic keyphrase extraction methods.},
	urldate = {2019-01-24},
	booktitle = {{LREC} - {Language} {Resources} and {Evaluation} {Conference}},
	author = {Bougouin, Adrien and Barreaux, Sabine and Romary, Laurent and Boudin, Florian and Daille, Béatrice},
	month = may,
	year = {2016},
	keywords = {dataset},
	file = {HAL PDF Full Text:/home/gallina-y/Zotero/storage/IDA35JRX/Bougouin et al. - 2016 - TermITH-Eval a French Standard-Based Resource for.pdf:application/pdf},
}

@inproceedings{wan_single_2008,
	address = {Chicago, Illinois},
	series = {{AAAI}'08},
	title = {Single {Document} {Keyphrase} {Extraction} {Using} {Neighborhood} {Knowledge}},
	isbn = {978-1-57735-368-3},
	url = {http://dl.acm.org/citation.cfm?id=1620163.1620205},
	abstract = {Existing methods for single document keyphrase extraction usually make use of only the information contained in the specified document. This paper proposes to use a small number of nearest neighbor documents to provide more knowledge to improve single document keyphrase extraction. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents. Experimental results demonstrate the good effectiveness and robustness of our proposed approach.},
	urldate = {2019-01-27},
	booktitle = {Proceedings of the 23rd {National} {Conference} on {Artificial} {Intelligence} - {Volume} 2},
	publisher = {AAAI Press},
	author = {Wan, Xiaojun and Xiao, Jianguo},
	year = {2008},
	keywords = {graph, kpe},
	pages = {855--860},
	file = {Wan and Xiao - Single Document Keyphrase Extraction Using Neighbo.pdf:/home/gallina-y/Zotero/storage/8WLNZJVG/Wan and Xiao - Single Document Keyphrase Extraction Using Neighbo.pdf:application/pdf},
}

@book{gollapalli_proceedings_2015,
	address = {Beijing, China},
	title = {Proceedings of the {ACL} 2015 {Workshop} on {Novel} {Computational} {Approaches} to {Keyphrase} {Extraction}},
	url = {http://www.aclweb.org/anthology/W15-36},
	urldate = {2019-01-27},
	publisher = {Association for Computational Linguistics},
	editor = {Gollapalli, Sujatha Das and Caragea, Cornelia and Li, Xiaoli and Giles, C. Lee},
	month = jul,
	year = {2015},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/JE9EZBNZ/Gollapalli et al. - 2015 - Proceedings of the ACL 2015 Workshop on Novel Comp.pdf:application/pdf},
}

@inproceedings{tuarob_twittdict:_2015,
	address = {Beijing, China},
	title = {{TwittDict}: {Extracting} {Social} {Oriented} {Keyphrase} {Semantics} from {Twitter}},
	shorttitle = {{TwittDict}},
	url = {http://www.aclweb.org/anthology/W15-3606},
	urldate = {2019-01-27},
	booktitle = {Proceedings of the {ACL} 2015 {Workshop} on {Novel} {Computational} {Approaches} to {Keyphrase} {Extraction}},
	publisher = {Association for Computational Linguistics},
	author = {Tuarob, Suppawong and Chu, Wanghuan and Chen, Dong and Tucker, Conrad},
	month = jul,
	year = {2015},
	pages = {25--31},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/AYC4W7IP/Tuarob et al. - 2015 - TwittDict Extracting Social Oriented Keyphrase Se.pdf:application/pdf},
}

@inproceedings{nakov_web_2015,
	address = {Beijing, China},
	title = {The {Web} as an {Implicit} {Training} {Set}: {Application} to {Noun} {Compounds} {Syntax} and {Semantics}},
	shorttitle = {The {Web} as an {Implicit} {Training} {Set}},
	url = {http://www.aclweb.org/anthology/W15-3604},
	urldate = {2019-01-27},
	booktitle = {Proceedings of the {ACL} 2015 {Workshop} on {Novel} {Computational} {Approaches} to {Keyphrase} {Extraction}},
	publisher = {Association for Computational Linguistics},
	author = {Nakov, Preslav},
	month = jul,
	year = {2015},
	pages = {18},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/8XX6I99N/Nakov - 2015 - The Web as an Implicit Training Set Application t.pdf:application/pdf},
}

@inproceedings{norman_technical_2015,
	address = {Beijing, China},
	title = {Technical {Term} {Extraction} {Using} {Measures} of {Neology}},
	url = {http://www.aclweb.org/anthology/W15-3602},
	urldate = {2019-01-27},
	booktitle = {Proceedings of the {ACL} 2015 {Workshop} on {Novel} {Computational} {Approaches} to {Keyphrase} {Extraction}},
	publisher = {Association for Computational Linguistics},
	author = {Norman, Christopher and Aizawa, Akiko},
	month = jul,
	year = {2015},
	pages = {2--9},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/IR8UE9F6/Norman and Aizawa - 2015 - Technical Term Extraction Using Measures of Neolog.pdf:application/pdf},
}

@inproceedings{paul_identification_2015,
	address = {Beijing, China},
	title = {Identification and {Classification} of {Emotional} {Key} {Phrases} from {Psychological} {Texts}},
	url = {http://www.aclweb.org/anthology/W15-3607},
	urldate = {2019-01-27},
	booktitle = {Proceedings of the {ACL} 2015 {Workshop} on {Novel} {Computational} {Approaches} to {Keyphrase} {Extraction}},
	publisher = {Association for Computational Linguistics},
	author = {Paul, Apurba and Das, Dipankar},
	month = jul,
	year = {2015},
	pages = {32--38},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/ESQZRX3B/Paul and Das - 2015 - Identification and Classification of Emotional Key.pdf:application/pdf},
}

@inproceedings{boudin_reducing_2015,
	address = {Beijing, China},
	title = {Reducing {Over}-generation {Errors} for {Automatic} {Keyphrase} {Extraction} using {Integer} {Linear} {Programming}},
	url = {http://www.aclweb.org/anthology/W15-3605},
	urldate = {2019-01-27},
	booktitle = {Proceedings of the {ACL} 2015 {Workshop} on {Novel} {Computational} {Approaches} to {Keyphrase} {Extraction}},
	publisher = {Association for Computational Linguistics},
	author = {Boudin, Florian},
	month = jul,
	year = {2015},
	keywords = {meta},
	pages = {19--24},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/RYAL89SN/Boudin - 2015 - Reducing Over-generation Errors for Automatic Keyp.pdf:application/pdf},
}

@inproceedings{kan_keywords_2015,
	address = {Beijing, China},
	title = {Keywords, phrases, clauses and sentences: topicality, indicativeness and informativeness at scales},
	shorttitle = {Keywords, phrases, clauses and sentences},
	url = {http://www.aclweb.org/anthology/W15-3601},
	urldate = {2019-01-27},
	booktitle = {Proceedings of the {ACL} 2015 {Workshop} on {Novel} {Computational} {Approaches} to {Keyphrase} {Extraction}},
	publisher = {Association for Computational Linguistics},
	author = {Kan, Min-Yen},
	month = jul,
	year = {2015},
	pages = {1},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/T6WCKI6T/Kan - 2015 - Keywords, phrases, clauses and sentences topicali.pdf:application/pdf},
}

@inproceedings{frank_domain-specific_1999,
	title = {Domain-specific keyphrase extraction},
	volume = {Volume 2},
	url = {https://researchcommons.waikato.ac.nz/handle/10289/1508},
	abstract = {Keyphrases are an important means of document summarization, clustering, and topic search. Only a small minority of documents have author-assigned keyphrases, and manually assigning keyphrases to existing documents is very laborious. Therefore it is highly desirable to automate the keyphrase extraction process. This paper shows that a simple procedure for keyphrase extraction based on the naive Bayes learning scheme performs comparably to the state of the art. It goes on to explain how this procedure's performance can be boosted by automatically tailoring the extraction process to the particular document collection at hand. Results on a large collection of technical reports in computer science show that the quality of the extracted keyphrases improves significantly when domain-specific information is exploited.},
	language = {en},
	urldate = {2019-01-28},
	booktitle = {16th {International} {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI} 99)},
	publisher = {Morgan Kaufmann Publishers Inc., San Francisco, CA, USA},
	author = {Frank, Eibe and Paynter, Gordon W. and Witten, Ian H. and Gutwin, Carl and Nevill-Manning, Craig G.},
	year = {1999},
	keywords = {kpe},
	pages = {668--673},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/4WJ9QGGM/Frank et al. - 1999 - Domain-specific keyphrase extraction.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/U6WUL5A6/1508.html:text/html},
}

@inproceedings{bougouin_keyphrase_2016,
	address = {Osaka, Japan},
	title = {Keyphrase {Annotation} with {Graph} {Co}-{Ranking}},
	url = {http://arxiv.org/abs/1611.02007},
	abstract = {Keyphrase annotation is the task of identifying textual units that represent the main content of a document. Keyphrase annotation is either carried out by extracting the most important phrases from a document, keyphrase extraction, or by assigning entries from a controlled domain-specific vocabulary, keyphrase assignment. Assignment methods are generally more reliable. They provide better-formed keyphrases, as well as keyphrases that do not occur in the document. But they are often silent on the contrary of extraction methods that do not depend on manually built resources. This paper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods.},
	urldate = {2019-01-28},
	booktitle = {Proceedings of {COLING} 2016, the 26th {International} {Conference} on {Computational} {Linguistics}: {Technical} {Papers}},
	author = {Bougouin, Adrien and Boudin, Florian and Daille, Béatrice},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02007},
	keywords = {supervised, kpe},
	file = {arXiv\:1611.02007 PDF:/home/gallina-y/Zotero/storage/RN7HAQ2G/Bougouin et al. - 2016 - Keyphrase Annotation with Graph Co-Ranking.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/TFVQ4ZSR/1611.html:text/html},
}

@article{boudin_how_2016,
	title = {How {Document} {Pre}-processing affects {Keyphrase} {Extraction} {Performance}},
	abstract = {The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientiﬁc articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing.},
	language = {en},
	journal = {In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)},
	author = {Boudin, Florian and Mougard, Hugo and Cram, Damien},
	year = {2016},
	keywords = {meta},
	pages = {8},
	file = {Boudin et al. - How Document Pre-processing affects Keyphrase Extr.pdf:/home/gallina-y/Zotero/storage/ZWVPYX9E/Boudin et al. - How Document Pre-processing affects Keyphrase Extr.pdf:application/pdf},
}

@article{sarkar_new_2010,
	title = {A {New} {Approach} to {Keyphrase} {Extraction} {Using} {Neural} {Networks}},
	volume = {7},
	url = {http://arxiv.org/abs/1004.3274},
	abstract = {Keyphrases provide a simple way of describing a document, giving the reader some clues about its contents. Keyphrases can be useful in a various applications such as retrieval engines, browsing interfaces, thesaurus construction, text mining etc.. There are also other tasks for which keyphrases are useful, as we discuss in this paper. This paper describes a neural network based approach to keyphrase extraction from scientific articles. Our results show that the proposed method performs better than some state-of-the art keyphrase extraction approaches.},
	number = {2},
	urldate = {2019-02-01},
	journal = {International Journal of Computer Science Issues},
	author = {Sarkar, Kamal and Nasipuri, Mita and Ghose, Suranjan},
	month = apr,
	year = {2010},
	note = {arXiv: 1004.3274},
	keywords = {kpe},
	file = {arXiv\:1004.3274 PDF:/home/gallina-y/Zotero/storage/GDKZ85HD/Sarkar et al. - 2010 - A New Approach to Keyphrase Extraction Using Neura.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/LZJKAJY9/1004.html:text/html},
}

@inproceedings{gollapalli_extracting_2014,
	title = {Extracting {Keyphrases} from {Research} {Papers} {Using} {Citation} {Networks}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8662},
	abstract = {Keyphrases for a document concisely describe the document using a small set of phrases. Keyphrases were previously shown to improve several document processing and retrieval tasks. In this work, we study keyphrase extraction from research papers by leveraging citation networks. We propose CiteTextRank for keyphrase extraction from research articles, a graph-based algorithm that incorporates evidence from both a document's content as well as the contexts in which the document is referenced within a citation network. Our model obtains significant improvements over the state-of-the-art models for this task. Specifically, on several datasets of research papers, CiteTextRank improves precision at rank 1 by as much as 9-20\% over state-of-the-art baselines.},
	language = {en},
	urldate = {2019-02-01},
	booktitle = {Twenty-{Eighth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Gollapalli, Sujatha Das and Caragea, Cornelia},
	month = jun,
	year = {2014},
	keywords = {graph, kpe},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/GDFU2956/Gollapalli and Caragea - 2014 - Extracting Keyphrases from Research Papers Using C.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/4XLIG524/8662.html:text/html},
}

@article{pagliardini_unsupervised_2018,
	title = {Unsupervised {Learning} of {Sentence} {Embeddings} using {Compositional} n-{Gram} {Features}},
	url = {http://arxiv.org/abs/1703.02507},
	doi = {10.18653/v1/N18-1049},
	abstract = {The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.},
	urldate = {2019-02-01},
	journal = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)},
	author = {Pagliardini, Matteo and Gupta, Prakhar and Jaggi, Martin},
	year = {2018},
	pages = {528--540},
	file = {arXiv\:1703.02507 PDF:/home/gallina-y/Zotero/storage/HWBMAH5N/Pagliardini et al. - 2018 - Unsupervised Learning of Sentence Embeddings using.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/7A7CBA4C/1703.html:text/html},
}

@article{blei_latent_2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1.},
	journal = {Journal of Machine Learning Research},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I. and Lafferty, John},
	year = {2003},
	pages = {2003},
	file = {Citeseer - Full Text PDF:/home/gallina-y/Zotero/storage/PMCXGXUE/Blei et al. - 2003 - Latent dirichlet allocation.pdf:application/pdf;Citeseer - Snapshot:/home/gallina-y/Zotero/storage/HWZ29SQC/summary.html:text/html},
}

@inproceedings{meng_deep_2017,
	title = {Deep keyphrase generation},
	volume = {1},
	copyright = {attached},
	isbn = {978-1-945626-75-3},
	url = {http://d-scholarship.pitt.edu/31824/},
	abstract = {© 2017 Association for Computational Linguistics. Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seqkeyphrase.},
	language = {en},
	urldate = {2019-02-01},
	booktitle = {{ACL} 2017 - 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}, {Proceedings} of the {Conference} ({Long} {Papers})},
	author = {Meng, R. and Zhao, S. and Han, S. and He, D. and Brusilovsky, P. and Chi, Y.},
	month = jan,
	year = {2017},
	keywords = {generation, o2o, code},
	pages = {582--592},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/VJEJP49B/Meng et al. - 2017 - Deep keyphrase generation.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/6K996PIF/31824.html:text/html},
}

@article{daille_indexation_2017,
	title = {Indexation d’articles scientifiques {Présentation} et résultats du défi fouille de textes {DEFT} 2016},
	volume = {17},
	issn = {25163280},
	url = {https://www.openscience.fr/Indexation-d-articles-scientifiques-Presentation-et-resultats-du-defi-fouille},
	doi = {10.21494/ISTE.OP.2018.0209},
	abstract = {This paper presents the 2016 edition of the DEFT text mining challenge. This edition adresses the keyword-based indexing of scientiﬁc papers with the aim of simulating a professional indexer. The corpus is composed of French bibliographic records on four domains: linguistics, information Science, archaeology and chemisty. The results have been evaluated in terms of precision, recall and f-measure computed after stemming upon the reference indexation. MOTS-CLÉS : indexation automatique, mot-clé, domaines de spécialité, articles scientiﬁques, français .},
	language = {fr},
	number = {1},
	urldate = {2019-02-01},
	journal = {Recherche d’information, document et web sémantique},
	author = {Daille, Béatrice and Barreaux, Sabine and Bougouin, Adrien and Boudin, Florian and Cram, Damien and Hazem, Amir},
	year = {2017},
	file = {Daille et al. - 2017 - Indexation d’articles scientifiques Présentation e.pdf:/home/gallina-y/Zotero/storage/TY2CJDDJ/Daille et al. - 2017 - Indexation d’articles scientifiques Présentation e.pdf:application/pdf},
}

@inproceedings{litvak_graph-based_2008,
	address = {Stroudsburg, PA, USA},
	series = {{MMIES} '08},
	title = {Graph-based {Keyword} {Extraction} for {Single}-document {Summarization}},
	isbn = {978-1-905593-51-4},
	url = {http://dl.acm.org/citation.cfm?id=1613172.1613178},
	abstract = {In this paper, we introduce and compare between two novel approaches, supervised and unsupervised, for identifying the keywords to be used in extractive summarization of text documents. Both our approaches are based on the graph-based syntactic representation of text and web documents, which enhances the traditional vector-space model by taking into account some structural document features. In the supervised approach, we train classification algorithms on a summarized collection of documents with the purpose of inducing a keyword identification model. In the unsupervised approach, we run the HITS algorithm on document graphs under the assumption that the top-ranked nodes should represent the document keywords. Our experiments on a collection of benchmark summaries show that given a set of summarized training documents, the supervised classification provides the highest keyword identification accuracy, while the highest F-measure is reached with a simple degree-based ranking. In addition, it is sufficient to perform only the first iteration of HITS rather than running it to its convergence.},
	urldate = {2019-02-08},
	booktitle = {Proceedings of the {Workshop} on {Multi}-source {Multilingual} {Information} {Extraction} and {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Litvak, Marina and Last, Mark},
	year = {2008},
	note = {event-place: Manchester, United Kingdom},
	keywords = {kpe},
	pages = {17--24},
	file = {ACM Full Text PDF:/home/gallina-y/Zotero/storage/5XSFGAYU/Litvak et Last - 2008 - Graph-based Keyword Extraction for Single-document.pdf:application/pdf},
}

@inproceedings{florescu_positionrank:_2017,
	address = {Vancouver, Canada},
	title = {{PositionRank}: {An} {Unsupervised} {Approach} to {Keyphrase} {Extraction} from {Scholarly} {Documents}},
	shorttitle = {{PositionRank}},
	url = {http://aclweb.org/anthology/P17-1102},
	doi = {10.18653/v1/P17-1102},
	abstract = {The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document’s content and can facilitate fast information processing. In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word’s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Speciﬁcally, on several datasets of research papers, PositionRank achieves improvements as high as 29.09\%.},
	language = {en},
	urldate = {2019-02-11},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for           {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Florescu, Corina and Caragea, Cornelia},
	year = {2017},
	keywords = {graph, kpe},
	pages = {1105--1115},
	file = {Florescu et Caragea - 2017 - PositionRank An Unsupervised Approach to Keyphras.pdf:/home/gallina-y/Zotero/storage/UB6VHQLC/Florescu et Caragea - 2017 - PositionRank An Unsupervised Approach to Keyphras.pdf:application/pdf},
}

@techreport{page_pagerank_1999,
	address = {Stanford University},
	title = {The {PageRank} {Citation} {Ranking}: {Bringing} {Order} to the {Web}},
	shorttitle = {The {PageRank} {Citation} {Ranking}},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
	institution = {Computer Science Department},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	year = {1999},
	file = {Citeseer - Full Text PDF:/home/gallina-y/Zotero/storage/XUD6HIN5/Page et al. - 1999 - The PageRank Citation Ranking Bringing Order to t.pdf:application/pdf;Citeseer - Snapshot:/home/gallina-y/Zotero/storage/W8JBBPCC/summary.html:text/html},
}

@inproceedings{wang_using_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Word} {Embeddings} to {Enhance} {Keyword} {Identification} for {Scientific} {Publications}},
	isbn = {978-3-319-19548-3},
	abstract = {Automatic keyword identification is a desirable but difficult task. It requires considerations of not only the extraction of important words or phrases from a text, but also the generation of abstractive ones that do not appear in the text. In this paper, we propose an approach that uses word embedding vectors as an external knowledge base for both keyword extraction and generation. Our evaluation shows that our approach outperforms many baseline algorithms, and is comparable to the state-of-the-art algorithm on our chosen dataset. In addition, we also introduce a new approach for evaluating the task of keyword extraction, that overcomes a common problem of overly strict matching criteria. We show that using word embedding vectors is a simpler, yet effective, method for both keyword extraction and generation.},
	language = {en},
	booktitle = {Databases {Theory} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Wang, Rui and Liu, Wei and McDonald, Chris},
	editor = {Sharaf, Mohamed A. and Cheema, Muhammad Aamir and Qi, Jianzhong},
	year = {2015},
	keywords = {generation, kpe},
	pages = {257--268},
	file = {Wang et al. - 2015 - Using Word Embeddings to Enhance Keyword Identific.pdf:/home/gallina-y/Zotero/storage/XFPG7Y6W/Wang et al. - 2015 - Using Word Embeddings to Enhance Keyword Identific.pdf:application/pdf},
}

@book{harpring_introduction_2010,
	edition = {Getty Research Institute},
	title = {Introduction to {Controlled} {Vocabularies}: {Terminology} for {Art}, {Architecture}, and {Other} {Cultural} {Works} {Updated} {Edition}},
	shorttitle = {Introduction to {Controlled} {Vocabularies}},
	url = {http://www.getty.edu/research/publications/electronic_publications/intro_controlled_vocab/pdf.html},
	abstract = {Patricia Harpring Series edited by This primer on the characteristics, scope, uses, and methods for building and maintaining controlled vocabularies for art and cultural materials explains how vocabularies should be integrated in cataloging systems; utilized for indexing and retrieval; and struc},
	language = {en},
	urldate = {2019-02-15},
	publisher = {Murtha Baca},
	author = {Harpring, Patricia},
	year = {2010},
	file = {Documents:/home/gallina-y/Zotero/storage/W2CMXAGC/pdf.html:text/html;Snapshot:/home/gallina-y/Zotero/storage/ZYVYGB4J/introduction-to-controlled-vocabularies-terminology-for-art-architecture-and-other-cultural-wor.html:text/html},
}

@inproceedings{daille_morphological_2000,
	address = {Stroudsburg, PA, USA},
	series = {{COLING} '00},
	title = {Morphological {Rule} {Induction} for {Terminology} {Acquistion}},
	isbn = {978-1-55860-717-0},
	url = {https://doi.org/10.3115/990820.990852},
	doi = {10.3115/990820.990852},
	abstract = {We present the identification in corpora of French relational adjectives (RAdj) such as gazeux (gascous) which is derived from the noun gaz (gas). RAdj appearing in nominal phrases are interesting for terminology acquisition because they hold a naming function. The derivational rules employed to compute the noun from which has been derived the RAdj are acquired semi-automatically from a tagged and a lemmatized corpora. These rules are then integrated into a termer which identifies RAdj thanks to their property of being paraphrasable by a prepositional phrase. RAdj and compound nouns which include a RAdj are then quantified, their linguistic precision is measured and their informative status is evaluated thanks to a thesaurus of the domain.},
	urldate = {2019-02-15},
	booktitle = {Proceedings of the 18th {Conference} on {Computational} {Linguistics} - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Daille, Béatrice},
	year = {2000},
	note = {event-place: Saarbrücken, Germany},
	pages = {215--221},
	file = {ACM Full Text PDF:/home/gallina-y/Zotero/storage/Y6LNLHAY/Daille - 2000 - Morphological Rule Induction for Terminology Acqui.pdf:application/pdf},
}

@inproceedings{hulth_study_2006,
	address = {Stroudsburg, PA, USA},
	series = {{ACL}-44},
	title = {A {Study} on {Automatically} {Extracted} {Keywords} in {Text} {Categorization}},
	url = {https://doi.org/10.3115/1220175.1220243},
	doi = {10.3115/1220175.1220243},
	abstract = {This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization. In summary we show that a higher performance --- as measured by micro-averaged F-measure on a standard text categorization collection --- is achieved when the full-text representation is combined with the automatically extracted keywords. The combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords. We also present results for experiments in which the keywords are the only input to the categorizer, either represented as unigrams or intact. Of these two experiments, the unigrams have the best performance, although neither performs as well as headlines only.},
	urldate = {2019-02-15},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Computational} {Linguistics} and the 44th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hulth, Anette and Megyesi, Beáta B.},
	year = {2006},
	note = {event-place: Sydney, Australia},
	pages = {537--544},
	file = {ACM Full Text PDF:/home/gallina-y/Zotero/storage/N92VQ54V/Hulth et Megyesi - 2006 - A Study on Automatically Extracted Keywords in Tex.pdf:application/pdf},
}

@book{witten_how_2009,
	address = {San Francisco, CA, USA},
	edition = {2nd},
	title = {How to {Build} a {Digital} {Library}, {Second} {Edition}},
	isbn = {978-0-12-374857-7},
	abstract = {How to Build a Digital Library is the only book that offers all the knowledge and tools needed to construct and maintain a digital library, regardless of the size or purpose. It is the perfectly self-contained resource for individuals, agencies, and institutions wishing to put this powerful tool to work in their burgeoning information treasuries. The Second Edition reflects new developments in the field as well as in the Greenstone Digital Library open source software. In Part I, the authors have added an entire new chapter on user groups, user support, collaborative browsing, user contributions, and so on. There is also new material on content-based queries, map-based queries, cross-media queries. There is an increased emphasis placed on multimedia by adding a "digitizing" section to each major media type. A new chapter has also been added on "internationalization," which will address Unicode standards, multi-language interfaces and collections, and issues with non-European languages (Chinese, Hindi, etc.). Part II, the software tools section, has been completely rewritten to reflect the new developments in Greenstone Digital Library Software, an internationally popular open source software tool with a comprehensive graphical facility for creating and maintaining digital libraries. As with the First Edition, a web site, implemented as a digital library, will accompany the book and provide access to color versions of all figures, two online appendices, a full-text sentence-level index, and an automatically generated glossary of acronyms and their definitions. In addition, demonstration digital library collections will be included to demonstrate particular points in the book. to access the online content please visit, http://www.greenstone.org/howto *Outlines the history of libraries-- both traditional and digital-- and their impact on present practices and future directions.*Written for both technical and non-technical audiences and covers the entire spectrum of media, including text, images, audio, video, and related XML standards.*Web-enhanced with software documentation, color illustrations, full-text index, source code, and more.},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Witten, Ian H. and Bainbridge, David and Nichols, David M.},
	year = {2009},
}

@inproceedings{evans_noun-phrase_1996,
	address = {Stroudsburg, PA, USA},
	series = {{ACL} '96},
	title = {Noun-phrase {Analysis} in {Unrestricted} {Text} for {Information} {Retrieval}},
	url = {https://doi.org/10.3115/981863.981866},
	doi = {10.3115/981863.981866},
	abstract = {Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text. This paper reports on the application of a few simple, yet robust and efficient noun-phrase analysis techniques to create better indexing phrases for information retrieval. In particular, we describe an hybrid approach to the extraction of meaningful (continuous or discontinuous) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics. Results of experiments show that indexing based on such extracted subcompound improves both recall and precision in an information retrieval system. The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction.},
	urldate = {2019-02-15},
	booktitle = {Proceedings of the 34th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Evans, David A. and Zhai, Chengxiang},
	year = {1996},
	note = {event-place: Santa Cruz, California},
	pages = {17--24},
	file = {ACM Full Text PDF:/home/gallina-y/Zotero/storage/PWRW642P/Evans et Zhai - 1996 - Noun-phrase Analysis in Unrestricted Text for Info.pdf:application/pdf},
}

@inproceedings{hamid_rezatofighi_deepsetnet:_2017,
	title = {{DeepSetNet}: {Predicting} {Sets} {With} {Deep} {Neural} {Networks}},
	shorttitle = {{DeepSetNet}},
	url = {http://openaccess.thecvf.com/content_iccv_2017/html/Rezatofighi_DeepSetNet_Predicting_Sets_ICCV_2017_paper.html},
	urldate = {2019-04-04},
	author = {Hamid Rezatofighi, S. and Kumar B G, Vijay and Milan, Anton and Abbasnejad, Ehsan and Dick, Anthony and Reid, Ian},
	year = {2017},
	note = {Train a second model to ouput a number which will be use as a k for Top-k},
	keywords = {set},
	pages = {5247--5256},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/Q35IGTWB/Hamid Rezatofighi et al. - 2017 - DeepSetNet Predicting Sets With Deep Neural Netwo.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/HPTZRUPX/Rezatofighi_DeepSetNet_Predicting_Sets_ICCV_2017_paper.html:text/html},
}

@incollection{zaheer_deep_2017,
	title = {Deep {Sets}},
	url = {http://papers.nips.cc/paper/6931-deep-sets.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan R and Smola, Alexander J},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	note = {Set as input},
	keywords = {set},
	pages = {3391--3401},
	file = {6931-deep-sets.pdf:/home/gallina-y/Zotero/storage/XM4FWWJ6/6931-deep-sets.pdf:application/pdf},
}

@article{rezatofighi_deep_2018,
	title = {Deep {Perm}-{Set} {Net}: {Learn} to predict sets with unknown permutation and cardinality using deep neural networks},
	shorttitle = {Deep {Perm}-{Set} {Net}},
	url = {http://arxiv.org/abs/1805.00613},
	abstract = {Many real-world problems, e.g. object detection, have outputs that are naturally expressed as sets of entities. This creates a challenge for traditional deep neural networks which naturally deal with structured outputs such as vectors, matrices or tensors. We present a novel approach for learning to predict sets with unknown permutation and cardinality using deep neural networks. Speciﬁcally, in our formulation we incorporate the permutation as unobservable variable and estimate its distribution during the learning process using alternating optimization. We demonstrate the validity of this new formulation on two relevant vision problems: object detection, for which our formulation outperforms state-of-the-art detectors such as Faster R-CNN and YOLO, and a complex CAPTCHA test, where we observe that, surprisingly, our set based network acquired the ability of mimicking arithmetics without any rules being coded.},
	language = {en},
	urldate = {2019-04-04},
	journal = {arXiv:1805.00613 [cs]},
	author = {Rezatofighi, S. Hamid and Kaskman, Roman and Motlagh, Farbod T. and Shi, Qinfeng and Cremers, Daniel and Leal-Taixé, Laura and Reid, Ian},
	month = may,
	year = {2018},
	note = {arXiv: 1805.00613},
	keywords = {set},
	file = {Rezatofighi et al. - 2018 - Deep Perm-Set Net Learn to predict sets with unkn.pdf:/home/gallina-y/Zotero/storage/86VK78LA/Rezatofighi et al. - 2018 - Deep Perm-Set Net Learn to predict sets with unkn.pdf:application/pdf},
}

@article{bello_seq2slate:_2018,
	title = {{Seq2Slate}: {Re}-ranking and {Slate} {Optimization} with {RNNs}},
	shorttitle = {{Seq2Slate}},
	url = {http://arxiv.org/abs/1810.02019},
	abstract = {Ranking is a central task in machine learning and information retrieval. In this task, it is especially important to present the user with a slate of items that is appealing as a whole. This in turn requires taking into account interactions between items, since intuitively, placing an item on the slate affects the decision of which other items should be placed alongside it. In this work, we propose a sequence-to-sequence model for ranking called seq2slate. At each step, the model predicts the next “best” item to place on the slate given the items already selected. The sequential nature of the model allows complex dependencies between the items to be captured directly in a ﬂexible and scalable way. We show how to learn the model end-to-end from weak supervision in the form of easily obtained click-through data. We further demonstrate the usefulness of our approach in experiments on standard ranking benchmarks as well as in a real-world recommendation system.},
	language = {en},
	urldate = {2019-04-04},
	journal = {arXiv:1810.02019 [cs, stat]},
	author = {Bello, Irwan and Kulkarni, Sayali and Jain, Sagar and Boutilier, Craig and Chi, Ed and Eban, Elad and Luo, Xiyang and Mackey, Alan and Meshi, Ofer},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.02019},
	keywords = {set},
	file = {Bello et al. - 2018 - Seq2Slate Re-ranking and Slate Optimization with .pdf:/home/gallina-y/Zotero/storage/PRW5SHA5/Bello et al. - 2018 - Seq2Slate Re-ranking and Slate Optimization with .pdf:application/pdf},
}

@article{asai_set_2018,
	title = {Set {Cross} {Entropy}: {Likelihood}-based {Permutation} {Invariant} {Loss} {Function} for {Probability} {Distributions}},
	shorttitle = {Set {Cross} {Entropy}},
	url = {http://arxiv.org/abs/1812.01217},
	abstract = {We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics deﬁned for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.},
	language = {en},
	urldate = {2019-04-04},
	journal = {arXiv:1812.01217 [cs, stat]},
	author = {Asai, Masataro},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01217},
	keywords = {set},
	file = {Asai - 2018 - Set Cross Entropy Likelihood-based Permutation In.pdf:/home/gallina-y/Zotero/storage/ZLBR4LRL/Asai - 2018 - Set Cross Entropy Likelihood-based Permutation In.pdf:application/pdf},
}

@article{ow_filtered_1988,
	title = {Filtered beam search in scheduling},
	volume = {26},
	issn = {0020-7543},
	url = {https://doi.org/10.1080/00207548808947840},
	doi = {10.1080/00207548808947840},
	abstract = {Beam search is a technique for searching decision trees, particularly where the solution space is vast. The technique involves systematically developing a small number of solutions in parallel so as to attempt to maximize the probability of finding a good solution with minimal search effort. In this paper, we systematically study the performance behaviour of beam search with other heuristic methods for scheduling, and the effects of using different evaluation functions to guide the search. We also develop a new variation of beam search, called filtered beam search which is computationally simple yet produces high quality solutions.},
	number = {1},
	urldate = {2019-04-24},
	journal = {International Journal of Production Research},
	author = {Ow, Peng Si and Morton, Thomas E.},
	month = jan,
	year = {1988},
	pages = {35--62},
	file = {Snapshot:/home/gallina-y/Zotero/storage/GTRMWQHW/00207548808947840.html:text/html},
}

@inproceedings{rezatofighi_joint_2018,
	title = {Joint {Learning} of {Set} {Cardinality} and {State} {Distribution}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16356},
	abstract = {We present a novel approach for learning to predict sets using deep learning. In recent years, deep neural networks have shown remarkable results in computer vision, natural language processing and other related problems. Despite their success,traditional architectures suffer from a serious limitation in that they are built to deal with structured input and output data,i.e. vectors or matrices. Many real-world problems, however, are naturally described as sets, rather than vectors. Existing techniques that allow for sequential data, such as recurrent neural networks, typically heavily depend on the input and output order and do not guarantee a valid solution. Here, we derive in a principled way, a mathematical formulation for set prediction where the output is permutation invariant. In particular, our approach jointly learns both the cardinality and the state distribution of the target set. We demonstrate the validity of our method on the task of multi-label image classification and achieve a new state of the art on the PASCAL VOC and MS COCO datasets.},
	language = {en},
	urldate = {2019-05-02},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Rezatofighi, S. Hamid and Milan, Anton and Shi, Qinfeng and Dick, Anthony and Reid, Ian},
	month = apr,
	year = {2018},
	keywords = {set},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/6STTW23Z/Rezatofighi et al. - 2018 - Joint Learning of Set Cardinality and State Distri.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/B6E7GFHS/16356.html:text/html},
}

@inproceedings{zhang_deep_2017,
	title = {Deep keyphrase generation with a convolutional sequence to sequence model},
	doi = {10.1109/ICSAI.2017.8248519},
	abstract = {Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas. Most previous studies realize the automatic keyphrase extraction through dividing the source text into multiple chunks and then rank and select the most suitable ones. These approaches ignore the deep semantics behind the text and could not predict the keyphrases not appearing in the source text. A sequence to sequence model to generate keyphrases from vocabulary could solve the issues above. However, traditional sequence to sequence model based on recurrent neural network(RNN) suffers from low efficiency problem. We propose an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations can be completely parallelized over all elements so as to better exploit the GPU hardware. Our use of gated linear units alleviates gradient propagation and we equip each decoder layer with a separate attention model. Moreover, we incorporate a copying mechanism to handle out-of-vocabulary phrases. In experiments, we evaluate our model on six datasets, and our proposed model is demonstrated to outperform state-of-the-art baseline models consistently and significantly, both on extracting the keyphrases existing in the source text and generating the absent keyphrases based on the sematic meaning of the text.},
	booktitle = {2017 4th {International} {Conference} on {Systems} and {Informatics} ({ICSAI})},
	author = {Zhang, Y. and Fang, Y. and Weidong, X.},
	month = nov,
	year = {2017},
	keywords = {generation, o2o},
	pages = {1477--1485},
	file = {IEEE Xplore Abstract Record:/home/gallina-y/Zotero/storage/UTIYPR9H/8248519.html:text/html;IEEE Xplore Full Text PDF:/home/gallina-y/Zotero/storage/CERI86LI/Zhang et al. - 2017 - Deep keyphrase generation with a convolutional seq.pdf:application/pdf},
}

@inproceedings{le_unsupervised_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Keyphrase} {Extraction}: {Introducing} {New} {Kinds} of {Words} to {Keyphrases}},
	isbn = {978-3-319-50127-7},
	shorttitle = {Unsupervised {Keyphrase} {Extraction}},
	abstract = {Current studies often extract keyphrases by collecting adjacent important adjectives and nouns. However, the statistics on four public corpora shows that about 15\% of keyphrases contain other kinds of words. Even so, incorporating such kinds of words to the noun phrase patterns is not a solution to improve the extraction performance. In this work, we propose a solution to improve the extraction performance by involving new kinds of words to keyphrases. We have experimented on four public corpora to demonstrate that our proposal improve the performance of keyphrase extraction and new kinds of words are introduced to keyphrases. In addition, our proposal is also superior to the current unsupervised keyphrase extraction approaches.},
	language = {en},
	booktitle = {{AI} 2016: {Advances} in {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Le, Tho Thi Ngoc and Nguyen, Minh Le and Shimazu, Akira},
	editor = {Kang, Byeong Ho and Bai, Quan},
	year = {2016},
	keywords = {kpe},
	pages = {665--671},
}

@inproceedings{wang_ptr:_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{PTR}: {Phrase}-{Based} {Topical} {Ranking} for {Automatic} {Keyphrase} {Extraction} in {Scientific} {Publications}},
	isbn = {978-3-319-46681-1},
	shorttitle = {{PTR}},
	abstract = {Automatic keyphrase extraction plays an important role for many information retrieval (IR) and natural language processing (NLP) tasks. Motivated by the facts that phrases have more semantic information than single words and a document consists of multiple semantic topics, we present PTR, a phrase-based topical ranking method for keyphrase extraction in scientific publications. Candidate keyphrases are divided into different topics by LDA and used as vertices in a phrase-based graph of the topic. We then decompose PageRank into multiple weighted-PageRank to rank phrases for each topic. Keyphrases are finally generated by selecting candidates according to their overall scores on all related topics. Experimental results show that PTR has good performance on several datasets.},
	language = {en},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer International Publishing},
	author = {Wang, Minmei and Zhao, Bo and Huang, Yihua},
	editor = {Hirose, Akira and Ozawa, Seiichi and Doya, Kenji and Ikeda, Kazushi and Lee, Minho and Liu, Derong},
	year = {2016},
	keywords = {lda, kpe},
	pages = {120--128},
}

@inproceedings{luan_scientific_2017,
	address = {Copenhagen, Denmark},
	title = {Scientific {Information} {Extraction} with {Semi}-supervised {Neural} {Tagging}},
	url = {http://aclweb.org/anthology/D17-1279},
	doi = {10.18653/v1/D17-1279},
	abstract = {This paper addresses the problem of extracting keyphrases from scientiﬁc articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.},
	language = {en},
	urldate = {2019-06-11},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Luan, Yi and Ostendorf, Mari and Hajishirzi, Hannaneh},
	year = {2017},
	pages = {2641--2651},
	file = {Luan et al. - 2017 - Scientific Information Extraction with Semi-superv.pdf:/home/gallina-y/Zotero/storage/D8VQQJIZ/Luan et al. - 2017 - Scientific Information Extraction with Semi-superv.pdf:application/pdf},
}

@article{yuan_generating_2018,
	title = {Generating {Diverse} {Numbers} of {Diverse} {Keyphrases}},
	url = {http://arxiv.org/abs/1810.05241},
	abstract = {Existing keyphrase generation studies suffer from the problems of generating duplicate phrases and deficient evaluation based on a fixed number of predicted phrases. We propose a recurrent generative model that generates multiple keyphrases sequentially from a text, with specific modules that promote generation diversity. We further propose two new metrics that consider a variable number of phrases. With both existing and proposed evaluation setups, our model demonstrates superior performance to baselines on three types of keyphrase generation datasets, including two newly introduced in this work: StackExchange and TextWorld ACG. In contrast to previous keyphrase generation approaches, our model generates sets of diverse keyphrases of a variable number.},
	urldate = {2019-06-11},
	journal = {arXiv:1810.05241 [cs]},
	author = {Yuan, Xingdi and Wang, Tong and Meng, Rui and Thaker, Khushboo and He, Daqing and Trischler, Adam},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.05241},
	keywords = {generation, o2m},
	file = {arXiv\:1810.05241 PDF:/home/gallina-y/Zotero/storage/IPVZCUZ8/Yuan et al. - 2018 - Generating Diverse Numbers of Diverse Keyphrases.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/EWMH8GA2/1810.html:text/html},
}

@inproceedings{yu_wikirank:improving_2018,
	title = {{WikiRank}:{Improving} {Keyphrase} {Extraction} {Based} on {Background} {Knowledge}},
	abstract = {Keyphrase is an efﬁcient representation of the main idea of documents. While background knowledge can provide valuable information about documents, they are rarely incorporated in keyphrase extraction methods. In this paper, we propose WikiRank, an unsupervised method for keyphrase extraction based on the background knowledge from Wikipedia. Firstly, we construct a semantic graph for the document. Then we transform the keyphrase extraction problem into an optimization problem on the graph. Finally, we get the optimal keyphrase set to be the output. Our method obtains improvements over other state-of-art models by more than 2\% in F1-score.},
	language = {en},
	author = {Yu, Yang and Ng, Vincent},
	year = {2018},
	keywords = {graph, kpe},
	pages = {5},
	file = {Yu et Ng - WikiRankImproving Keyphrase Extraction Based on B.pdf:/home/gallina-y/Zotero/storage/T2L9WQKQ/Yu et Ng - WikiRankImproving Keyphrase Extraction Based on B.pdf:application/pdf},
}

@inproceedings{huang_towards_2007,
	address = {Banff, Alberta, Canada},
	series = {{WWW} '07},
	title = {Towards multi-granularity multi-facet e-book retrieval},
	isbn = {978-1-59593-654-7},
	url = {http://portal.acm.org/citation.cfm?doid=1242572.1242835},
	doi = {10.1145/1242572.1242835},
	abstract = {Generally speaking, digital libraries have multiple granularities of semantic units: book, chapter, page, paragraph and word. However, there are two limitations of current eBook retrieval systems: (1) the granularity of retrievable units is either too big or too small, scales such as chapters, paragraphs are ignored; (2) the retrieval results should be grouped by facets to facilitate user’s browsing and exploration. To overcome these limitations, we propose a multi-granularity multi-facet eBook retrieval approach.},
	language = {en},
	urldate = {2019-06-28},
	booktitle = {Proceedings of the 16th international conference on {World} {Wide} {Web}},
	publisher = {ACM Press},
	author = {Huang, Chong and Tian, Yonghong and Zhou, Zhi and Huang, Tiejun},
	year = {2007},
	pages = {1331},
	file = {Huang et al. - 2007 - Towards multi-granularity multi-facet e-book retri.pdf:/home/gallina-y/Zotero/storage/Q9TQBU5J/Huang et al. - 2007 - Towards multi-granularity multi-facet e-book retri.pdf:application/pdf},
}

@inproceedings{he_keyphrase_2018,
	address = {Fort Worth, Texas, USA},
	title = {Keyphrase {Extraction} {Based} on {Prior} {Knowledge}},
	isbn = {978-1-4503-5178-2},
	url = {http://dl.acm.org/citation.cfm?doid=3197026.3203869},
	doi = {10.1145/3197026.3203869},
	abstract = {Keyphrase is an important way to quickly get the topic of a document by providing highly-summative information. The previous approaches for keyphrase extraction simply rank keyphrases according to statistics-based model or graph-based model, which ignore the influence of external knowledge. In this paper, we take prior knowledge, which contains controlled vocabulary of keyphrases and their prior probability, into consideration to enhance previous methods. First, we build a controlled vocabulary of keyphrases introduced by keyphrases from existing collections and a keyphrase candidate set is filtered from a given document by it. Then, we use prior probability to represent the importance of keyphrases candidate with TF-IDF and TextRank. Finally, a supervised learning algorithm is used to learn optimal weights of these three features. Experiments on four benchmark datasets show the great advantages of prior knowledge on keyphrase extraction. Furthermore, we achieve competitive performance compared with the state-of-the art methods.},
	language = {en},
	urldate = {2019-06-28},
	booktitle = {Proceedings of the 18th {ACM}/{IEEE} on {Joint} {Conference} on {Digital} {Libraries}  - {JCDL} '18},
	publisher = {ACM Press},
	author = {He, Guoxiu and Fang, Junwei and Cui, Haoran and Wu, Chuan and Lu, Wei},
	year = {2018},
	keywords = {kpe},
	pages = {341--342},
	file = {He et al. - 2018 - Keyphrase Extraction Based on Prior Knowledge.pdf:/home/gallina-y/Zotero/storage/2XKL3RWK/He et al. - 2018 - Keyphrase Extraction Based on Prior Knowledge.pdf:application/pdf},
}

@inproceedings{gu_incorporating_2016,
	address = {Berlin, Germany},
	title = {Incorporating {Copying} {Mechanism} in {Sequence}-to-{Sequence} {Learning}},
	url = {http://aclweb.org/anthology/P16-1154},
	doi = {10.18653/v1/P16-1154},
	abstract = {We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure. COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efﬁcacy of COPYNET. For example, COPYNET can outperform regular RNN-based model with remarkable margins on text summarization tasks.},
	language = {en},
	urldate = {2019-07-09},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O.K.},
	year = {2016},
	pages = {1631--1640},
	file = {Gu et al. - 2016 - Incorporating Copying Mechanism in Sequence-to-Seq.pdf:/home/gallina-y/Zotero/storage/KA6EUIWF/Gu et al. - 2016 - Incorporating Copying Mechanism in Sequence-to-Seq.pdf:application/pdf},
}

@article{papagiannopoulou_review_2019,
	title = {A {Review} of {Keyphrase} {Extraction}},
	url = {http://arxiv.org/abs/1905.05044},
	abstract = {Automated keyphrase extraction is a crucial textual information processing task regarding the most types of digital content management systems. It concerns the selection of representative and characteristic phrases from a document that express all aspects related to its content. This article introduces the task of keyphrase extraction and provides a view of existing work that is well organized and comprehensive. Moreover, it discusses the different evaluation approaches giving meaningful insights and highlighting open issues. Finally, a comparative experimental study for popular unsupervised techniques on five datasets is presented.},
	language = {en},
	urldate = {2019-07-15},
	journal = {arXiv:1905.05044 [cs]},
	author = {Papagiannopoulou, Eirini and Tsoumakas, Grigorios},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05044},
	keywords = {meta},
	file = {Papagiannopoulou et Tsoumakas - 2019 - A Review of Keyphrase Extraction.pdf:/home/gallina-y/Zotero/storage/TBT3SZKN/Papagiannopoulou et Tsoumakas - 2019 - A Review of Keyphrase Extraction.pdf:application/pdf},
}

@inproceedings{vijayakumar_diverse_2018,
	title = {Diverse {Beam} {Search}: {Decoding} {Diverse} {Solutions} from {Neural} {Sequence} {Models}},
	abstract = {Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates – resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method ﬁnds better top-1 solutions by controlling for the exploration and exploitation of the search space – implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Further, we study the role of diversity for image-grounded language generation tasks as the complexity of the image changes. We observe that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.},
	language = {en},
	author = {Vijayakumar, Ashwin K and Cogswell, Michael and Selvaraju, Ramprasath R and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
	year = {2018},
	keywords = {diverse decoding},
	pages = {16},
	file = {Vijayakumar et al. - DIVERSE BEAM SEARCH DECODING DIVERSE SOLUTIONS FR.pdf:/home/gallina-y/Zotero/storage/CLSH6VLG/Vijayakumar et al. - DIVERSE BEAM SEARCH DECODING DIVERSE SOLUTIONS FR.pdf:application/pdf},
}

@article{li_mutual_2016,
	title = {Mutual {Information} and {Diverse} {Decoding} {Improve} {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1601.00372},
	abstract = {Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., p(y{\textbar}x), an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the ﬁrst pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures.},
	language = {en},
	urldate = {2019-11-13},
	journal = {arXiv:1601.00372 [cs]},
	author = {Li, Jiwei and Jurafsky, Dan},
	month = mar,
	year = {2016},
	note = {arXiv: 1601.00372},
	keywords = {diverse decoding},
	file = {Li and Jurafsky - 2016 - Mutual Information and Diverse Decoding Improve Ne.pdf:/home/gallina-y/Zotero/storage/3X85G5KT/Li and Jurafsky - 2016 - Mutual Information and Diverse Decoding Improve Ne.pdf:application/pdf},
}

@article{li_simple_2016,
	title = {A {Simple}, {Fast} {Diverse} {Decoding} {Algorithm} for {Neural} {Generation}},
	url = {http://arxiv.org/abs/1611.08562},
	abstract = {In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed. We further propose a variation that is capable of automatically adjusting its diversity decoding rates for different inputs using reinforcement learning (RL). We observe a further performance boost from this RL technique. This paper includes material from the unpublished script "Mutual Information and Diverse Decoding Improve Neural Machine Translation" (Li and Jurafsky, 2016).},
	language = {en},
	urldate = {2019-11-13},
	journal = {arXiv:1611.08562 [cs]},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	month = dec,
	year = {2016},
	note = {arXiv: 1611.08562},
	keywords = {diverse decoding},
	file = {Li et al. - 2016 - A Simple, Fast Diverse Decoding Algorithm for Neur.pdf:/home/gallina-y/Zotero/storage/BVMJQNLT/Li et al. - 2016 - A Simple, Fast Diverse Decoding Algorithm for Neur.pdf:application/pdf},
}

@article{chen_title-guided_2019,
	title = {Title-{Guided} {Encoding} for {Keyphrase} {Generation}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4587},
	doi = {10.1609/aaai.v33i01.33016268},
	abstract = {Keyphrase generation (KG) aims to generate a set of keyphrases given a document, which is a fundamental task in natural language processing (NLP). Most previous methods solve this problem in an extractive manner, while recently, several attempts are made under the generative setting using deep neural networks. However, the state-of-the-art generative methods simply treat the document title and the document main body equally, ignoring the leading role of the title to the overall document. To solve this problem, we introduce a new model called Title-Guided Network (TG-Net) for automatic keyphrase generation task based on the encoderdecoder architecture with two new features: (i) the title is additionally employed as a query-like input, and (ii) a titleguided encoder gathers the relevant information from the title to each word in the document. Experiments on a range of KG datasets demonstrate that our model outperforms the state-of-the-art models with a large margin, especially for documents with either very low or very high title length ratios.},
	language = {en},
	number = {01},
	urldate = {2020-02-07},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Wang and Gao, Yifan and Zhang, Jiani and King, Irwin and Lyu, Michael R.},
	month = jul,
	year = {2019},
	keywords = {generation, o2o},
	pages = {6268--6275},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/WBRG6GQ3/Chen et al. - 2019 - Title-Guided Encoding for Keyphrase Generation.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/6KS7DST9/4587.html:text/html},
}

@inproceedings{bengio_curriculum_2009,
	address = {Montreal, Quebec, Canada},
	title = {Curriculum learning},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
	doi = {10.1145/1553374.1553380},
	abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them “curriculum learning”. In the context of recent research studying the diﬃculty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that signiﬁcant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an eﬀect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
	language = {en},
	urldate = {2019-09-02},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
	year = {2009},
	pages = {1--8},
	file = {Bengio et al. - 2009 - Curriculum learning.pdf:/home/gallina-y/Zotero/storage/ABTVME9Q/Bengio et al. - 2009 - Curriculum learning.pdf:application/pdf},
}

@incollection{sukhbaatar_end--end_2015,
	title = {End-{To}-{End} {Memory} {Networks}},
	url = {http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf},
	urldate = {2019-08-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Sukhbaatar, Sainbayar and szlam, arthur and Weston, Jason and Fergus, Rob},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2440--2448},
	file = {NIPS Full Text PDF:/home/gallina-y/Zotero/storage/V9ERVPPL/Sukhbaatar et al. - 2015 - End-To-End Memory Networks.pdf:application/pdf;NIPS Snapshot:/home/gallina-y/Zotero/storage/DH4BIEI4/5846-end-to-end-memory-networks.html:text/html},
}

@inproceedings{zhao_incorporating_2019,
	address = {Florence, Italy},
	title = {Incorporating {Linguistic} {Constraints} into {Keyphrase} {Generation}},
	url = {https://www.aclweb.org/anthology/P19-1515},
	doi = {10.18653/v1/P19-1515},
	abstract = {Keyphrases, that concisely describe the highlevel topics discussed in a document, are very useful for a wide range of natural language processing tasks. Though existing keyphrase generation methods have achieved remarkable performance on this task, they generate many overlapping phrases (including sub-phrases or super-phrases) of keyphrases. In this paper, we propose the parallel Seq2Seq network with the coverage attention to alleviate the overlapping phrase problem. Speciﬁcally, we integrate the linguistic constraints of keyphrases into the basic Seq2Seq network on the source side, and employ the multi-task learning framework on the target side. In addition, in order to prevent from generating overlapping phrases with correct syntax, we introduce the coverage vector to keep track of the attention history and to decide whether the parts of source text have been covered by existing generated keyphrases. The experimental results show that our method can outperform the state-of-the-art CopyRNN on scientiﬁc datasets, and is also more effective in news domain.},
	language = {en},
	urldate = {2020-02-07},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jing and Zhang, Yuxiang},
	year = {2019},
	keywords = {generation, o2o},
	pages = {5224--5233},
	file = {Zhao et Zhang - 2019 - Incorporating Linguistic Constraints into Keyphras.pdf:/home/gallina-y/Zotero/storage/AVHHS7AF/Zhao et Zhang - 2019 - Incorporating Linguistic Constraints into Keyphras.pdf:application/pdf},
}

@inproceedings{alzaidy_bi-lstm-crf_2019,
	address = {San Francisco, CA, USA},
	title = {Bi-{LSTM}-{CRF} {Sequence} {Labeling} for {Keyphrase} {Extraction} from {Scholarly} {Documents}},
	isbn = {978-1-4503-6674-8},
	url = {http://dl.acm.org/citation.cfm?doid=3308558.3313642},
	doi = {10.1145/3308558.3313642},
	abstract = {In this paper, we address the keyphrase extraction problem as sequence labeling and propose a model that jointly exploits the complementary strengths of Conditional Random Fields that capture label dependencies through a transition parameter matrix consisting of the transition probabilities from one label to the neighboring label, and Bidirectional Long Short Term Memory networks that capture hidden semantics in text through the long distance dependencies. Our results on three datasets of scholarly documents show that the proposed model substantially outperforms strong baselines and previous approaches for keyphrase extraction.},
	language = {en},
	urldate = {2020-02-07},
	booktitle = {The {World} {Wide} {Web} {Conference} on   - {WWW} '19},
	publisher = {ACM Press},
	author = {Alzaidy, Rabah and Caragea, Cornelia and Giles, C. Lee},
	year = {2019},
	keywords = {extraction},
	pages = {2551--2557},
	file = {Alzaidy et al. - 2019 - Bi-LSTM-CRF Sequence Labeling for Keyphrase Extrac.pdf:/home/gallina-y/Zotero/storage/RHEQ8TPP/Alzaidy et al. - 2019 - Bi-LSTM-CRF Sequence Labeling for Keyphrase Extrac.pdf:application/pdf},
}

@article{nogueira_document_2019,
	title = {Document {Expansion} by {Query} {Prediction}},
	url = {http://arxiv.org/abs/1904.08375},
	abstract = {One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents’ content. From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latencycritical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.},
	language = {en},
	urldate = {2020-02-07},
	journal = {arXiv:1904.08375 [cs]},
	author = {Nogueira, Rodrigo and Yang, Wei and Lin, Jimmy and Cho, Kyunghyun},
	month = sep,
	year = {2019},
	note = {arXiv: 1904.08375},
	file = {Nogueira et al. - 2019 - Document Expansion by Query Prediction.pdf:/home/gallina-y/Zotero/storage/KSGYLA3H/Nogueira et al. - 2019 - Document Expansion by Query Prediction.pdf:application/pdf},
}

@article{chen_expertseer_2015,
	title = {{ExpertSeer}: a {Keyphrase} {Based} {Expert} {Recommender} for {Digital} {Libraries}},
	shorttitle = {{ExpertSeer}},
	url = {http://arxiv.org/abs/1511.02058},
	abstract = {We describe ExpertSeer, a generic framework for expert recommendation based on the contents of a digital library. Given a query term q, ExpertSeer recommends experts of q by retrieving authors who published relevant papers determined by related keyphrases and the quality of papers. The system is based on a simple yet effective keyphrase extractor and the Bayes' rule for expert recommendation. ExpertSeer is domain independent and can be applied to different disciplines and applications since the system is automated and not tailored to a specific discipline. Digital library providers can employ the system to enrich their services and organizations can discover experts of interest within an organization. To demonstrate the power of ExpertSeer, we apply the framework to build two expert recommender systems. The first, CSSeer, utilizes the CiteSeerX digital library to recommend experts primarily in computer science. The second, ChemSeer, uses publicly available documents from the Royal Society of Chemistry (RSC) to recommend experts in chemistry. Using one thousand computer science terms as benchmark queries, we compared the top-n experts (n=3, 5, 10) returned by CSSeer to two other expert recommenders -- Microsoft Academic Search and ArnetMiner -- and a simulator that imitates the ranking function of Google Scholar. Although CSSeer, Microsoft Academic Search, and ArnetMiner mostly return prestigious researchers who published several papers related to the query term, it was found that different expert recommenders return moderately different recommendations. To further study their performance, we obtained a widely used benchmark dataset as the ground truth for comparison. The results show that our system outperforms Microsoft Academic Search and ArnetMiner in terms of Precision-at-k (P@k) for k=3, 5, 10. We also conducted several case studies to validate the usefulness of our system.},
	language = {en},
	urldate = {2020-02-10},
	journal = {arXiv:1511.02058 [cs]},
	author = {Chen, Hung-Hsuan and Ororbia II, Alexander G. and Giles, C. Lee},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02058},
	file = {Chen et al. - 2015 - ExpertSeer a Keyphrase Based Expert Recommender f.pdf:/home/gallina-y/Zotero/storage/IMRK3ZER/Chen et al. - 2015 - ExpertSeer a Keyphrase Based Expert Recommender f.pdf:application/pdf},
}

@article{sahrawat_keyphrase_2019,
	title = {Keyphrase {Extraction} from {Scholarly} {Articles} as {Sequence} {Labeling} using {Contextualized} {Embeddings}},
	url = {http://arxiv.org/abs/1910.08840},
	abstract = {In this paper, we formulate keyphrase extraction from scholarly articles as a sequence labeling task solved using a BiLSTM-CRF, where the words in the input text are represented using deep contextualized embeddings. We evaluate the proposed architecture using both contextualized and ﬁxed word embedding models on three different benchmark datasets (Inspec, SemEval 2010, SemEval 2017), and compare with existing popular unsupervised and supervised techniques. Our results quantify the beneﬁts of: (a) using contextualized embeddings (e.g. BERT) over ﬁxed word embeddings (e.g. Glove); (b) using a BiLSTM-CRF architecture with contextualized word embeddings over ﬁne-tuning the contextualized word embedding model directly; and (c) using genre-speciﬁc contextualized embeddings (SciBERT). Through error analysis, we also provide some insights into why particular models work better than the others. Lastly, we present a case study where we analyze different self-attention layers of the two best models (BERT and SciBERT) to better understand the predictions made by each for the task of keyphrase extraction.},
	language = {en},
	urldate = {2020-03-10},
	journal = {arXiv:1910.08840 [cs]},
	author = {Sahrawat, Dhruva and Mahata, Debanjan and Kulkarni, Mayank and Zhang, Haimin and Gosangi, Rakesh and Stent, Amanda and Sharma, Agniv and Kumar, Yaman and Shah, Rajiv Ratn and Zimmermann, Roger},
	month = oct,
	year = {2019},
	note = {ArXiv : 1910.08840},
	keywords = {extraction},
	file = {Sahrawat et al. - 2019 - Keyphrase Extraction from Scholarly Articles as Se.pdf:/home/gallina-y/Zotero/storage/Q8QLTV6T/Sahrawat et al. - 2019 - Keyphrase Extraction from Scholarly Articles as Se.pdf:application/pdf},
}

@article{meng_does_2019,
	title = {Does {Order} {Matter}? {An} {Empirical} {Study} on {Generating} {Multiple} {Keyphrases} as a {Sequence}},
	shorttitle = {Does {Order} {Matter}?},
	url = {http://arxiv.org/abs/1909.03590},
	abstract = {Recently, concatenating multiple keyphrases as a target sequence has been proposed as a new learning paradigm for keyphrase generation. Existing studies concatenate target keyphrases in different orders but no study has examined the effects of ordering on models' behavior. In this paper, we propose several orderings for concatenation and inspect the important factors for training a successful keyphrase generation model. By running comprehensive comparisons, we observe one preferable ordering and summarize a number of empirical findings and challenges, which can shed light on future research on this line of work.},
	urldate = {2020-02-27},
	journal = {arXiv:1909.03590 [cs]},
	author = {Meng, Rui and Yuan, Xingdi and Wang, Tong and Brusilovsky, Peter and Trischler, Adam and He, Daqing},
	month = oct,
	year = {2019},
	note = {arXiv: 1909.03590},
	keywords = {meta},
	file = {arXiv Fulltext PDF:/home/gallina-y/Zotero/storage/X7QBUHS4/Meng et al. - 2019 - Does Order Matter An Empirical Study on Generatin.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/6ZTKB9TP/1909.html:text/html},
}

@inproceedings{zhang_mike_2017,
	address = {Singapore, Singapore},
	title = {{MIKE}: {Keyphrase} {Extraction} by {Integrating} {Multidimensional} {Information}},
	isbn = {978-1-4503-4918-5},
	shorttitle = {{MIKE}},
	url = {http://dl.acm.org/citation.cfm?doid=3132847.3132956},
	doi = {10.1145/3132847.3132956},
	abstract = {Traditional supervised keyphrase extraction models depend on the features of labelled keyphrases while prevailing unsupervised models mainly rely on structure of the word graph, with candidate words as nodes and edges capturing the co-occurrence information between words. However, systematically integrating all these multidimensional heterogeneous information into a uni ed model is relatively unexplored. In this paper, we focus on how to e ectively exploit multidimensional information to improve the keyphrase extraction performance (MIKE). Speci cally, we propose a random-walk parametric model, MIKE, that learns the latent representation for a candidate keyphrase that captures the mutual in uences among all information, and simultaneously optimizes the parameters and ranking scores of candidates in the word graph. We use the gradient-descent algorithm to optimize our model and show the comprehensive experiments with two publicly-available WWW and KDD datasets in Computer Science. Experimental results demonstrate that our approach signi cantly outperforms the state-of-the-art graph-based keyphrase extraction approaches.},
	language = {en},
	urldate = {2020-02-26},
	booktitle = {Proceedings of the 2017 {ACM} on {Conference} on {Information} and {Knowledge} {Management} - {CIKM} '17},
	publisher = {ACM Press},
	author = {Zhang, Yuxiang and Chang, Yaocheng and Liu, Xiaoqing and Gollapalli, Sujatha Das and Li, Xiaoli and Xiao, Chunjing},
	year = {2017},
	keywords = {kpe},
	pages = {1349--1358},
	file = {Zhang et al. - 2017 - MIKE Keyphrase Extraction by Integrating Multidim.pdf:/home/gallina-y/Zotero/storage/4TF42HWK/Zhang et al. - 2017 - MIKE Keyphrase Extraction by Integrating Multidim.pdf:application/pdf},
}

@article{song_unified_2018,
	title = {A {Unified} {Query}-based {Generative} {Model} for {Question} {Generation} and {Question} {Answering}},
	url = {http://arxiv.org/abs/1709.01058},
	abstract = {We propose a query-based generative model for solving both tasks of question generation (QG) and question answering (QA). The model follows the classic encoderdecoder framework. The encoder takes a passage and a query as input then performs query understanding by matching the query with the passage from multiple perspectives. The decoder is an attention-based Long Short Term Memory (LSTM) model with copy and coverage mechanisms. In the QG task, a question is generated from the system given the passage and the target answer, whereas in the QA task, the answer is generated given the question and the passage. During the training stage, we leverage a policy-gradient reinforcement learning algorithm to overcome exposure bias, a major problem resulted from sequence learning with cross-entropy loss. For the QG task, our experiments show higher performances than the state-of-the-art results. When used as additional training data, the automatically generated questions even improve the performance of a strong extractive QA system. In addition, our model shows better performance than the state-of-the-art baselines of the generative QA task.},
	language = {en},
	urldate = {2020-02-26},
	journal = {arXiv:1709.01058 [cs]},
	author = {Song, Linfeng and Wang, Zhiguo and Hamza, Wael},
	month = aug,
	year = {2018},
	note = {arXiv: 1709.01058},
	file = {Song et al. - 2018 - A Unified Query-based Generative Model for Questio.pdf:/home/gallina-y/Zotero/storage/RJ2AJFC5/Song et al. - 2018 - A Unified Query-based Generative Model for Questio.pdf:application/pdf},
}

@inproceedings{paroubek_indexation_2012,
	address = {Grenoble, France},
	title = {Indexation libre et contrôlée d'articles scientifiques. {Présentation} et résultats du défi fouille de textes {DEFT2012} ({Controlled} and free indexing of scientific papers. {Presentation} and results of the {DEFT2012} text-mining challenge) [in {French}]},
	url = {https://www.aclweb.org/anthology/W12-1101},
	urldate = {2020-02-21},
	booktitle = {{JEP}-{TALN}-{RECITAL} 2012, {Workshop} {DEFT} 2012: {DÉfi} {Fouille} de {Textes} ({DEFT} 2012 {Workshop}: {Text} {Mining} {Challenge})},
	publisher = {ATALA/AFCP},
	author = {Paroubek, Patrick and Zweigenbaum, Pierre and Forest, Dominic and Grouin, Cyril},
	month = jun,
	year = {2012},
	pages = {1--13},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/6NP2XU6J/Paroubek et al. - 2012 - Indexation libre et contrôlée d'articles scientifi.pdf:application/pdf},
}

@inproceedings{caragea_citation-enhanced_2014,
	address = {Doha, Qatar},
	title = {Citation-{Enhanced} {Keyphrase} {Extraction} from {Research} {Papers}: {A} {Supervised} {Approach}},
	shorttitle = {Citation-{Enhanced} {Keyphrase} {Extraction} from {Research} {Papers}},
	url = {https://www.aclweb.org/anthology/D14-1150},
	doi = {10.3115/v1/D14-1150},
	urldate = {2020-02-21},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Caragea, Cornelia and Bulgarov, Florin Adrian and Godea, Andreea and Das Gollapalli, Sujatha},
	month = oct,
	year = {2014},
	keywords = {kpe},
	pages = {1435--1446},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/WFYW8Q3K/Caragea et al. - 2014 - Citation-Enhanced Keyphrase Extraction from Resear.pdf:application/pdf},
}

@inproceedings{marujo_keyphrase_2011,
	title = {Keyphrase {Cloud} {Generation} of {Broadcast} {News}},
	abstract = {This paper describes an enhanced automatic keyphrase extraction method applied to Broadcast News. The keyphrase extraction process is used to create a concept level for each news. On top of words resulting from a speech recognition system output and news indexation and it contributes to the generation of a tag/keyphrase cloud of the top news included in a Multimedia Monitoring Solution system for TV and Radio news/programs, running daily, and monitoring 12 TV channels and 4 Radios.},
	booktitle = {{INTERSPEECH}},
	author = {Marujo, Luís and Viveiros, Márcio and Neto, João Paulo da Silva},
	year = {2011},
	keywords = {dataset},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/XNWWAXHJ/Marujo et al. - 2011 - Keyphrase Cloud Generation of Broadcast News.pdf:application/pdf},
}

@inproceedings{marujo_supervised_2012,
	address = {Istanbul, Turkey},
	title = {Supervised {Topical} {Key} {Phrase} {Extraction} of {News} {Stories} using {Crowdsourcing}, {Light} {Filtering} and {Co}-reference {Normalization}},
	url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/672_Paper.pdf},
	urldate = {2020-02-21},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'12)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Marujo, Luís and Gershman, Anatole and Carbonell, Jaime and Frederking, Robert and Neto, João P.},
	month = may,
	year = {2012},
	keywords = {dataset},
	pages = {399--403},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/MPHT3RNQ/Marujo et al. - 2012 - Supervised Topical Key Phrase Extraction of News S.pdf:application/pdf},
}

@techreport{krapivin_large_2009,
	type = {Departmental {Technical} {Report}},
	title = {Large {Dataset} for {Keyphrases} {Extraction}},
	url = {http://eprints.biblio.unitn.it/1671/},
	abstract = {We propose a large dataset for machine learning-based automatic keyphrase extraction. The dataset has a high quality and consist of 2,000 of scientific papers from computer science domain published by ACM. Each paper has its keyphrases assigned by the authors and verified by the reviewers. Different parts of papers, such as title and abstract, are separated, enabling extraction based on a part of an article's text. The content of each paper is converted from PDF to plain text. The pieces of formulae, tables, figures and LaTeX mark up were removed automatically. For removal we have used Maximum Entropy Model-based machine learning and achieved 97.04\% precision. Preliminary investigation with help of the state of the art keyphrase extraction system KEA shows keyphrases recognition accuracy improvement for refined texts.},
	urldate = {2020-02-21},
	institution = {University of Trento},
	author = {Krapivin, Mikalai and Autaeu, Aliaksandr and Marchese, Maurizio},
	year = {2009},
	keywords = {dataset},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/DLHHE7C8/Krapivin et al. - 2009 - Large Dataset for Keyphrases Extraction.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/ZVCLE9P3/1671.html:text/html},
}

@phdthesis{schutz_keyphrase_2008,
	address = {Galway},
	title = {Keyphrase extraction from single documents in the open domain exploiting linguistic and statistical methods},
	abstract = {Submitted in ful llment of the requirements for the degree of},
	school = {NationalUniversityofIreland},
	author = {Schutz, Alexander Thorsten},
	year = {2008},
	keywords = {kpe},
	file = {Citeseer - Full Text PDF:/home/gallina-y/Zotero/storage/3NYAPEXG/Schutz et Science - 2008 - Keyphrase extraction from single documents in the .pdf:application/pdf;Citeseer - Snapshot:/home/gallina-y/Zotero/storage/U2F8QC4I/summary.html:text/html},
}

@inproceedings{gallina_kptimes_2019,
	address = {Tokyo, Japan},
	title = {{KPTimes}: {A} {Large}-{Scale} {Dataset} for {Keyphrase} {Generation} on {News} {Documents}},
	shorttitle = {{KPTimes}},
	url = {https://www.aclweb.org/anthology/W19-8617},
	doi = {10.18653/v1/W19-8617},
	abstract = {Keyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text. Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations. In this paper we present KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset, we show how editors tag documents, and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain. The dataset is available online at https:// github.com/ygorg/KPTimes.},
	urldate = {2020-02-21},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Gallina, Ygor and Boudin, Florian and Daille, Beatrice},
	month = oct,
	year = {2019},
	keywords = {dataset},
	pages = {130--135},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/7NJQFDDQ/Gallina et al. - 2019 - KPTimes A Large-Scale Dataset for Keyphrase Gener.pdf:application/pdf},
}

@article{mu_keyphrase_2020,
	title = {Keyphrase {Extraction} with {Span}-based {Feature} {Representations}},
	url = {http://arxiv.org/abs/2002.05407},
	abstract = {Keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. Since keyphrase extraction is able to facilitate the management, categorization, and retrieval of information, it has received much attention in recent years. There are three approaches to address keyphrase extraction: (i) traditional two-step ranking method, (ii) sequence labeling and (iii) generation using neural networks. Two-step ranking approach is based on feature engineering, which is labor intensive and domain dependent. Sequence labeling is not able to tackle overlapping phrases. Generation methods (i.e., Sequence-to-sequence neural network models) overcome those shortcomings, so they have been widely studied and gain state-of-the-art performance. However, generation methods can not utilize context information effectively. In this paper, we propose a novelty Span Keyphrase Extraction model that extracts span-based feature representation of keyphrase directly from all the content tokens. In this way, our model obtains representation for each keyphrase and further learns to capture the interaction between keyphrases in one document to get better ranking results. In addition, with the help of tokens, our model is able to extract overlapped keyphrases. Experimental results on the benchmark datasets show that our proposed model outperforms the existing methods by a large margin.},
	urldate = {2020-03-11},
	journal = {arXiv:2002.05407 [cs]},
	author = {Mu, Funan and Yu, Zhenting and Wang, LiFeng and Wang, Yequan and Yin, Qingyu and Sun, Yibo and Liu, Liqun and Ma, Teng and Tang, Jing and Zhou, Xing},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.05407},
	keywords = {extraction},
	file = {arXiv Fulltext PDF:/home/gallina-y/Zotero/storage/LF8PERBP/Mu et al. - 2020 - Keyphrase Extraction with Span-based Feature Repre.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/VG73KPVB/2002.html:text/html},
}

@inproceedings{chen_integrated_2019,
	address = {Minneapolis, Minnesota},
	title = {An {Integrated} {Approach} for {Keyphrase} {Generation} via {Exploring} the {Power} of {Retrieval} and {Extraction}},
	url = {https://www.aclweb.org/anthology/N19-1292},
	doi = {10.18653/v1/N19-1292},
	abstract = {In this paper, we present a novel integrated approach for keyphrase generation (KG). Unlike previous works which are purely extractive or generative, we first propose a new multi-task learning framework that jointly learns an extractive model and a generative model. Besides extracting keyphrases, the output of the extractive model is also employed to rectify the copy probability distribution of the generative model, such that the generative model can better identify important contents from the given document. Moreover, we retrieve similar documents with the given document from training data and use their associated keyphrases as external knowledge for the generative model to produce more accurate keyphrases. For further exploiting the power of extraction and retrieval, we propose a neural-based merging module to combine and re-rank the predicted keyphrases from the enhanced generative model, the extractive model, and the retrieved keyphrases. Experiments on the five KG benchmarks demonstrate that our integrated approach outperforms the state-of-the-art methods.},
	urldate = {2020-03-10},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Wang and Chan, Hou Pong and Li, Piji and Bing, Lidong and King, Irwin},
	month = jun,
	year = {2019},
	keywords = {generation, o2o, code},
	pages = {2846--2856},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/SFJ8Q6CR/Chen et al. - 2019 - An Integrated Approach for Keyphrase Generation vi.pdf:application/pdf},
}

@inproceedings{chan_neural_2019,
	address = {Florence, Italy},
	title = {Neural {Keyphrase} {Generation} via {Reinforcement} {Learning} with {Adaptive} {Rewards}},
	url = {https://www.aclweb.org/anthology/P19-1208},
	doi = {10.18653/v1/P19-1208},
	abstract = {Generating keyphrases that summarize the main points of a document is a fundamental task in natural language processing. Although existing generative models are capable of predicting multiple keyphrases for an input document as well as determining the number of keyphrases to generate, they still suffer from the problem of generating too few keyphrases. To address this problem, we propose a reinforcement learning (RL) approach for keyphrase generation, with an adaptive reward function that encourages a model to generate both sufficient and accurate keyphrases. Furthermore, we introduce a new evaluation method that incorporates name variations of the ground-truth keyphrases using the Wikipedia knowledge base. Thus, our evaluation method can more robustly evaluate the quality of predicted keyphrases. Extensive experiments on five real-world datasets of different scales demonstrate that our RL approach consistently and significantly improves the performance of the state-of-the-art generative models with both conventional and new evaluation methods.},
	urldate = {2020-03-10},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Chan, Hou Pong and Chen, Wang and Wang, Lu and King, Irwin},
	month = jul,
	year = {2019},
	keywords = {generation, o2m, code},
	pages = {2163--2174},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/ICP3PY25/Chan et al. - 2019 - Neural Keyphrase Generation via Reinforcement Lear.pdf:application/pdf},
}

@incollection{goh_keyphrase_2007,
	address = {Berlin, Heidelberg},
	title = {Keyphrase {Extraction} in {Scientific} {Publications}},
	volume = {4822},
	isbn = {978-3-540-77093-0 978-3-540-77094-7},
	url = {http://link.springer.com/10.1007/978-3-540-77094-7_41},
	abstract = {We present a keyphrase extraction algorithm for scientiﬁc publications. Different from previous work, we introduce features that capture the positions of phrases in document with respect to logical sections found in scientiﬁc discourse. We also introduce features that capture salient morphological phenomena found in scientiﬁc keyphrases, such as whether a candidate keyphrase is an acronyms or uses speciﬁc terminologically productive sufﬁxes. We have implemented these features on top of a baseline feature set used by Kea [1]. In our evaluation using a corpus of 120 scientiﬁc publications multiply annotated for keyphrases, our system signiﬁcantly outperformed Kea at the p {\textless} .05 level. As we know of no other existing multiply annotated keyphrase document collections, we have also made our evaluation corpus publicly available. We hope that this contribution will spur future comparative research.},
	language = {en},
	urldate = {2020-07-22},
	booktitle = {Asian {Digital} {Libraries}. {Looking} {Back} 10 {Years} and {Forging} {New} {Frontiers}},
	publisher = {Springer Berlin Heidelberg},
	author = {Nguyen, Thuy Dung and Kan, Min-Yen},
	editor = {Goh, Dion Hoe-Lian and Cao, Tru Hoang and Sølvberg, Ingeborg Torvik and Rasmussen, Edie},
	year = {2007},
	doi = {10.1007/978-3-540-77094-7_41},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {kpe},
	pages = {317--326},
	file = {Nguyen et Kan - 2007 - Keyphrase Extraction in Scientific Publications.pdf:/home/gallina-y/Zotero/storage/NYD98NKA/Nguyen et Kan - 2007 - Keyphrase Extraction in Scientific Publications.pdf:application/pdf},
}

@inproceedings{sar_shalom_distant_2020,
	title = {Distant {Supervision} for {Keyphrase} {Extraction} using {Search} {Queries}},
	doi = {10.1109/BigDataService49289.2020.00019},
	abstract = {Keyphrase extraction aims at automatically selecting small set of phrases in a document, that best describe its main ideas. There is great need for better methods of keyphrase extraction in the absence of labeled data, as currently unsupervised algorithms fail to achieve adequate performance, compared to their supervised counterparts. In this paper we suggest a widely applicable distant supervision framework based on auxiliary data from query logs. By propagating information from queries and subsequent consumption of content, weak labels are produced, transforming the problem into the easier supervised task. Evaluation on a large dataset shows the superiority of this approach over unsupervised alternatives.},
	booktitle = {2020 {IEEE} {Sixth} {International} {Conference} on {Big} {Data} {Computing} {Service} and {Applications} ({BigDataService})},
	author = {Sar Shalom, Oren and Resheff, Hezi and Zhicharevich, Alex and Cohen, Rami},
	month = aug,
	year = {2020},
	keywords = {extraction, to read},
	pages = {70--77},
	file = {IEEE Xplore Abstract Record:/home/gallina-y/Zotero/storage/C5M2L3YY/9179571.html:text/html;IEEE Xplore Full Text PDF:/home/gallina-y/Zotero/storage/Y5PZDGGC/Sar Shalom et al. - 2020 - Distant Supervision for Keyphrase Extraction using.pdf:application/pdf},
}

@article{grabar_recherche_2019,
	title = {Recherche et extraction d'information dans des cas cliniques. {Présentation} de la campagne d'évaluation {DEFT} 2019},
	abstract = {This paper presents the DEFT 2019 challenge on the analysis of clinical texts in French. These texts are Clinical Cases, published and discussed within scientiﬁc papers, and indexed by keywords. We propose three independent tasks : the indexing of clinical cases and discussions, primarily evaluated using the mean average precision (MAP), the pairing between clinical cases and discussions, evaluated using precision, and the information extraction among four categories (age, gender, origin of consultation, outcome), evaluated in terms of recall, precision and F-measure. We present the results obtained by the participants on each task. MOTS-CLÉS : Cas clinique, fouille de texte, extraction d’information, recherche d’information, évaluation.},
	language = {fr},
	author = {Grabar, Natalia and Grouin, Cyril and Hamon, Thierry and Claveau, Vincent},
	year = {2019},
	pages = {11},
	file = {Grabar et al. - Recherche et extraction d'information dans des cas.pdf:/home/gallina-y/Zotero/storage/J6KISMLK/Grabar et al. - Recherche et extraction d'information dans des cas.pdf:application/pdf},
}

@article{mao_mesh_2017,
	title = {{MeSH} {Now}: automatic {MeSH} indexing at {PubMed} scale via learning to rank},
	volume = {8},
	issn = {2041-1480},
	shorttitle = {{MeSH} {Now}},
	url = {http://jbiomedsem.biomedcentral.com/articles/10.1186/s13326-017-0123-3},
	doi = {10.1186/s13326-017-0123-3},
	abstract = {Background: MeSH indexing is the task of assigning relevant MeSH terms based on a manual reading of scholarly publications by human indexers. The task is highly important for improving literature retrieval and many other scientific investigations in biomedical research. Unfortunately, given its manual nature, the process of MeSH indexing is both time-consuming (new articles are not immediately indexed until 2 or 3 months later) and costly (approximately ten dollars per article). In response, automatic indexing by computers has been previously proposed and attempted but remains challenging. In order to advance the state of the art in automatic MeSH indexing, a community-wide shared task called BioASQ was recently organized. Methods: We propose MeSH Now, an integrated approach that first uses multiple strategies to generate a combined list of candidate MeSH terms for a target article. Through a novel learning-to-rank framework, MeSH Now then ranks the list of candidate terms based on their relevance to the target article. Finally, MeSH Now selects the highest-ranked MeSH terms via a post-processing module. Results: We assessed MeSH Now on two separate benchmarking datasets using traditional precision, recall and F1score metrics. In both evaluations, MeSH Now consistently achieved over 0.60 in F-score, ranging from 0.610 to 0. 612. Furthermore, additional experiments show that MeSH Now can be optimized by parallel computing in order to process MEDLINE documents on a large scale. Conclusions: We conclude that MeSH Now is a robust approach with state-of-the-art performance for automatic MeSH indexing and that MeSH Now is capable of processing PubMed scale documents within a reasonable time frame. Availability: http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/MeSHNow/.
Results: We assessed MeSH Now on two separate benchmarking datasets using traditional precision, recall and F1score metrics. In both evaluations, MeSH Now consistently achieved over 0.60 in F-score, ranging from 0.610 to 0. 612. Furthermore, additional experiments show that MeSH Now can be optimized by parallel computing in order to process MEDLINE documents on a large scale. Conclusions: We conclude that MeSH Now is a robust approach with state-of-the-art performance for automatic MeSH indexing and that MeSH Now is capable of processing PubMed scale documents within a reasonable time frame. Availability: http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/MeSHNow/.},
	language = {en},
	number = {1},
	urldate = {2020-09-24},
	journal = {Journal of Biomedical Semantics},
	author = {Mao, Yuqing and Lu, Zhiyong},
	month = dec,
	year = {2017},
	keywords = {kpe},
	file = {Mao et Lu - 2017 - MeSH Now automatic MeSH indexing at PubMed scale .pdf:/home/gallina-y/Zotero/storage/HPYKYGQD/Mao et Lu - 2017 - MeSH Now automatic MeSH indexing at PubMed scale .pdf:application/pdf},
}

@article{firoozeh_keyword_2020,
	title = {Keyword extraction: {Issues} and methods},
	volume = {26},
	issn = {1351-3249, 1469-8110},
	shorttitle = {Keyword extraction},
	url = {https://www.cambridge.org/core/product/identifier/S1351324919000457/type/journal_article},
	doi = {10.1017/S1351324919000457},
	abstract = {Due to the considerable growth of the volume of text documents on the Internet and in digital libraries, manual analysis of these documents is no longer feasible. Having efficient approaches to keyword extraction in order to retrieve the ‘key’ elements of the studied documents is now a necessity. Keyword extraction has been an active research field for many years, covering various applications in Text Mining, Information Retrieval, and Natural Language Processing, and meeting different requirements. However, it is not a unified domain of research. In spite of the existence of many approaches in the field, there is no single approach that effectively extracts keywords from different data sources. This shows the importance of having a comprehensive review, which discusses the complexity of the task and categorizes the main approaches of the field based on the features and methods of extraction that they use. This paper presents a general introduction to the field of keyword/keyphrase extraction. Unlike the existing surveys, different aspects of the problem along with the main challenges in the field are discussed. This mainly includes the unclear definition of ‘keyness’, complexities of targeting proper features for capturing desired keyness properties and selecting efficient extraction methods, and also the evaluation issues. By classifying a broad range of state-of-the-art approaches and analysing the benefits and drawbacks of different features and methods, we provide a clearer picture of them. This review is intended to help readers find their way around all the works related to keyword extraction and guide them in choosing or designing a method that is appropriate for the application they are targeting.},
	language = {en},
	number = {3},
	urldate = {2020-10-08},
	journal = {Natural Language Engineering},
	author = {Firoozeh, Nazanin and Nazarenko, Adeline and Alizon, Fabrice and Daille, Béatrice},
	month = may,
	year = {2020},
	keywords = {meta},
	pages = {259--291},
	file = {Firoozeh et al. - 2020 - Keyword extraction Issues and methods.pdf:/home/gallina-y/Zotero/storage/8N69NZQV/Firoozeh et al. - 2020 - Keyword extraction Issues and methods.pdf:application/pdf},
}

@inproceedings{chen_exclusive_2020,
	address = {Online},
	title = {Exclusive {Hierarchical} {Decoding} for {Deep} {Keyphrase} {Generation}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.103},
	doi = {10.18653/v1/2020.acl-main.103},
	abstract = {Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. Previous work in this setting employs a sequential decoding process to generate keyphrases. However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document. Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources. To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set. Both the soft and the hard exclusion mechanisms keep track of previouslypredicted keyphrases within a window size to enhance the diversity of the generated keyphrases. Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases1.},
	language = {en},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Wang and Chan, Hou Pong and Li, Piji and King, Irwin},
	year = {2020},
	keywords = {generation, o2m, code},
	pages = {1095--1105},
	file = {Chen et al. - 2020 - Exclusive Hierarchical Decoding for Deep Keyphrase.pdf:/home/gallina-y/Zotero/storage/D67E8PXY/Chen et al. - 2020 - Exclusive Hierarchical Decoding for Deep Keyphrase.pdf:application/pdf},
}

@article{diao_keyphrase_2020,
	title = {Keyphrase {Generation} with {Cross}-{Document} {Attention}},
	url = {http://arxiv.org/abs/2004.09800},
	abstract = {Keyphrase generation aims to produce a set of phrases summarizing the essentials of a given document. Conventional methods normally apply an encoder-decoder architecture to generate the output keyphrases for an input document, where they are designed to focus on each current document so they inevitably omit crucial corpus-level information carried by other similar documents, i.e., the cross-document dependency and latent topics. In this paper, we propose CDKGEN, a Transformerbased keyphrase generator, which expands the Transformer to global attention with crossdocument attention networks to incorporate available documents as references so as to generate better keyphrases with the guidance of topic information. On top of the proposed Transformer + cross-document attention architecture, we also adopt a copy mechanism to enhance our model via selecting appropriate words from documents to deal with outof-vocabulary words in keyphrases. Experiment results on ﬁve benchmark datasets illustrate the validity and effectiveness of our model, which achieves the state-of-the-art performance on all datasets. Further analyses conﬁrm that the proposed model is able to generate keyphrases consistent with references while keeping sufﬁcient diversity. The code of CDKGEN is available at https://github. com/SVAIGBA/CDKGen.},
	language = {en},
	urldate = {2020-10-09},
	journal = {arXiv:2004.09800 [cs]},
	author = {Diao, Shizhe and Song, Yan and Zhang, Tong},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.09800},
	keywords = {generation, o2m, code},
	file = {Diao et al. - 2020 - Keyphrase Generation with Cross-Document Attention.pdf:/home/gallina-y/Zotero/storage/CQDY787I/Diao et al. - 2020 - Keyphrase Generation with Cross-Document Attention.pdf:application/pdf},
}

@inproceedings{sun_divgraphpointer_2019,
	address = {New York, NY, USA},
	series = {{SIGIR}'19},
	title = {{DivGraphPointer}: {A} {Graph} {Pointer} {Network} for {Extracting} {Diverse} {Keyphrases}},
	isbn = {978-1-4503-6172-9},
	shorttitle = {{DivGraphPointer}},
	url = {https://doi.org/10.1145/3331184.3331219},
	doi = {10.1145/3331184.3331219},
	abstract = {Keyphrase extraction from documents is useful to a variety of applications such as information retrieval and document summarization. This paper presents an end-to-end method called DivGraphPointer for extracting a set of diversified keyphrases from a document. DivGraphPointer combines the advantages of traditional graph-based ranking methods and recent neural network-based approaches. Specifically, given a document, a word graph is constructed from the document based on word proximity and is encoded with graph convolutional networks, which effectively capture document-level word salience by modeling long-range dependency between words in the document and aggregating multiple appearances of identical words into one node. Furthermore, we propose a diversified point network to generate a set of diverse keyphrases out of the word graph in the decoding process. Experimental results on five benchmark data sets show that our proposed method significantly outperforms the existing state-of-the-art approaches.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the 42nd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Zhiqing and Tang, Jian and Du, Pan and Deng, Zhi-Hong and Nie, Jian-Yun},
	month = jul,
	year = {2019},
	keywords = {extraction},
	pages = {755--764},
	file = {Version soumise:/home/gallina-y/Zotero/storage/7EZ4RKDD/Sun et al. - 2019 - DivGraphPointer A Graph Pointer Network for Extra.pdf:application/pdf},
}

@article{ahmad_select_2020,
	title = {Select, {Extract} and {Generate}: {Neural} {Keyphrase} {Generation} with {Syntactic} {Guidance}},
	shorttitle = {Select, {Extract} and {Generate}},
	url = {http://arxiv.org/abs/2008.01739},
	abstract = {In recent years, deep neural sequence-to-sequence framework has demonstrated promising results in keyphrase generation. However, processing long documents using such deep neural networks requires high computational resources. To reduce the computational cost, the documents are typically truncated before given as inputs. As a result, the models may miss essential points conveyed in a document. Moreover, most of the existing methods are either extractive (identify important phrases from the document) or generative (generate phrases word by word), and hence they do not benefit from the advantages of both modeling techniques. To address these challenges, we propose {\textbackslash}emph\{SEG-Net\}, a neural keyphrase generation model that is composed of two major components, (1) a selector that selects the salient sentences in a document, and (2) an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses a self-attentive architecture, known as, {\textbackslash}emph\{Transformer\} as the building block with a couple of uniqueness. First, SEG-Net incorporates a novel {\textbackslash}emph\{layer-wise\} coverage attention to summarize most of the points discussed in the target document. Second, it uses an {\textbackslash}emph\{informed\} copy attention mechanism to encourage focusing on different segments of the document during keyphrase extraction and generation. Besides, SEG-Net jointly learns keyphrase generation and their part-of-speech tag prediction, where the later provides syntactic supervision to the former. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin in both domains.},
	urldate = {2020-10-09},
	journal = {arXiv:2008.01739 [cs]},
	author = {Ahmad, Wasi Uddin and Bai, Xiao and Lee, Soomin and Chang, Kai-Wei},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.01739},
	keywords = {generation, to read},
	file = {arXiv Fulltext PDF:/home/gallina-y/Zotero/storage/N52ZJALU/Ahmad et al. - 2020 - Select, Extract and Generate Neural Keyphrase Gen.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/NM2M5LBF/2008.html:text/html},
}

@inproceedings{gollapalli_incorporating_2017,
	address = {San Francisco, California, USA},
	series = {{AAAI}'17},
	title = {Incorporating expert knowledge into keyphrase extraction},
	abstract = {Keyphrases that efficiently summarize a document's content are used in various document processing and retrieval tasks. Current state-of-the-art techniques for keyphrase extraction operate at a phrase-level and involve scoring candidate phrases based on features of their component words. In this paper, we learn keyphrase taggers for research papers using token-based features incorporating linguistic, surface-form, and document-structure information through sequence labeling. We experimentally illustrate that using withindocument features alone, our tagger trained with Conditional Random Fields performs on-par with existing state-of-the-art systems that rely on information from Wikipedia and citation networks. In addition, we are also able to harness recent work on feature labeling to seamlessly incorporate expert knowledge and predictions from existing systems to enhance the extraction performance further. We highlight the modeling advantages of our keyphrase taggers and show significant performance improvements on two recently-compiled datasets of keyphrases from Computer Science research papers.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Gollapalli, Sujatha Das and Li, Xiao-Li and Yang, Peng},
	month = feb,
	year = {2017},
	keywords = {kpe},
	pages = {3180--3187},
	file = {Gollapalli et al. - Incorporating Expert Knowledge into Keyphrase Extr.pdf:/home/gallina-y/Zotero/storage/D7M745YY/Gollapalli et al. - Incorporating Expert Knowledge into Keyphrase Extr.pdf:application/pdf},
}

@inproceedings{kalyan_trainable_2019,
	title = {Trainable {Decoding} of {Sets} of {Sequences} for {Neural} {Sequence} {Models}},
	url = {http://proceedings.mlr.press/v97/kalyan19a.html},
	abstract = {Many sequence prediction tasks admit multiple correct outputs and so, it is often useful to decode a set of outputs that maximize some task-specific set-level metric. However, retooling standard se...},
	language = {en},
	urldate = {2020-10-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kalyan, Ashwin and Anderson, Peter and Lee, Stefan and Batra, Dhruv},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {set},
	pages = {3211--3221},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/VKHUMWKA/Kalyan et al. - 2019 - Trainable Decoding of Sets of Sequences for Neural.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/YFUWJMF5/kalyan19a.html:text/html},
}

@inproceedings{mahata_key2vec_2018,
	address = {New Orleans, Louisiana},
	title = {{Key2Vec}: {Automatic} {Ranked} {Keyphrase} {Extraction} from {Scientific} {Articles} using {Phrase} {Embeddings}},
	shorttitle = {{Key2Vec}},
	url = {https://www.aclweb.org/anthology/N18-2100},
	doi = {10.18653/v1/N18-2100},
	abstract = {Keyphrase extraction is a fundamental task in natural language processing that facilitates mapping of documents to a set of representative phrases. In this paper, we present an unsupervised technique (Key2Vec) that leverages phrase embeddings for ranking keyphrases extracted from scientific articles. Specifically, we propose an effective way of processing text documents for training multi-word phrase embeddings that are used for thematic representation of scientific articles and ranking of keyphrases extracted from them using theme-weighted PageRank. Evaluations are performed on benchmark datasets producing state-of-the-art results.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Mahata, Debanjan and Kuriakose, John and Shah, Rajiv Ratn and Zimmermann, Roger},
	month = jun,
	year = {2018},
	keywords = {kpe},
	pages = {634--639},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/3K2MKBAC/Mahata et al. - 2018 - Key2Vec Automatic Ranked Keyphrase Extraction fro.pdf:application/pdf},
}

@inproceedings{marcu_rhetorical_1997,
	address = {Madrid, Spain},
	title = {The {Rhetorical} {Parsing} of {Unrestricted} {Natural} {Language} {Texts}},
	url = {https://www.aclweb.org/anthology/P97-1013},
	doi = {10.3115/976909.979630},
	urldate = {2020-10-09},
	booktitle = {35th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and 8th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Marcu, Daniel},
	month = jul,
	year = {1997},
	pages = {96--103},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/MAIRJ2C5/Marcu - 1997 - The Rhetorical Parsing of Unrestricted Natural Lan.pdf:application/pdf},
}

@phdthesis{neveol_automatisation_2005,
	type = {Theses},
	title = {Automatisation des tâches documentaires dans un catalogue de santé en ligne.},
	url = {https://tel.archives-ouvertes.fr/tel-00011549},
	urldate = {2020-10-09},
	school = {INSA de Rouen},
	author = {Neveol, Aurelie},
	month = nov,
	year = {2005},
	file = {HAL PDF Full Text:/home/gallina-y/Zotero/storage/EK8R5XIN/Neveol - 2005 - Automatisation des tâches documentaires dans un ca.pdf:application/pdf},
}

@inproceedings{prasad_glocal_2019,
	address = {Minneapolis, Minnesota},
	title = {Glocal: {Incorporating} {Global} {Information} in {Local} {Convolution} for {Keyphrase} {Extraction}},
	shorttitle = {Glocal},
	url = {https://www.aclweb.org/anthology/N19-1182},
	doi = {10.18653/v1/N19-1182},
	abstract = {Graph Convolutional Networks (GCNs) are a class of spectral clustering techniques that leverage localized convolution filters to perform supervised classification directly on graphical structures. While such methods model nodes' local pairwise importance, they lack the capability to model global importance relative to other nodes of the graph. This causes such models to miss critical information in tasks where global ranking is a key component for the task, such as in keyphrase extraction. We address this shortcoming by allowing the proper incorporation of global information into the GCN family of models through the use of scaled node weights. In the context of keyphrase extraction, incorporating global random walk scores obtained from TextRank boosts performance significantly. With our proposed method, we achieve state-of-the-art results, bettering a strong baseline by an absolute 2\% increase in F1 score.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Prasad, Animesh and Kan, Min-Yen},
	month = jun,
	year = {2019},
	keywords = {kpe},
	pages = {1837--1846},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/EY2VNPL6/Prasad et Kan - 2019 - Glocal Incorporating Global Information in Local .pdf:application/pdf},
}

@inproceedings{bouhandi_deft_2019,
	title = {{DeFT} 2019 : {Auto}-encodeurs, {Gradient} {Boosting} et combinaisons de modèles pour l’identification automatique de mots-clés. {Participation} de l’équipe {TALN} du {LS2N}},
	shorttitle = {{DeFT} 2019},
	url = {http://hal.univ-nantes.fr/hal-02433723},
	language = {fr},
	urldate = {2020-10-09},
	author = {Bouhandi, Mérième and Boudin, Florian and Gallina, Ygor},
	month = jul,
	year = {2019},
	file = {actes_DEFT_CH_PFIA2019.pdf:/home/gallina-y/Zotero/storage/W52FA3YY/actes_DEFT_CH_PFIA2019.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/JB6J72YZ/hal-02433723v1.html:text/html},
}

@inproceedings{gallina_large-scale_2020,
	address = {New York, NY, USA},
	series = {{JCDL} '20},
	title = {Large-{Scale} {Evaluation} of {Keyphrase} {Extraction} {Models}},
	isbn = {978-1-4503-7585-6},
	url = {https://doi.org/10.1145/3383583.3398517},
	doi = {10.1145/3383583.3398517},
	abstract = {Keyphrase extraction models are usually evaluated under different, not directly comparable, experimental setups. As a result, it remains unclear how well proposed models actually perform, and how they compare to each other. In this work, we address this issue by presenting a systematic large-scale analysis of state-of-the-art keyphrase extraction models involving multiple benchmark datasets from various sources and domains. Our main results reveal that state-of-the-art models are in fact still challenged by simple baselines on some datasets. We also present new insights about the impact of using author- or reader-assigned keyphrases as a proxy for gold standard, and give recommendations for strong baselines and reliable benchmark datasets.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} in 2020},
	publisher = {Association for Computing Machinery},
	author = {Gallina, Ygor and Boudin, Florian and Daille, Béatrice},
	month = aug,
	year = {2020},
	keywords = {meta},
	pages = {271--278},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/LI3QTR6L/Gallina et al. - 2020 - Large-Scale Evaluation of Keyphrase Extraction Mod.pdf:application/pdf},
}

@phdthesis{bougouin_indexation_2015,
	type = {These de doctorat},
	title = {Indexation automatique par termes-clés en domaines de spécialité},
	copyright = {Licence Etalab},
	url = {https://www.theses.fr/2015NANT2047},
	abstract = {Les termes-clés, ou mots-clés, sont des mots ou des expressions qui représentent le contenu d’un document. Ils en donnent une représentation synthétique et permettent de l’indexer pour la recherche d’information. Cette thèse s’intéresse à l’indexation automatique par termes-clés de documents en domaines de spécialité. La tâche est difficile à réaliser et les méthodes actuelles peinent encore à atteindre des résultats satisfaisants. Notre démarche s’organise en deux temps. Dans un premier temps, nous nous intéressons à l’indexation par termes-clés en général. Nous proposons une méthode pour sélectionner des termes-clés candidats dans un document en nous focalisant sur la catégorie des adjectifs qu’ils peuvent contenir, puis proposons uneméthode pour les ordonner par importance. Cette dernière, TopicRank, se situe en aval de la sélection des candidats. C’est une méthode à base de graphe qui groupe les termes-clés candidats véhiculant le même sujet, projette les sujets dans un graphe et extrait un terme-clé par sujet. Nos expériences montrent que TopicRank est significativement meilleur que les précédentes méthodes à base de graphe. Dans un second temps, nous adaptons notre travail à l’indexation par termes-clés en domaines de spécialité. Nous étudions la méthodologie d’indexation manuelle de documentalistes et la simulons à l’aide de TopicCoRank. TopicCoRank ajoute à TopicRank un graphe qui représente le domaine de spécialité du document. Grâce à ce second graphe, TopicCoRank possède la rare capacité à fournir des termes-clés qui n’apparaissent pas dans les documents. Appliqué à quatre domaines de spécialité, TopicCoRank améliore significativement TopicRank.},
	urldate = {2020-10-09},
	school = {Nantes},
	author = {Bougouin, Adrien},
	collaborator = {Daille, Béatrice and Boudin, Florian},
	month = jan,
	year = {2015},
	keywords = {meta},
	file = {Bougouin - 2015 - Indexation automatique par termes-clés en domaines.pdf:/home/gallina-y/Zotero/storage/PMZWBNF8/Bougouin - 2015 - Indexation automatique par termes-clés en domaines.pdf:application/pdf},
}

@inproceedings{tu_modeling_2016,
	address = {Berlin, Germany},
	title = {Modeling {Coverage} for {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/P16-1008},
	doi = {10.18653/v1/P16-1008},
	urldate = {2020-10-12},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tu, Zhaopeng and Lu, Zhengdong and Liu, Yang and Liu, Xiaohua and Li, Hang},
	month = aug,
	year = {2016},
	pages = {76--85},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/2H8TM7SQ/Tu et al. - 2016 - Modeling Coverage for Neural Machine Translation.pdf:application/pdf},
}

@inproceedings{see_get_2017,
	address = {Vancouver, Canada},
	title = {Get {To} {The} {Point}: {Summarization} with {Pointer}-{Generator} {Networks}},
	shorttitle = {Get {To} {The} {Point}},
	url = {https://www.aclweb.org/anthology/P17-1099},
	doi = {10.18653/v1/P17-1099},
	abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
	urldate = {2020-10-12},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
	month = jul,
	year = {2017},
	pages = {1073--1083},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/46WNMBNI/See et al. - 2017 - Get To The Point Summarization with Pointer-Gener.pdf:application/pdf},
}

@inproceedings{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2020-10-12},
	booktitle = {Proceedings of the 2017 {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv: 1609.02907},
	file = {arXiv Fulltext PDF:/home/gallina-y/Zotero/storage/AW5IY5J4/Kipf et Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:/home/gallina-y/Zotero/storage/9LB2TR5Q/1609.html:text/html},
}

@article{campos_yake_2020,
	title = {{YAKE}! {Keyword} extraction from single documents using multiple local features},
	volume = {509},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025519308588},
	doi = {10.1016/j.ins.2019.09.013},
	language = {en},
	urldate = {2020-10-28},
	journal = {Information Sciences},
	author = {Campos, Ricardo and Mangaravite, Vítor and Pasquali, Arian and Jorge, Alípio and Nunes, Célia and Jatowt, Adam},
	month = jan,
	year = {2020},
	keywords = {kpe},
	pages = {257--289},
	file = {Campos et al. - 2020 - YAKE! Keyword extraction from single documents usi.pdf:/home/gallina-y/Zotero/storage/4L2ISHXR/Campos et al. - 2020 - YAKE! Keyword extraction from single documents usi.pdf:application/pdf},
}

@inproceedings{doostmohammadi_persian_2019,
	title = {Persian {Keyphrase} {Generation} {Using} {Sequence}-to-{Sequence} {Models}},
	doi = {10.1109/IranianCEE.2019.8786505},
	abstract = {Keyphrases are a very short summary of an input text and provide the main subjects discussed in the text. Keyphrase extraction is a useful upstream task and can be used in various natural language processing problems, for example, text summarization and information retrieval, to name a few. However, not all the keyphrases are explicitly mentioned in the body of the text. In real-world examples there are always some topics that are discussed implicitly. Extracting such keyphrases requires a generative approach, which is adopted here. In this paper, we try to tackle the problem of keyphrase generation and extraction from news articles using deep sequence-to-sequence models. These models significantly outperform the conventional methods such as Topic Rank, KPMiner, and KEA in the task of keyphrase extraction 1.},
	booktitle = {2019 27th {Iranian} {Conference} on {Electrical} {Engineering} ({ICEE})},
	author = {Doostmohammadi, E. and Bokaei, M. H. and Sameti, H.},
	month = apr,
	year = {2019},
	note = {ISSN: 2642-9527},
	keywords = {dataset},
	pages = {2010--2015},
	file = {IEEE Xplore Abstract Record:/home/gallina-y/Zotero/storage/CYUSX6ZP/8786505.html:text/html;IEEE Xplore Full Text PDF:/home/gallina-y/Zotero/storage/GEC9X4Q2/Doostmohammadi et al. - 2019 - Persian Keyphrase Generation Using Sequence-to-Seq.pdf:application/pdf},
}

@inproceedings{mohseni_title_2020,
	title = {Title {Generation} and {Keyphrase} {Extraction} from {Persian} {Scientific} {Texts}},
	doi = {10.1109/CSICC49403.2020.9050113},
	abstract = {Modern neural-based approaches, which usually rely on large volumes of training data, have presented magnificent progress in various fields of text processing. However, these approaches have not been studied adequately in low resource languages. In this paper we focus on title generation and keyphrase extraction in the Persian language. We build a large corpus of scientific Persian texts which enables us to train end-to-end neural models for generating titles and extracting keyphrases. We investigate the effect of input length on modeling Persian text in both tasks. Additionally, we compare subword-level processing with the word-level one and show that even a straightforward subword encoding method enhances results greatly on Persian as an agglutinative language. For keyphrase extraction we formulate the task in two different ways: training the model to output all keyphrases at once; training the model to output one keyphrase each time and then extract n-best keyphrases during decoding. The latter improves the performance greatly.},
	booktitle = {2020 25th {International} {Computer} {Conference}, {Computer} {Society} of {Iran} ({CSICC})},
	author = {Mohseni, M. and Faili, H.},
	month = jan,
	year = {2020},
	keywords = {extraction},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/gallina-y/Zotero/storage/BIQMEMFJ/9050113.html:text/html;IEEE Xplore Full Text PDF:/home/gallina-y/Zotero/storage/NFUYXBEV/Mohseni et Faili - 2020 - Title Generation and Keyphrase Extraction from Per.pdf:application/pdf},
}

@book{frakes_information_1992,
	address = {USA},
	title = {Information retrieval: data structures and algorithms},
	isbn = {978-0-13-463837-9},
	shorttitle = {Information retrieval},
	publisher = {Prentice-Hall, Inc.},
	editor = {Frakes, William B. and Baeza-Yates, Ricardo},
	year = {1992},
}

@incollection{fox_lexical_1992,
	address = {USA},
	title = {Lexical analysis and stoplists},
	isbn = {978-0-13-463837-9},
	urldate = {2021-01-14},
	booktitle = {Information retrieval: data structures and algorithms},
	publisher = {Prentice-Hall, Inc.},
	author = {Fox, Christopher},
	month = jun,
	year = {1992},
	pages = {102--130},
}

@phdthesis{amar_les_1997,
	address = {Lyon, France},
	type = {Thèse de doctorat},
	title = {Les fondements théoriques de l'indexation: une approche linguistique},
	shorttitle = {Les fondements théoriques de l'indexation},
	abstract = {Cette recherche se donne pour objectif de fonder, du point de vue d'une théorie linguistique, la pratique professionnelle de l'indexation. Après un premier chapitre où sont présentés objet d'étude et méthode d'analyse, la recherche se poursuit sur quatre chapitres regroupés en deux parties. La première partie, qui s'intitule "Les problèmes théoriques de l'indexation", s'attache a expliquer, sur les questions du lexique et de la référence, les problèes d'indistinctions et de chevauchements entre faits de langue et faits d'indexation dans le but de distinguer les propriétés de langue (niveau linguistique) et leur utilisation documentaire (niveau de la pratique). Dans la deuxième partie, "Contribution aux fondements théoriques de l'indexation", est proposée une reformulation de l'indexation sous ses deux aspects : processus et résultat. Sous l'angle du processus, l'indexation se laisse définir par deux types de stratégies discursives : l'une concerne la sélection des sources ; l'autre l'exposition des documents. Sous l'angle du résultat, l'indexation se présente sous une morphologie spécifique : le descripteur, en tant qu'élément d'une chaine de référence, a pour forme linguistique privilégiée le syntagme nominal et, plus précisément, la synapsie. Le matériau utilisé dans cette recherche est de deux types : un ensemble de discours de la pratique sur elle-même, une enquête effectuée aupres de dix organismes documentaires. This research deals with theoretical foundations of indexing seen from a linguistic point of view. The object and the method of study are described in the first chapter. The first part, entitled "Theoretical problems of indexing", contains two chapters, one about lexicon, the other about reference. The second part, "Contribution to theoretical foundations of indexing" sets out to define indexing in a linguistic theory : indexing and descriptor are analysed on a discursive level.},
	language = {français},
	school = {Université Lumière},
	author = {Amar, Muriel},
	collaborator = {Le Guern, Michel},
	year = {1997},
	file = {Les fondements théoriques de l'indexationune approche linguistique:/home/gallina-y/Zotero/storage/4TWPPNKY/amar_m.html:text/html},
}

@article{luo_sensenet_2020,
	title = {{SenSeNet}: {Neural} {Keyphrase} {Generation} with {Document} {Structure}},
	shorttitle = {{SenSeNet}},
	url = {http://arxiv.org/abs/2012.06754},
	abstract = {Keyphrase Generation (KG) is the task of generating central topics from a given document or literary work, which captures the crucial information necessary to understand the content. Documents such as scientiﬁc literature contain rich meta-sentence information, which represents the logicalsemantic structure of the documents. However, previous approaches ignore the constraints of document logical structure, and hence they mistakenly generate keyphrases from unimportant sentences. To address this problem, we propose a new method called Sentence Selective Network (SenSeNet) to incorporate the meta-sentence inductive bias into KG. In SenSeNet, we use a straight-through estimator for end-toend training and incorporate weak supervision in the training of the sentence selection module. Experimental results show that SenSeNet can consistently improve the performance of major KG models based on seq2seq framework, which demonstrate the effectiveness of capturing structural information and distinguishing the signiﬁcance of sentences in KG task.},
	language = {en},
	urldate = {2021-01-18},
	journal = {arXiv:2012.06754 [cs]},
	author = {Luo, Yichao and Li, Zhengyan and Wang, Bingning and Xing, Xiaoyu and Zhang, Qi and Huang, Xuanjing},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.06754},
	keywords = {generation, o2m},
	file = {Luo et al. - 2020 - SenSeNet Neural Keyphrase Generation with Documen.pdf:/home/gallina-y/Zotero/storage/WBITDC6C/Luo et al. - 2020 - SenSeNet Neural Keyphrase Generation with Documen.pdf:application/pdf},
}

@inproceedings{jones_phrasier_1999,
	address = {Berkeley, California, United States},
	title = {Phrasier: a system for interactive document retrieval using keyphrases},
	isbn = {978-1-58113-096-6},
	shorttitle = {Phrasier},
	url = {http://portal.acm.org/citation.cfm?doid=312624.312671},
	doi = {10.1145/312624.312671},
	abstract = {UsersÕ information needs are often too complex to be effectively expressed in standard query interfaces to full-text retrieval systems. A typical need is to find documents that are similar to a given source document, yet describing the content of a document in a few terms is a difficult task. We describe Phrasier, an interactive system for browsing, querying and relating documents within a digital library. Phrasier exploits keyphrases that have been automatically extracted from source documents to create links to similar documents and to suggest appropriate query phrases to users. PhrasierÕs keyphrase-based retrieval engine returns ranked lists of documents that are similar to a given source text. Evaluation indicates that PhrasierÕs keyphrase-based retrieval performs as well as full-text retrieval when recall and relevance scores assigned by human assessors are considered.},
	language = {en},
	urldate = {2021-01-18},
	booktitle = {Proceedings of the 22nd annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '99},
	publisher = {ACM Press},
	author = {Jones, Steve and Staveley, Mark S.},
	year = {1999},
	pages = {160--167},
	file = {Jones et Staveley - 1999 - Phrasier a system for interactive document retrie.pdf:/home/gallina-y/Zotero/storage/EN5PG9HZ/Jones et Staveley - 1999 - Phrasier a system for interactive document retrie.pdf:application/pdf},
}

@inproceedings{cano_keyphrase_2019,
	title = {Keyphrase {Generation}: {A} {Multi}-{Aspect} {Survey}},
	shorttitle = {Keyphrase {Generation}},
	doi = {10.23919/FRUCT48121.2019.8981519},
	abstract = {Extractive keyphrase generation research has been around since the nineties, but the more advanced abstractive approach based on the encoder-decoder framework and sequence-to-sequence learning has been explored only recently. In fact, more than a dozen of abstractive methods have been proposed in the last three years, producing meaningful keyphrases and achieving state-of-the-art scores. In this survey, we examine various aspects of the extractive keyphrase generation methods and focus mostly on the more recent abstractive methods that are based on neural networks. We pay particular attention to the mechanisms that have driven the perfection of the later. A huge collection of scientific article metadata and the corresponding keyphrases is created and released for the research community. We also present various keyphrase generation and text summarization research patterns and trends of the last two decades.},
	booktitle = {2019 25th {Conference} of {Open} {Innovations} {Association} ({FRUCT})},
	author = {Çano, E. and Bojar, O.},
	month = nov,
	year = {2019},
	note = {ISSN: 2305-7254},
	keywords = {meta},
	pages = {85--94},
	file = {IEEE Xplore Abstract Record:/home/gallina-y/Zotero/storage/AQBB73DB/8981519.html:text/html;IEEE Xplore Full Text PDF:/home/gallina-y/Zotero/storage/AY2XS6W7/Çano et Bojar - 2019 - Keyphrase Generation A Multi-Aspect Survey.pdf:application/pdf},
}

@article{curry_method_1944,
	title = {The method of steepest descent for non-linear minimization problems},
	volume = {2},
	issn = {0033-569X, 1552-4485},
	url = {https://www.ams.org/qam/1944-02-03/S0033-569X-1944-10667-3/},
	doi = {10.1090/qam/10667},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	number = {3},
	urldate = {2021-02-19},
	journal = {Quarterly of Applied Mathematics},
	author = {Curry, Haskell B.},
	year = {1944},
	pages = {258--261},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/V34AEMPZ/Curry - 1944 - The method of steepest descent for non-linear mini.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/L2QJ6BVY/S0033-569X-1944-10667-3.html:text/html},
}

@inproceedings{gallina_etat_2019,
	address = {Toulouse, France},
	title = {État de l'art des méthodes d'apprentissage profond pour l'extraction automatique de termes-clés},
	url = {https://hal.archives-ouvertes.fr/hal-02395693},
	urldate = {2021-03-01},
	booktitle = {21e {Rencontre} des Étudiants {Chercheurs} en {Informatique} pour le {Traitement} {Automatique} des {Langues} ({RECITAL})},
	author = {Gallina, Ygor},
	month = jul,
	year = {2019},
	keywords = {meta},
	file = {HAL PDF Full Text:/home/gallina-y/Zotero/storage/YGHMES3R/Gallina - 2019 - État de l'art des méthodes d'apprentissage profond.pdf:application/pdf},
}

@article{jones_statistical_1972,
	title = {A statistical interpretation of term specificity and its application in retrieval},
	volume = {28},
	issn = {0022-0418},
	url = {https://doi.org/10.1108/eb026526},
	doi = {10.1108/eb026526},
	abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently‐occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
	language = {en},
	number = {1},
	urldate = {2021-03-04},
	journal = {Journal of Documentation},
	author = {Jones, Karen Spärck},
	month = jan,
	year = {1972},
	note = {Publisher: MCB UP Ltd},
	pages = {11--21},
	file = {Jones - A statistical interpretation of term specificity a.pdf:/home/gallina-y/Zotero/storage/KHGE2UD3/Jones - A statistical interpretation of term specificity a.pdf:application/pdf},
}

@inproceedings{petrov_universal_2012,
	address = {Istanbul, Turkey},
	title = {A {Universal} {Part}-of-{Speech} {Tagset}},
	url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf},
	abstract = {To facilitate future research in unsupervised induction of syntactic structure and to standardize best-practices, we propose a tagset that consists of twelve universal part-of-speech categories. In addition to the tagset, we develop a mapping from 25 different treebank tagsets to this universal set. As a result, when combined with the original treebank data, this universal tagset and mapping produce a dataset consisting of common parts-of-speech for 22 different languages. We highlight the use of this resource via three experiments, that (1) compare tagging accuracies across languages, (2) present an unsupervised grammar induction approach that does not use gold standard part-of-speech tags, and (3) use the universal tags to transfer dependency parsers between languages, achieving state-of-the-art results.},
	urldate = {2021-03-08},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'12)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Petrov, Slav and Das, Dipanjan and McDonald, Ryan},
	month = may,
	year = {2012},
	pages = {2089--2096},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/XJRKE595/Petrov et al. - 2012 - A Universal Part-of-Speech Tagset.pdf:application/pdf},
}

@article{unisist_indexing_1977,
	title = {Indexing {Principles}},
	volume = {4},
	issn = {0943-7444},
	url = {https://www.nomos-elibrary.de/index.php?doi=10.5771/0943-7444-1977-1-29},
	doi = {10.5771/0943-7444-1977-1-29},
	number = {1},
	urldate = {2021-03-08},
	journal = {KNOWLEDGE ORGANIZATION},
	author = {{UNISIST} and {UNESCO}},
	year = {1977},
	pages = {29--34},
	file = {ic_4_1977_1_g.pdf:/home/gallina-y/Zotero/storage/26276NT6/ic_4_1977_1_g.pdf:application/pdf;The UNISIST Draft on Indexing Principles. Text eBook (1977) / 0943-7444 | Nomos eLibrary:/home/gallina-y/Zotero/storage/4LAM33UC/the-unisist-draft-on-indexing-principles-text-volume-4-1977-issue-1.html:text/html},
}

@article{zhang_world_2004,
	title = {World {Wide} {Web} site summarization},
	volume = {2},
	issn = {1570-1263},
	url = {https://content.iospress.com/articles/web-intelligence-and-agent-systems-an-international-journal/wia00026},
	abstract = {Summaries of Web sites help Web users get an idea of the site contents without having to spend time browsing the sites. Currently, manually constructed summaries of Web sites by volunteer experts are available, such as the DMOZ Open Directory Project},
	language = {en},
	number = {1},
	urldate = {2021-03-11},
	journal = {Web Intelligence and Agent Systems: An International Journal},
	author = {Zhang, Yongzheng and Zincir-Heywood, Nur and Milios, Evangelos},
	month = jan,
	year = {2004},
	note = {Publisher: IOS Press},
	pages = {39--53},
	file = {Snapshot:/home/gallina-y/Zotero/storage/67WS7B4G/wia00026.html:text/html;Zhang et al. - World Wide Web Site Summarization.pdf:/home/gallina-y/Zotero/storage/22EWAFGG/Zhang et al. - World Wide Web Site Summarization.pdf:application/pdf},
}

@misc{noauthor_rules-based_2018,
	title = {Rules-based tagging for metadata},
	url = {https://media.lac-group.com/blog/rules-based-tagging-metadata/},
	abstract = {With a large volume of content, it’s tempting to solely use an automated system for tagging and annotation. While today’s machine learning and rules-based platforms can achieve a reasonable level of classification, they haven’t yet approached human ability when it comes to reading more deeply into the material and applying tags that might be relevant, but not obvious.  Librarians and taxonomists can breathe a sigh of relief – they won’t be replaced by machines any time soon! In fact, human discernment and reasoning are necessary for the accurate application of metadata tags.},
	language = {en-US},
	urldate = {2021-03-11},
	journal = {Media and Archive},
	month = may,
	year = {2018},
	note = {Section: Blog},
	file = {Snapshot:/home/gallina-y/Zotero/storage/RGALQV5B/rules-based-tagging-metadata.html:text/html},
}

@misc{ellis_new_2015,
	title = {The {New} {York} {Times} built a robot to help make article tagging easier},
	url = {https://www.niemanlab.org/2015/07/the-new-york-times-built-a-robot-to-help-making-article-tagging-easier/},
	abstract = {Developed by the Times R\&D lab, the Editor tool scans text to suggest article tags in real time. But the automatic tagging system won't be moving into the newsroom soon.},
	urldate = {2021-03-11},
	journal = {Nieman Lab},
	author = {Ellis, Justin},
	month = jun,
	year = {2015},
}

@inproceedings{luong_effective_2015,
	address = {Lisbon, Portugal},
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D15-1166},
	doi = {10.18653/v1/D15-1166},
	language = {en},
	urldate = {2021-03-15},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
	year = {2015},
	pages = {1412--1421},
	file = {Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:/home/gallina-y/Zotero/storage/6GX88WVW/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf},
}

@phdthesis{ramiandrisoa_detection_2020,
	title = {Détection des signaux faibles dans application des masses de données},
	author = {RAMIANDRISOA, Iarivony Faneva},
	year = {2020},
	file = {manuscrit_faneva.pdf:/home/gallina-y/Zotero/storage/4X27296Z/manuscrit_faneva.pdf:application/pdf},
}

@article{sun_joint_2020,
	title = {Joint {Keyphrase} {Chunking} and {Salience} {Ranking} with {BERT}},
	url = {http://arxiv.org/abs/2004.13639},
	abstract = {An effective keyphrase extraction system requires to produce self-contained high quality phrases that are also key to the document topic. This paper presents BERT-JointKPE, a multitask BERT-based model for keyphrase extraction. JointKPE employs a chunking network to identify high-quality phrases and a ranking network to learn their salience in the document. The model is trained jointly on the chunking task and the ranking task, balancing the estimation of keyphrase quality and salience. Experiments on two benchmarks demonstrate JointKPE’s robust effectiveness with different BERT variants. Our analyses show that JointKPE has advantages in predicting long keyphrases and extracting phrases that are not entities but also meaningful. The source code of this paper can be obtained from https: //github.com/thunlp/BERT-KPE.},
	language = {en},
	urldate = {2021-03-19},
	journal = {arXiv:2004.13639 [cs]},
	author = {Sun, Si and Xiong, Chenyan and Liu, Zhenghao and Liu, Zhiyuan and Bao, Jie},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.13639},
	keywords = {extraction, to read, code},
	file = {Sun et al. - 2020 - Joint Keyphrase Chunking and Salience Ranking with.pdf:/home/gallina-y/Zotero/storage/Q5RGVT3N/Sun et al. - 2020 - Joint Keyphrase Chunking and Salience Ranking with.pdf:application/pdf},
}

@book{minsky_perceptrons_1988,
	address = {Cambridge, MA, USA},
	title = {Perceptrons: expanded edition},
	isbn = {978-0-262-63111-2},
	shorttitle = {Perceptrons},
	publisher = {MIT Press},
	author = {Minsky, Marvin L. and Papert, Seymour A.},
	year = {1988},
}

@inproceedings{wang_disconnected_2018,
	address = {Melbourne, Australia},
	title = {Disconnected {Recurrent} {Neural} {Networks} for {Text} {Categorization}},
	url = {http://aclweb.org/anthology/P18-1215},
	doi = {10.18653/v1/P18-1215},
	abstract = {Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies, but it does not do well in extracting key patterns. In contrast, convolutional neural network (CNN) is good at extracting local and position-invariant features. In this paper, we present a novel model named disconnected recurrent neural network (DRNN), which incorporates position-invariance into RNN. By limiting the distance of information ﬂow in RNN, the hidden state at each time step is restricted to represent words near the current position. The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization.},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Baoxin},
	year = {2018},
	pages = {2311--2320},
	file = {Wang - 2018 - Disconnected Recurrent Neural Networks for Text Ca.pdf:/home/gallina-y/Zotero/storage/WHXFVY7V/Wang - 2018 - Disconnected Recurrent Neural Networks for Text Ca.pdf:application/pdf},
}

@inproceedings{zukov-gregoric_named_2018,
	address = {Melbourne, Australia},
	title = {Named {Entity} {Recognition} {With} {Parallel} {Recurrent} {Neural} {Networks}},
	url = {http://aclweb.org/anthology/P18-2012},
	doi = {10.18653/v1/P18-2012},
	abstract = {We present a new architecture for named entity recognition. Our model employs multiple independent bidirectional LSTM units across the same input and promotes diversity among them by employing an inter-model regularization term. By distributing computation across multiple smaller LSTMs we ﬁnd a reduction in the total number of parameters. We ﬁnd our architecture achieves state-of-the-art performance on the CoNLL 2003 NER dataset.},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Žukov-Gregorič, Andrej and Bachrach, Yoram and Coope, Sam},
	year = {2018},
	pages = {69--74},
	file = {Žukov-Gregorič et al. - 2018 - Named Entity Recognition With Parallel Recurrent N.pdf:/home/gallina-y/Zotero/storage/CUX3UUUF/Žukov-Gregorič et al. - 2018 - Named Entity Recognition With Parallel Recurrent N.pdf:application/pdf},
}

@article{sandhaus_new_2008,
	title = {The new york times annotated corpus},
	volume = {6},
	doi = {https://doi.org/10.35111/77ba-9x74},
	number = {12},
	journal = {Linguistic Data Consortium, Philadelphia},
	author = {Sandhaus, Evan},
	year = {2008},
	keywords = {dataset},
	pages = {e26752},
}

@article{brin_anatomy_1998,
	series = {Proceedings of the {Seventh} {International} {World} {Wide} {Web} {Conference}},
	title = {The anatomy of a large-scale hypertextual {Web} search engine},
	volume = {30},
	issn = {0169-7552},
	url = {https://www.sciencedirect.com/science/article/pii/S016975529800110X},
	doi = {10.1016/S0169-7552(98)00110-X},
	abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine — the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
	language = {en},
	number = {1},
	urldate = {2021-04-06},
	journal = {Computer Networks and ISDN Systems},
	author = {Brin, Sergey and Page, Lawrence},
	month = apr,
	year = {1998},
	pages = {107--117},
	file = {ScienceDirect Full Text PDF:/home/gallina-y/Zotero/storage/JZ7P7BME/Brin et Page - 1998 - The anatomy of a large-scale hypertextual Web sear.pdf:application/pdf;ScienceDirect Snapshot:/home/gallina-y/Zotero/storage/Y2DGQ47U/S016975529800110X.html:text/html},
}

@inproceedings{zhang_keyword_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Keyword {Extraction} {Using} {Support} {Vector} {Machine}},
	isbn = {978-3-540-35226-6},
	doi = {10.1007/11775300_8},
	abstract = {This paper is concerned with keyword extraction. By keyword extraction, we mean extracting a subset of words/phrases from a document that can describe the ‘meaning’ of the document. Keywords are of benefit to many text mining applications. However, a large number of documents do not have keywords and thus it is necessary to assign keywords before enjoying the benefit from it. Several research efforts have been done on keyword extraction. These methods make use of the ‘global context information’, which makes the performance of extraction restricted. A thorough and systematic investigation on the issue is thus needed. In this paper, we propose to make use of not only ‘global context information’, but also ‘local context information’ for extracting keywords from documents. As far as we know, utilizing both ‘global context information’ and ‘local context information’ in keyword extraction has not been sufficiently investigated previously. Methods for performing the tasks on the basis of Support Vector Machines have also been proposed in this paper. Features in the model have been defined. Experimental results indicate that the proposed SVM based method can significantly outperform the baseline methods for keyword extraction. The proposed method has been applied to document classification, a typical text mining processing. Experimental results show that the accuracy of document classification can be significantly improved by using the keyword extraction method.},
	language = {en},
	booktitle = {Advances in {Web}-{Age} {Information} {Management}},
	publisher = {Springer},
	author = {Zhang, Kuo and Xu, Hui and Tang, Jie and Li, Juanzi},
	editor = {Yu, Jeffrey Xu and Kitsuregawa, Masaru and Leong, Hong Va},
	year = {2006},
	keywords = {kpe},
	pages = {85--96},
	file = {Zhang et al. - 2006 - Keyword Extraction Using Support Vector Machine.pdf:/home/gallina-y/Zotero/storage/RMIYKVV8/Zhang et al. - 2006 - Keyword Extraction Using Support Vector Machine.pdf:application/pdf},
}

@inproceedings{medelyan_thesaurus_2006,
	address = {New York, NY, USA},
	series = {{JCDL} '06},
	title = {Thesaurus based automatic keyphrase indexing},
	isbn = {978-1-59593-354-6},
	url = {https://doi.org/10.1145/1141753.1141819},
	doi = {10.1145/1141753.1141819},
	abstract = {We propose a new method that enhances automatic keyphrase extraction by using semantic information on terms and phrases gleaned from a domain-specific thesaurus. We evaluate the results against keyphrase sets assigned by a state-of-the-art keyphrase extraction system and those assigned by six professional indexers.},
	urldate = {2021-04-12},
	booktitle = {Proceedings of the 6th {ACM}/{IEEE}-{CS} joint conference on {Digital} libraries},
	publisher = {Association for Computing Machinery},
	author = {Medelyan, Olena and Witten, Ian H.},
	month = jun,
	year = {2006},
	pages = {296--297},
	file = {Texte intégral:/home/gallina-y/Zotero/storage/B3JPD7LE/Medelyan et Witten - 2006 - Thesaurus based automatic keyphrase indexing.pdf:application/pdf},
}

@inproceedings{barker_using_2000,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Noun} {Phrase} {Heads} to {Extract} {Document} {Keyphrases}},
	isbn = {978-3-540-45486-1},
	doi = {10.1007/3-540-45486-1_4},
	abstract = {Automatically extracting keyphrases from documents is a task with many applications in information retrieval and natural language processing. Document retrieval can be biased towards documents containing relevant keyphrases; documents can be classified or categorized based on their keyphrases; automatic text summarization may extract sentences with high keyphrase scores.This paper describes a simple system for choosing noun phrases from a document as keyphrases. A noun phrase is chosen based on its length, its frequency and the frequency of its head noun. Noun phrases are extracted from a text using a base noun phrase skimmer and an off-the-shelf online dictionary.Experiments involving human judges reveal several interesting results: the simple noun phrase-based system performs roughly as well as a state-of-the-art, corpus-trained keyphrase extractor; ratings for individual keyphrases do not necessarily correlate with ratings for sets of keyphrases for a document; agreement among unbiased judges on the keyphrase rating task is poor.},
	language = {en},
	booktitle = {Advances in {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Barker, Ken and Cornacchia, Nadia},
	editor = {Hamilton, Howard J.},
	year = {2000},
	keywords = {kpe},
	pages = {40--52},
	file = {Barker et Cornacchia - 2000 - Using Noun Phrase Heads to Extract Document Keyphr.pdf:/home/gallina-y/Zotero/storage/FEMINRXK/Barker et Cornacchia - 2000 - Using Noun Phrase Heads to Extract Document Keyphr.pdf:application/pdf},
}

@article{robertson_okapi_1999,
	title = {Okapi at {TREC} 7: automatic ad hoc, ltering, {VLC} and interactive track},
	shorttitle = {Okapi at trec-7},
	language = {en},
	journal = {Proceedings of the Seventh Text REetrieval Conference (TREC-7), 1999},
	author = {Robertson, S E and Walker, S and Beaulieu, M},
	year = {1999},
	pages = {12},
	file = {Robertson et al. - Okapi at TREC 7 automatic ad hoc, ltering, VLC an.pdf:/home/gallina-y/Zotero/storage/PH8ZSP5E/Robertson et al. - Okapi at TREC 7 automatic ad hoc, ltering, VLC an.pdf:application/pdf},
}

@inproceedings{jones_human_2001,
	address = {New York, NY, USA},
	series = {{JCDL} '01},
	title = {Human evaluation of {Kea}, an automatic keyphrasing system},
	isbn = {978-1-58113-345-5},
	url = {https://doi.org/10.1145/379437.379473},
	doi = {10.1145/379437.379473},
	abstract = {This paper describes an evaluation of the Kea automatic keyphrase extr action algorithm. Tools that automatically identify keyphrases are desirable because document keyphrases have numerous applications in digital library systems, but are costly and time consuming to manually assign. Keyphrase extraction algorithms are usually evaluated by comparison to author-specified keywords, but this methodology has several well-known shortcomings. The results presented in this paper are based on subjective evaluations of the quality and appropriateness of keyphrases by human assessors, and make a number of contributions. First, they validate previous evaluations of Kea that rely on author keywords. Second, they show Kea's performance is comparable to that of similar systems that have been evaluated by human assessors. Finally, they justify the use of author keyphrases as a performance metric by showing that authors generally choose good keywords.},
	urldate = {2021-04-16},
	booktitle = {Proceedings of the 1st {ACM}/{IEEE}-{CS} joint conference on {Digital} libraries},
	publisher = {Association for Computing Machinery},
	author = {Jones, Steve and Paynter, Gordon W.},
	month = jan,
	year = {2001},
	keywords = {meta},
	pages = {148--156},
	file = {ED459824.pdf:/home/gallina-y/Zotero/storage/6VYQEB43/ED459824.pdf:application/pdf},
}

@inproceedings{partalas_results_2013,
	title = {Results of the {First} {BioASQ} {Workshop}},
	url = {https://hal.archives-ouvertes.fr/hal-00953048},
	abstract = {The goal of the BioASQ project is to push the research frontier towards hybrid information systems. We aim to promote systems and approaches that are able to deal with the whole diversity of the Web, especially for, but not restricted to the context of bio-medicine. This goal is pursued by the organization of challenges. The first challenge consisted of two tasks: semantic indexing and question answering. 157 systems were registered by 12 different participants for the semantic indexing task, of which between 19 and 29 participated in each batch. The question answering task was tackled by 15 systems, which were developed by three different organizations. Between 2 and 5 of these systems addressed each batch. Overall, the best systems were able to outperform the strong baselines provided in the experiments in two out of three settings. This suggests that advances over the state of the art were achieved through the BioASQ challenge but also that the benchmark in itself is very challenging. In this paper, we present the data used during the challenge as well as the technologies which were at the core of the participants' frameworks.},
	language = {en},
	urldate = {2021-04-16},
	booktitle = {Proceedings of the first {Workshop} on {Bio}-{Medical} {Semantic} {Indexing} and {Question} {Answering}, a {Post}-{Conference} {Workshop} of {Conference} and {Labs} of the {Evaluation} {Forum} 2013},
	author = {Partalas, Ioannis and Gaussier, Éric and Ngomo, œ},
	year = {2013},
	pages = {1},
	file = {Snapshot:/home/gallina-y/Zotero/storage/C85NTLBE/hal-00953048.html:text/html},
}

@inproceedings{boudin_keyphrase_2020,
	address = {Online},
	title = {Keyphrase {Generation} for {Scientific} {Document} {Retrieval}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.105},
	doi = {10.18653/v1/2020.acl-main.105},
	abstract = {Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval. This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models. Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains. Our code is available at https://github.com/boudinfl/ir-using-kg},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Boudin, Florian and Gallina, Ygor and Aizawa, Akiko},
	month = jul,
	year = {2020},
	keywords = {meta},
	pages = {1118--1126},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/BTVB9H92/Boudin et al. - 2020 - Keyphrase Generation for Scientific Document Retri.pdf:application/pdf},
}

@inproceedings{lai_joint_2020,
	address = {Barcelona, Spain (Online)},
	title = {A {Joint} {Learning} {Approach} based on {Self}-{Distillation} for {Keyphrase} {Extraction} from {Scientific} {Documents}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.56},
	doi = {10.18653/v1/2020.coling-main.56},
	abstract = {Keyphrase extraction is the task of extracting a small set of phrases that best describe a document. Most existing benchmark datasets for the task typically have limited numbers of annotated documents, making it challenging to train increasingly complex neural networks. In contrast, digital libraries store millions of scientiﬁc articles online, covering a wide range of topics. While a signiﬁcant portion of these articles contain keyphrases provided by their authors, most other articles lack such kind of annotations. Therefore, to effectively utilize these large amounts of unlabeled articles, we propose a simple and efﬁcient joint learning approach based on the idea of self-distillation. Experimental results show that our approach consistently improves the performance of baseline models for keyphrase extraction. Furthermore, our best models outperform previous methods for the task, achieving new state-of-the-art results on two public benchmarks: Inspec and SemEval-2017.},
	language = {en},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Lai, Tuan and Bui, Trung and Kim, Doo Soon and Tran, Quan Hung},
	year = {2020},
	keywords = {extraction, to read},
	pages = {649--656},
	file = {Lai et al. - 2020 - A Joint Learning Approach based on Self-Distillati.pdf:/home/gallina-y/Zotero/storage/QXV8ZVRB/Lai et al. - 2020 - A Joint Learning Approach based on Self-Distillati.pdf:application/pdf},
}

@inproceedings{bahuleyan_diverse_2020,
	address = {Barcelona, Spain (Online)},
	title = {Diverse {Keyphrase} {Generation} with {Neural} {Unlikelihood} {Training}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.462},
	doi = {10.18653/v1/2020.coling-main.462},
	language = {en},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Bahuleyan, Hareesh and El Asri, Layla},
	year = {2020},
	keywords = {generation, to read, code},
	pages = {5271--5287},
	file = {Bahuleyan et El Asri - 2020 - Diverse Keyphrase Generation with Neural Unlikelih.pdf:/home/gallina-y/Zotero/storage/GE95JHL8/Bahuleyan et El Asri - 2020 - Diverse Keyphrase Generation with Neural Unlikelih.pdf:application/pdf},
}

@inproceedings{saxena_keygames_2020,
	address = {Barcelona, Spain (Online)},
	title = {{KeyGames}: {A} {Game} {Theoretic} {Approach} to {Automatic} {Keyphrase} {Extraction}},
	shorttitle = {{KeyGames}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.184},
	doi = {10.18653/v1/2020.coling-main.184},
	abstract = {In this paper, we introduce two advancements in the automatic keyphrase extraction (AKE) space - KeyGames and pke+. KeyGames is an unsupervised AKE framework that employs the concept of evolutionary game theory and consistent labelling problem to ensure consistent classification of candidates into keyphrase and non-keyphrase. Pke+ is a python based pipeline built on top of the existing pke library to standardize various AKE steps, namely candidate extraction and evaluation, to ensure truly systematic and comparable performance analysis of AKE models. In the experiments section, we compare the performance of KeyGames across three publicly available datasets (Inspec 2001, SemEval 2010, DUC 2001) against the results quoted by the existing state-of-the-art models as well as their performance when reproduced using pke+. The results show that KeyGames outperforms most of the state-of-the-art systems while generalizing better on input documents with different domains and length. Further, pke+'s pre-processing brings out improvement in several other system's quoted performance as well.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Saxena, Arnav and Mangal, Mudit and Jain, Goonjan},
	month = dec,
	year = {2020},
	keywords = {kpe, to read},
	pages = {2037--2048},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/DMP6X544/Saxena et al. - 2020 - KeyGames A Game Theoretic Approach to Automatic K.pdf:application/pdf},
}

@inproceedings{park_scientific_2020,
	address = {Barcelona, Spain (Online)},
	title = {Scientific {Keyphrase} {Identification} and {Classification} by {Pre}-{Trained} {Language} {Models} {Intermediate} {Task} {Transfer} {Learning}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.472},
	doi = {10.18653/v1/2020.coling-main.472},
	abstract = {Scientific keyphrase identification and classification is the task of detecting and classifying keyphrases from scholarly text with their types from a set of predefined classes. This task has a wide range of benefits, but it is still challenging in performance due to the lack of large amounts of labeled data required for training deep neural models. In order to overcome this challenge, we explore pre-trained language models BERT and SciBERT with intermediate task transfer learning, using 42 data-rich related intermediate-target task combinations. We reveal that intermediate task transfer learning on SciBERT induces a better starting point for target task fine-tuning compared with BERT and achieves competitive performance in scientific keyphrase identification and classification compared to both previous works and strong baselines. Interestingly, we observe that BERT with intermediate task transfer learning fails to improve the performance of scientific keyphrase identification and classification potentially due to significant catastrophic forgetting. This result highlights that scientific knowledge achieved during the pre-training of language models on large scientific collections plays an important role in the target tasks. We also observe that sequence tagging related intermediate tasks, especially syntactic structure learning tasks such as POS Tagging, tend to work best for scientific keyphrase identification and classification.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Park, Seoyeon and Caragea, Cornelia},
	month = dec,
	year = {2020},
	keywords = {extraction, to read},
	pages = {5409--5419},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/5CWZRRLW/Park et Caragea - 2020 - Scientific Keyphrase Identification and Classifica.pdf:application/pdf},
}

@inproceedings{cano_two_2020,
	address = {Marseille, France},
	title = {Two {Huge} {Title} and {Keyword} {Generation} {Corpora} of {Research} {Articles}},
	isbn = {979-10-95546-34-4},
	url = {https://www.aclweb.org/anthology/2020.lrec-1.823},
	abstract = {Recent developments in sequence-to-sequence learning with neural networks have considerably improved the quality of automatically generated text summaries and document keywords, stipulating the need for even bigger training corpora. Metadata of research articles are usually easy to find online and can be used to perform research on various tasks. In this paper, we introduce two huge datasets for text summarization (OAGSX) and keyword generation (OAGKX) research, containing 34 million and 23 million records, respectively. The data were retrieved from the Open Academic Graph which is a network of research profiles and publications. We carefully processed each record and also tried several extractive and abstractive methods of both tasks to create performance baselines for other researchers. We further illustrate the performance of those methods previewing their outputs. In the near future, we would like to apply topic modeling on the two sets to derive subsets of research articles from more specific disciplines.},
	language = {English},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Çano, Erion and Bojar, Ondřej},
	month = may,
	year = {2020},
	keywords = {dataset},
	pages = {6663--6671},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/YKEW4K7C/Çano et Bojar - 2020 - Two Huge Title and Keyword Generation Corpora of R.pdf:application/pdf},
}

@inproceedings{patel_exploiting_2021,
	address = {Online},
	title = {Exploiting {Position} and {Contextual} {Word} {Embeddings} for {Keyphrase} {Extraction} from {Scientific} {Papers}},
	url = {https://www.aclweb.org/anthology/2021.eacl-main.136},
	abstract = {Keyphrases associated with research papers provide an effective way to find useful information in the large and growing scholarly digital collections. In this paper, we present KPRank, an unsupervised graph-based algorithm for keyphrase extraction that exploits both positional information and contextual word embeddings into a biased PageRank. Our experimental results on five benchmark datasets show that KPRank that uses contextual word embeddings with additional position signal outperforms previous approaches and strong baselines for this task.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Patel, Krutarth and Caragea, Cornelia},
	month = apr,
	year = {2021},
	keywords = {kpe, to read},
	pages = {1585--1591},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/T35DWL9P/Patel et Caragea - 2021 - Exploiting Position and Contextual Word Embeddings.pdf:application/pdf},
}

@inproceedings{cano_keyphrase_2019-1,
	address = {Minneapolis, Minnesota},
	title = {Keyphrase {Generation}: {A} {Text} {Summarization} {Struggle}},
	shorttitle = {Keyphrase {Generation}},
	url = {https://www.aclweb.org/anthology/N19-1070},
	doi = {10.18653/v1/N19-1070},
	abstract = {Authors' keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment with popular text summarization neural architectures. Despite using advanced deep learning models, large quantities of data and many days of computation, our systematic evaluation on four test datasets reveals that the explored text summarization methods could not produce better keyphrases than the simpler unsupervised methods, or the existing supervised ones.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Çano, Erion and Bojar, Ondřej},
	month = jun,
	year = {2019},
	keywords = {meta},
	pages = {666--672},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/REH2IBRE/Çano et Bojar - 2019 - Keyphrase Generation A Text Summarization Struggl.pdf:application/pdf},
}

@inproceedings{koloski_extending_2021,
	address = {Online},
	title = {Extending {Neural} {Keyword} {Extraction} with {TF}-{IDF} tagset matching},
	url = {https://www.aclweb.org/anthology/2021.hackashop-1.4},
	abstract = {Keyword extraction is the task of identifying words (or multi-word expressions) that best describe a given document and serve in news portals to link articles of similar topics. In this work, we develop and evaluate our methods on four novel data sets covering less-represented, morphologically-rich languages in European news media industry (Croatian, Estonian, Latvian, and Russian). First, we perform evaluation of two supervised neural transformer-based methods, Transformer-based Neural Tagger for Keyword Identification (TNT-KID) and Bidirectional Encoder Representations from Transformers (BERT) with an additional Bidirectional Long Short-Term Memory Conditional Random Fields (BiLSTM CRF) classification head, and compare them to a baseline Term Frequency - Inverse Document Frequency (TF-IDF) based unsupervised approach. Next, we show that by combining the keywords retrieved by both neural transformer-based methods and extending the final set of keywords with an unsupervised TF-IDF based technique, we can drastically improve the recall of the system, making it appropriate for usage as a recommendation system in the media house environment.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the {EACL} {Hackashop} on {News} {Media} {Content} {Analysis} and {Automated} {Report} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Koloski, Boshko and Pollak, Senja and Škrlj, Blaž and Martinc, Matej},
	month = apr,
	year = {2021},
	keywords = {extraction, to read, code},
	pages = {22--29},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/CRF5TW8E/Koloski et al. - 2021 - Extending Neural Keyword Extraction with TF-IDF ta.pdf:application/pdf},
}

@inproceedings{yuan_one_2020,
	address = {Online},
	title = {One {Size} {Does} {Not} {Fit} {All}: {Generating} and {Evaluating} {Variable} {Number} of {Keyphrases}},
	shorttitle = {One {Size} {Does} {Not} {Fit} {All}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.710},
	doi = {10.18653/v1/2020.acl-main.710},
	abstract = {Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives. We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs. We further propose two evaluation metrics tailored towards the variable-number generation. We also introduce a new dataset StackEx that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Yuan, Xingdi and Wang, Tong and Meng, Rui and Thaker, Khushboo and Brusilovsky, Peter and He, Daqing and Trischler, Adam},
	month = jul,
	year = {2020},
	keywords = {generation, o2m, code},
	pages = {7961--7975},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/U4E7KUJJ/Yuan et al. - 2020 - One Size Does Not Fit All Generating and Evaluati.pdf:application/pdf},
}

@inproceedings{wang_topic-aware_2019,
	address = {Florence, Italy},
	title = {Topic-{Aware} {Neural} {Keyphrase} {Generation} for {Social} {Media} {Language}},
	url = {https://www.aclweb.org/anthology/P19-1240},
	doi = {10.18653/v1/P19-1240},
	abstract = {A huge volume of user-generated content is daily produced on social media. To facilitate automatic language understanding, we study keyphrase prediction, distilling salient information from massive posts. While most existing methods extract words from source posts to form keyphrases, we propose a sequence-to-sequence (seq2seq) based neural keyphrase generation framework, enabling absent keyphrases to be created. Moreover, our model, being topic-aware, allows joint modeling of corpus-level latent topic representations, which helps alleviate data sparsity widely exhibited in social media language. Experiments on three datasets collected from English and Chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics. Further discussions show that our model learns meaningful topics, which interprets its superiority in social media keyphrase generation.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yue and Li, Jing and Chan, Hou Pong and King, Irwin and Lyu, Michael R. and Shi, Shuming},
	month = jul,
	year = {2019},
	keywords = {generation, code},
	pages = {2516--2526},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/FQBZP43P/Wang et al. - 2019 - Topic-Aware Neural Keyphrase Generation for Social.pdf:application/pdf},
}

@inproceedings{skianis_rep_2020,
	title = {Rep the {Set}: {Neural} {Networks} for {Learning} {Set} {Representations}},
	shorttitle = {Rep the {Set}},
	url = {http://proceedings.mlr.press/v108/skianis20a.html},
	abstract = {In several domains, data objects can be decomposed into sets of simpler objects. It is then natural to represent each object as the set of its components or parts. Many conventional machine learnin...},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Skianis, Konstantinos and Nikolentzos, Giannis and Limnios, Stratis and Vazirgiannis, Michalis},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {set},
	pages = {1410--1420},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/PZ27RWXY/Skianis et al. - 2020 - Rep the Set Neural Networks for Learning Set Repr.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/IM5K38HG/skianis20a.html:text/html},
}

@inproceedings{hou_identification_2019,
	address = {Florence, Italy},
	title = {Identification of {Tasks}, {Datasets}, {Evaluation} {Metrics}, and {Numeric} {Scores} for {Scientific} {Leaderboards} {Construction}},
	url = {https://www.aclweb.org/anthology/P19-1513},
	doi = {10.18653/v1/P19-1513},
	abstract = {While the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions, keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult. The community could greatly benefit from an automatic system able to summarize scientific results, e.g., in the form of a leaderboard. In this paper we build two datasets and develop a framework (TDMS-IE) aimed at automatically extracting task, dataset, metric and score from NLP papers, towards the automatic construction of leaderboards. Experiments show that our model outperforms several baselines by a large margin. Our model is a first step towards automatic leaderboard construction, e.g., in the NLP domain.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hou, Yufang and Jochim, Charles and Gleize, Martin and Bonin, Francesca and Ganguly, Debasis},
	month = jul,
	year = {2019},
	pages = {5203--5213},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/8DTWQJCJ/Hou et al. - 2019 - Identification of Tasks, Datasets, Evaluation Metr.pdf:application/pdf},
}

@inproceedings{holtzman_curious_2019,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {https://openreview.net/forum?id=rygGQyrFvH},
	abstract = {Current language generation systems either aim for high likelihood and devolve into generic repetition or miscalibrate their stochasticity—we provide evidence of both and propose a solution:...},
	language = {en},
	urldate = {2021-04-28},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = sep,
	year = {2019},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/5XFU2GMN/Holtzman et al. - 2019 - The Curious Case of Neural Text Degeneration.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/3TYSJKUA/forum.html:text/html},
}

@inproceedings{hajdik_neural_2019,
	address = {Minneapolis, Minnesota},
	title = {Neural {Text} {Generation} from {Rich} {Semantic} {Representations}},
	url = {https://www.aclweb.org/anthology/N19-1235},
	doi = {10.18653/v1/N19-1235},
	abstract = {We propose neural models to generate high-quality text from structured representations based on Minimal Recursion Semantics (MRS). MRS is a rich semantic representation that encodes more precise semantic detail than other representations such as Abstract Meaning Representation (AMR). We show that a sequence-to-sequence model that maps a linearization of Dependency MRS, a graph-based representation of MRS, to text can achieve a BLEU score of 66.11 when trained on gold data. The performance of the model can be improved further using a high-precision, broad coverage grammar-based parser to generate a large silver training corpus, achieving a final BLEU score of 77.17 on the full test set, and 83.37 on the subset of test data most closely matching the silver data domain. Our results suggest that MRS-based representations are a good choice for applications that need both structured semantics and the ability to produce natural language text as output.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hajdik, Valerie and Buys, Jan and Goodman, Michael Wayne and Bender, Emily M.},
	month = jun,
	year = {2019},
	pages = {2259--2266},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/4GAMHGUS/Hajdik et al. - 2019 - Neural Text Generation from Rich Semantic Represen.pdf:application/pdf},
}

@article{gao_sequential_2019,
	title = {A {Sequential} {Set} {Generation} {Method} for {Predicting} {Set}-{Valued} {Outputs}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4136},
	doi = {10.1609/aaai.v33i01.33012835},
	language = {en},
	number = {01},
	urldate = {2021-04-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Gao, Tian and Chen, Jie and Chenthamarakshan, Vijil and Witbrock, Michael},
	month = jul,
	year = {2019},
	note = {Number: 01},
	keywords = {set},
	pages = {2835--2842},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/XJZ7WDIY/Gao et al. - 2019 - A Sequential Set Generation Method for Predicting .pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://www.aclweb.org/anthology/N19-1423},
	doi = {10.18653/v1/N19-1423},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/UGVBZYY4/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{beltagy_scibert_2019,
	address = {Hong Kong, China},
	title = {{SciBERT}: {A} {Pretrained} {Language} {Model} for {Scientific} {Text}},
	shorttitle = {{SciBERT}},
	url = {https://www.aclweb.org/anthology/D19-1371},
	doi = {10.18653/v1/D19-1371},
	abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
	month = nov,
	year = {2019},
	pages = {3615--3620},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/RJM5CDUC/Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf:application/pdf},
}

@inproceedings{balakrishnan_constrained_2019,
	address = {Florence, Italy},
	title = {Constrained {Decoding} for {Neural} {NLG} from {Compositional} {Representations} in {Task}-{Oriented} {Dialogue}},
	url = {https://www.aclweb.org/anthology/P19-1080},
	doi = {10.18653/v1/P19-1080},
	abstract = {Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Balakrishnan, Anusha and Rao, Jinfeng and Upasani, Kartikeya and White, Michael and Subba, Rajen},
	month = jul,
	year = {2019},
	pages = {831--844},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/CCI8ZP84/Balakrishnan et al. - 2019 - Constrained Decoding for Neural NLG from Compositi.pdf:application/pdf},
}

@inproceedings{yang_deep_2019,
	address = {Florence, Italy},
	title = {A {Deep} {Reinforced} {Sequence}-to-{Set} {Model} for {Multi}-{Label} {Classification}},
	url = {https://www.aclweb.org/anthology/P19-1518},
	doi = {10.18653/v1/P19-1518},
	abstract = {Multi-label classification (MLC) aims to predict a set of labels for a given instance. Based on a pre-defined label order, the sequence-to-sequence (Seq2Seq) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels. However, the output labels are essentially an unordered set rather than an ordered sequence. This inconsistency tends to result in some intractable problems, e.g., sensitivity to the label order. To remedy this, we propose a simple but effective sequence-to-set model. The proposed model is trained via reinforcement learning, where reward feedback is designed to be independent of the label order. In this way, we can reduce the dependence of the model on the label order, as well as capture high-order correlations between labels. Extensive experiments show that our approach can substantially outperform competitive baselines, as well as effectively reduce the sensitivity to the label order.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Pengcheng and Luo, Fuli and Ma, Shuming and Lin, Junyang and Sun, Xu},
	month = jul,
	year = {2019},
	keywords = {set},
	pages = {5252--5258},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/XBJEJKCZ/Yang et al. - 2019 - A Deep Reinforced Sequence-to-Set Model for Multi-.pdf:application/pdf},
}

@inproceedings{varamin_deep_2018,
	address = {New York, NY, USA},
	series = {{MobiQuitous} '18},
	title = {Deep {Auto}-{Set}: {A} {Deep} {Auto}-{Encoder}-{Set} {Network} for {Activity} {Recognition} {Using} {Wearables}},
	isbn = {978-1-4503-6093-7},
	shorttitle = {Deep {Auto}-{Set}},
	url = {https://doi.org/10.1145/3286978.3287024},
	doi = {10.1145/3286978.3287024},
	abstract = {Automatic recognition of human activities from time-series sensor data (referred to as HAR) is a growing area of research in ubiquitous computing. Most recent research in the field adopts supervised deep learning paradigms to automate extraction of intrinsic features from raw signal inputs and addresses HAR as a multi-class classification problem where detecting a single activity class within the duration of a sensory data segment suffices. However, due to the innate diversity of human activities and their corresponding duration, no data segment is guaranteed to contain sensor recordings of a single activity type. In this paper, we express HAR more naturally as a set prediction problem where the predictions are sets of ongoing activity elements with unfixed and unknown cardinality. For the first time, we address this problem by presenting a novel HAR approach that learns to output activity sets using deep neural networks. Moreover, motivated by the limited availability of annotated HAR datasets as well as the unfortunate immaturity of existing unsupervised systems, we complement our supervised set learning scheme with a prior unsupervised feature learning process that adopts convolutional auto-encoders to exploit unlabeled data. The empirical experiments on two widely adopted HAR datasets demonstrate the substantial improvement of our proposed methodology over the baseline models.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 15th {EAI} {International} {Conference} on {Mobile} and {Ubiquitous} {Systems}: {Computing}, {Networking} and {Services}},
	publisher = {Association for Computing Machinery},
	author = {Varamin, Alireza Abedin and Abbasnejad, Ehsan and Shi, Qinfeng and Ranasinghe, Damith C. and Rezatofighi, Hamid},
	month = nov,
	year = {2018},
	keywords = {set},
	pages = {246--253},
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {https://www.aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = jun,
	year = {2018},
	keywords = {elmo},
	pages = {2227--2237},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/L98MGTK5/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf:application/pdf},
}

@inproceedings{nema_diversity_2017,
	address = {Vancouver, Canada},
	title = {Diversity driven attention model for query-based abstractive summarization},
	url = {https://www.aclweb.org/anthology/P17-1098},
	doi = {10.18653/v1/P17-1098},
	abstract = {Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28\% (absolute) in ROUGE-L scores.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Nema, Preksha and Khapra, Mitesh M. and Laha, Anirban and Ravindran, Balaraman},
	month = jul,
	year = {2017},
	pages = {1063--1072},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/B2ERW9LX/Nema et al. - 2017 - Diversity driven attention model for query-based a.pdf:application/pdf},
}

@inproceedings{goo_abstractive_2018,
	title = {Abstractive {Dialogue} {Summarization} with {Sentence}-{Gated} {Modeling} {Optimized} by {Dialogue} {Acts}},
	doi = {10.1109/SLT.2018.8639531},
	abstract = {Neural abstractive summarization has been increasingly studied, where the prior work mainly focused on summarizing single-speaker documents (news, scientific publications, etc). In dialogues, there are diverse interactive patterns between speakers, which are usually defined as dialogue acts. The interactive signals may provide informative cues for better summarizing dialogues. This paper proposes to explicitly leverage dialogue acts in a neural summarization model, where a sentence-gated mechanism is designed for modeling the relationships between dialogue acts and the summary. The experiments show that our proposed model significantly improves the abstractive summarization performance compared to the state-of-the-art baselines on the AMI meeting corpus, demonstrating the usefulness of the interactive signal provided by dialogue acts.1},
	booktitle = {2018 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Goo, Chih-Wen and Chen, Yun-Nung},
	month = dec,
	year = {2018},
	pages = {735--742},
	file = {arXiv\:1809.05715 PDF:/home/gallina-y/Zotero/storage/QTKQJZIT/Goo and Chen - 2018 - Abstractive Dialogue Summarization with Sentence-G.pdf:application/pdf;IEEE Xplore Abstract Record:/home/gallina-y/Zotero/storage/4YWMQNU2/8639531.html:text/html},
}

@inproceedings{vaswani_attention_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	pages = {6000--6010},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/6YCRIGE5/Vaswani et al. - 2017 - Attention is all you need.pdf:application/pdf},
}

@inproceedings{subramanian_neural_2018,
	address = {Melbourne, Australia},
	title = {Neural {Models} for {Key} {Phrase} {Extraction} and {Question} {Generation}},
	url = {https://www.aclweb.org/anthology/W18-2609},
	doi = {10.18653/v1/W18-2609},
	abstract = {We propose a two-stage neural model to tackle question generation from documents. First, our model estimates the probability that word sequences in a document are ones that a human would pick when selecting candidate answers by training a neural key-phrase extractor on the answers in a question-answering corpus. Predicted key phrases then act as target answers and condition a sequence-to-sequence question-generation model with a copy mechanism. Empirically, our key-phrase extraction model significantly outperforms an entity-tagging baseline and existing rule-based approaches. We further demonstrate that our question generation system formulates fluent, answerable questions from key phrases. This two-stage system could be used to augment or generate reading comprehension datasets, which may be leveraged to improve machine reading systems or in educational settings.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the {Workshop} on {Machine} {Reading} for {Question} {Answering}},
	publisher = {Association for Computational Linguistics},
	author = {Subramanian, Sandeep and Wang, Tong and Yuan, Xingdi and Zhang, Saizheng and Trischler, Adam and Bengio, Yoshua},
	month = jul,
	year = {2018},
	keywords = {kpe},
	pages = {78--88},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/45IB76W2/Subramanian et al. - 2018 - Neural Models for Key Phrase Extraction and Questi.pdf:application/pdf},
}

@inproceedings{rei_semi-supervised_2017,
	address = {Vancouver, Canada},
	title = {Semi-supervised {Multitask} {Learning} for {Sequence} {Labeling}},
	url = {https://www.aclweb.org/anthology/P17-1194},
	doi = {10.18653/v1/P17-1194},
	abstract = {We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rei, Marek},
	month = jul,
	year = {2017},
	pages = {2121--2130},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/DRDG8HIX/Rei - 2017 - Semi-supervised Multitask Learning for Sequence La.pdf:application/pdf},
}

@inproceedings{bingel_identifying_2017,
	address = {Valencia, Spain},
	title = {Identifying beneficial task relations for multi-task learning in deep neural networks},
	url = {https://www.aclweb.org/anthology/E17-2026},
	abstract = {Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Volume} 2, {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Bingel, Joachim and Søgaard, Anders},
	month = apr,
	year = {2017},
	pages = {164--169},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/4YRYA2UD/Bingel et Søgaard - 2017 - Identifying beneficial task relations for multi-ta.pdf:application/pdf},
}

@inproceedings{mou_how_2016,
	address = {Austin, Texas},
	title = {How {Transferable} are {Neural} {Networks} in {NLP} {Applications}?},
	url = {https://www.aclweb.org/anthology/D16-1046},
	doi = {10.18653/v1/D16-1046},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mou, Lili and Meng, Zhao and Yan, Rui and Li, Ge and Xu, Yan and Zhang, Lu and Jin, Zhi},
	month = nov,
	year = {2016},
	pages = {479--489},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/5CYSXXDH/Mou et al. - 2016 - How Transferable are Neural Networks in NLP Applic.pdf:application/pdf},
}

@inproceedings{li_diversity-promoting_2016,
	address = {San Diego, California},
	title = {A {Diversity}-{Promoting} {Objective} {Function} for {Neural} {Conversation} {Models}},
	url = {https://www.aclweb.org/anthology/N16-1014},
	doi = {10.18653/v1/N16-1014},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
	month = jun,
	year = {2016},
	keywords = {distinct-n, metric},
	pages = {110--119},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/B42X3HBG/Li et al. - 2016 - A Diversity-Promoting Objective Function for Neura.pdf:application/pdf},
}

@inproceedings{lei_rationalizing_2016,
	address = {Austin, Texas},
	title = {Rationalizing {Neural} {Predictions}},
	url = {https://www.aclweb.org/anthology/D16-1011},
	doi = {10.18653/v1/D16-1011},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	month = nov,
	year = {2016},
	pages = {107--117},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/WCGUKEMC/Lei et al. - 2016 - Rationalizing Neural Predictions.pdf:application/pdf},
}

@inproceedings{vinyals_order_2016,
	title = {Order matters: {Sequence} to sequence for sets},
	shorttitle = {Order matters},
	url = {http://arxiv.org/abs/1511.06391},
	urldate = {2021-04-28},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
	year = {2016},
	keywords = {set},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/8JS3GX56/Vinyals et al. - 2016 - Order matters Sequence to sequence for sets.pdf:application/pdf},
}

@inproceedings{luong_addressing_2015,
	address = {Beijing, China},
	title = {Addressing the {Rare} {Word} {Problem} in {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/P15-1002},
	doi = {10.3115/v1/P15-1002},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Luong, Thang and Sutskever, Ilya and Le, Quoc and Vinyals, Oriol and Zaremba, Wojciech},
	month = jul,
	year = {2015},
	pages = {11--19},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/DXJVFDIY/Luong et al. - 2015 - Addressing the Rare Word Problem in Neural Machine.pdf:application/pdf},
}

@inproceedings{sutskever_sequence_2014,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'14},
	title = {Sequence to sequence learning with neural networks},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {MIT Press},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = dec,
	year = {2014},
	pages = {3104--3112},
	file = {Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:/home/gallina-y/Zotero/storage/J6JTQ4SV/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@inproceedings{cho_learning_2014,
	address = {Doha, Qatar},
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/D14-1179},
	doi = {10.3115/v1/D14-1179},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = oct,
	year = {2014},
	keywords = {gru},
	pages = {1724--1734},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/YPYCVC66/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf:application/pdf},
}

@inproceedings{turney_coherent_2003,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'03},
	title = {Coherent keyphrase extraction via web mining},
	abstract = {Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. A limitation of previous keyphrase extraction algorithms is that the selected keyphrases are occasionally incoherent. That is, the majority of the output keyphrases may fit together well, but there may be a minority that appear to be outliers, with no clear semantic relation to the majority or to each other. This paper presents enhancements to the Kea keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. The approach is to use the degree of statistical association among candidate keyphrases as evidence that they may be semantically related. The statistical association is measured using web mining. Experiments demonstrate that the enhancements improve the quality of the extracted keyphrases. Furthermore, the enhancements are not domain-specific: the algorithm generalizes well when it is trained on one domain (computer science documents) and tested on another (physics documents).},
	urldate = {2021-04-28},
	booktitle = {Proceedings of the 18th international joint conference on {Artificial} intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Turney, Peter D.},
	month = aug,
	year = {2003},
	keywords = {kpe},
	pages = {434--439},
	file = {arXiv\:cs/0308033 PDF:/home/gallina-y/Zotero/storage/QTTDEGR2/Turney - 2003 - Coherent Keyphrase Extraction via Web Mining.pdf:application/pdf},
}

@article{shen_unsupervised_2021,
	title = {Unsupervised {Deep} {Keyphrase} {Generation}},
	url = {http://arxiv.org/abs/2104.08729},
	abstract = {Keyphrase generation aims to summarize long documents with a collection of salient phrases. Deep neural models have demonstrated a remarkable success in this task, capable of predicting keyphrases that are even absent from a document. However, such abstractiveness is acquired at the expense of a substantial amount of annotated data. In this paper, we present a novel method for keyphrase generation, AutoKeyGen, without the supervision of any human annotation. Motivated by the observation that an absent keyphrase in one document can appear in other places, in whole or in part, we ﬁrst construct a phrase bank by pooling all phrases in a corpus. With this phrase bank, we then draw candidate absent keyphrases for each document through a partial matching process. To rank both types of candidates, we combine their lexical- and semantic-level similarities to the input document. Moreover, we utilize these top-ranked candidates as to train a deep generative model for more absent keyphrases. Extensive experiments demonstrate that AutoKeyGen outperforms all unsupervised baselines and can even beat strong supervised method in certain cases.},
	language = {en},
	urldate = {2021-04-28},
	journal = {arXiv:2104.08729 [cs]},
	author = {Shen, Xianjie and Wang, Yinghan and Meng, Rui and Shang, Jingbo},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.08729},
	keywords = {generation, code},
	file = {Shen et al. - 2021 - Unsupervised Deep Keyphrase Generation.pdf:/home/gallina-y/Zotero/storage/3G9HSHGT/Shen et al. - 2021 - Unsupervised Deep Keyphrase Generation.pdf:application/pdf},
}

@inproceedings{santosh_sasake_2020,
	address = {Barcelona, Spain (Online)},
	title = {{SaSAKE}: {Syntax} and {Semantics} {Aware} {Keyphrase} {Extraction} from {Research} {Papers}},
	shorttitle = {{SaSAKE}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.469},
	doi = {10.18653/v1/2020.coling-main.469},
	abstract = {Keyphrases in a research paper succinctly capture the primary content of the paper and also assist in indexing the paper at a concept level. Given the huge rate at which scientific papers are published today, it is important to have effective ways of automatically extracting keyphrases from a research paper. In this paper, we present a novel method, Syntax and Semantics Aware Keyphrase Extraction (SaSAKE), to extract keyphrases from research papers. It uses a transformer architecture, stacking up sentence encoders to incorporate sequential information, and graph encoders to incorporate syntactic and semantic dependency graph information. Incorporation of these dependency graphs helps to alleviate long-range dependency problems and identify the boundaries of multi-word keyphrases effectively. Experimental results on three benchmark datasets show that our proposed method SaSAKE achieves state-of-the-art performance in keyphrase extraction from scientific papers.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Santosh, T.y.s.s and Kumar Sanyal, Debarshi and Bhowmick, Plaban Kumar and Das, Partha Pratim},
	month = dec,
	year = {2020},
	keywords = {extraction},
	pages = {5372--5383},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/IEJ5X5DX/Santosh et al. - 2020 - SaSAKE Syntax and Semantics Aware Keyphrase Extra.pdf:application/pdf},
}

@inproceedings{augenstein_multi-task_2017,
	address = {Vancouver, Canada},
	title = {Multi-{Task} {Learning} of {Keyphrase} {Boundary} {Classification}},
	url = {https://www.aclweb.org/anthology/P17-2054},
	doi = {10.18653/v1/P17-2054},
	abstract = {Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Augenstein, Isabelle and Søgaard, Anders},
	month = jul,
	year = {2017},
	keywords = {extraction},
	pages = {341--346},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/M7VHEKHL/Augenstein et Søgaard - 2017 - Multi-Task Learning of Keyphrase Boundary Classifi.pdf:application/pdf},
}

@inproceedings{ye_semi-supervised_2018,
	address = {Brussels, Belgium},
	title = {Semi-{Supervised} {Learning} for {Neural} {Keyphrase} {Generation}},
	url = {https://www.aclweb.org/anthology/D18-1447},
	doi = {10.18653/v1/D18-1447},
	abstract = {We study the problem of generating keyphrases that summarize the key points for a given document. While sequence-to-sequence (seq2seq) models have achieved remarkable performance on this task (Meng et al., 2017), model training often relies on large amounts of labeled data, which is only applicable to resource-rich domains. In this paper, we propose semi-supervised keyphrase generation methods by leveraging both labeled data and large-scale unlabeled samples for learning. Two strategies are proposed. First, unlabeled documents are first tagged with synthetic keyphrases obtained from unsupervised keyphrase extraction methods or a self-learning algorithm, and then combined with labeled samples for training. Furthermore, we investigate a multi-task learning framework to jointly learn to generate keyphrases as well as the titles of the articles. Experimental results show that our semi-supervised learning-based methods outperform a state-of-the-art model trained with labeled data only.},
	urldate = {2021-04-30},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ye, Hai and Wang, Lu},
	month = oct,
	year = {2018},
	keywords = {generation, o2m},
	pages = {4142--4153},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/76692P2P/Ye et Wang - 2018 - Semi-Supervised Learning for Neural Keyphrase Gene.pdf:application/pdf},
}

@inproceedings{wasson_using_1998,
	title = {Using {Leading} {Text} for {News} {Summaries}: {Evaluation} {Results} and {Implications} for {Commercial} {Summarization} {Applications}},
	shorttitle = {Using {Leading} {Text} for {News} {Summaries}},
	url = {https://www.aclweb.org/anthology/C98-2217},
	urldate = {2021-05-21},
	booktitle = {{COLING} 1998 {Volume} 2: {The} 17th {International} {Conference} on {Computational} {Linguistics}},
	author = {Wasson, Mark},
	year = {1998},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/VFFRW6BU/Wasson - 1998 - Using Leading Text for News Summaries Evaluation .pdf:application/pdf},
}

@article{brandow_automatic_1995,
	series = {Summarizing {Text}},
	title = {Automatic condensation of electronic publications by sentence selection},
	volume = {31},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/030645739500052I},
	doi = {10.1016/0306-4573(95)00052-I},
	abstract = {As electronic information access becomes the norm, and the variety of retrievable material increases, automatic methods of summarizing or condensing text will become critical. This paper describes a system that performs domain-independent automatic condensation of news from a large commercial news service encompassing 41 different publications. This system was evaluated against a system that condensed the same articles using only the first portion of the texts (the lead), up to the target length of the summaries. Three lengths of articles were evaluated for 250 documents by both systems, totalling 1500 suitability judgements in all. The outcome of perhaps the largest evaluation of human vs machine summarization performed to date was unexpected. The lead-based summaries outperformed the “intelligent” summaries significantly, achieving acceptability ratings of over 90\%, compared to 74.4\%. This paper briefly reviews the literature, details the implications of these results, and addresses the remaining hopes for content-based summarization. We expect the results presented here to be useful to other researchers currently investigating the viability of summarization through sentence selection heuristics.},
	language = {en},
	number = {5},
	urldate = {2021-05-21},
	journal = {Information Processing \& Management},
	author = {Brandow, Ronald and Mitze, Karl and Rau, Lisa F.},
	month = sep,
	year = {1995},
	pages = {675--685},
	file = {ScienceDirect Full Text PDF:/home/gallina-y/Zotero/storage/7YZATZ7F/Brandow et al. - 1995 - Automatic condensation of electronic publications .pdf:application/pdf;ScienceDirect Snapshot:/home/gallina-y/Zotero/storage/RLIKIKNZ/030645739500052I.html:text/html},
}

@inproceedings{ma_detecting_2016,
	address = {New York, New York, USA},
	series = {{IJCAI}'16},
	title = {Detecting rumors from microblogs with recurrent neural networks},
	isbn = {978-1-57735-770-4},
	abstract = {Microblogging platforms are an ideal place for spreading rumors and automatically debunking rumors is a crucial problem. To detect rumors, existing approaches have relied on hand-crafted features for employing machine learning algorithms that require daunting manual effort. Upon facing a dubious claim, people dispute its truthfulness by posting various cues over time, which generates long-distance dependencies of evidence. This paper presents a novel method that learns continuous representations of microblog events for identifying rumors. The proposed model is based on recurrent neural networks (RNN) for learning the hidden representations that capture the variation of contextual information of relevant posts over time. Experimental results on datasets from two real-world microblog platforms demonstrate that (1) the RNN method outperforms state-of-the-art rumor detection models that use hand-crafted features; (2) performance of the RNN-based algorithm is further improved via sophisticated recurrent units and extra hidden layers; (3) RNN-based method detects rumors more quickly and accurately than existing techniques, including the leading online rumor debunking services.},
	urldate = {2021-05-21},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Ma, Jing and Gao, Wei and Mitra, Prasenjit and Kwon, Sejeong and Jansen, Bernard J. and Wong, Kam-Fai and Cha, Meeyoung},
	month = jul,
	year = {2016},
	pages = {3818--3824},
}

@article{ye_one2set_2021,
	title = {{ONE2SET} : {Generating} {Diverse} {Keyphrases} as a {Set}},
	abstract = {Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predeﬁned order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predeﬁned order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm ONE2SET without predeﬁning an order to concatenate the keyphrases. To ﬁt this paradigm, we propose a novel model that utilizes a ﬁxed set of learned control codes as conditions to generate a set of keyphrases in parallel. To solve the problem that there is no correspondence between each prediction and target during training, we propose a K-step target assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the duplication ratio of generated keyphrases. The experimental results on multiple benchmarks demonstrate that our approach signiﬁcantly outperforms the state-of-the-art methods.},
	language = {en},
	author = {Ye, Jiacheng and Gui, Tao and Luo, Yichao and Xu, Yige and Zhang, Qi},
	year = {2021},
	keywords = {generation, set, code},
	pages = {11},
	file = {Ye et al. - 1School of Computer Science, Fudan University 2Ins.pdf:/home/gallina-y/Zotero/storage/4W7PSMGJ/Ye et al. - 1School of Computer Science, Fudan University 2Ins.pdf:application/pdf},
}

@article{duguid_newspaper_2010,
	title = {Newspaper discourse informalisation: a diachronic comparison from keywords},
	volume = {5},
	issn = {1749-5032, 1755-1676},
	shorttitle = {Newspaper discourse informalisation},
	url = {https://www.euppublishing.com/doi/10.3366/cor.2010.0102},
	doi = {10.3366/cor.2010.0102},
	abstract = {In this paper, I provide an overview of certain types of salient items found in the keyword lists of the SiBol 1993 and SiBol 2005 corpora with the objective of diachronic analysis of a particular text type, namely, that of British broadsheet newspapers. I analysed the keyword lists (see Partington, 2010: Section 2) in search of items that could be assigned to semantic sets, which could be glossed as hyperbole, vagueness and informal evaluation. The appearance of these sets in the keywords for 2005 seems to point to changes over time in newspaper prose style. The newspapers under consideration thus appear to have altered both in their function and in their relationship with their readership; and this is reﬂected in the salient lexis and its contexts of use. An increase in conversational and informal styles emerges, along with a notable increase in a particular kind of evaluative and promotional language as a result of a proportional increase in soft news, supplements and reviews.},
	language = {en},
	number = {2},
	urldate = {2021-06-01},
	journal = {Corpora},
	author = {Duguid, Alison},
	month = nov,
	year = {2010},
	pages = {109--138},
	file = {Duguid - 2010 - Newspaper discourse informalisation a diachronic .pdf:/home/gallina-y/Zotero/storage/VKUX4KMK/Duguid - 2010 - Newspaper discourse informalisation a diachronic .pdf:application/pdf},
}

@book{lancaster_indexing_2003,
	address = {Champaign, Ill.},
	title = {Indexing and abstracting in theory and practice},
	isbn = {978-0-87845-122-7},
	language = {en},
	publisher = {University of Illinois},
	author = {Lancaster, F. Wilfrid},
	year = {2003},
	note = {OCLC: 185285797},
}

@inproceedings{gero_word_2021,
	address = {Online},
	title = {Word centrality constrained representation for keyphrase extraction},
	url = {https://www.aclweb.org/anthology/2021.bionlp-1.17},
	abstract = {To keep pace with the increased generation and digitization of documents, automated methods that can improve search, discovery and mining of the vast body of literature are essential. Keyphrases provide a concise representation by identifying salient concepts in a document. Various supervised approaches model keyphrase extraction using local context to predict the label for each token and perform much better than the unsupervised counterparts. Unfortunately, this method fails for short documents where the context is unclear. Moreover, keyphrases, which are usually the gist of a document, need to be the central theme. We propose a new extraction model that introduces a centrality constraint to enrich the word representation of a Bidirectional long short-term memory. Performance evaluation on 2 publicly available datasets demonstrate our model outperforms existing state-of-the art approaches.},
	urldate = {2021-06-02},
	booktitle = {Proceedings of the 20th {Workshop} on {Biomedical} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Gero, Zelalem and Ho, Joyce},
	month = jun,
	year = {2021},
	keywords = {extraction},
	pages = {155--161},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/Y9PDJEPR/Gero et Ho - 2021 - Word centrality constrained representation for key.pdf:application/pdf},
}

@inproceedings{kontoulis_keyphrase_2021,
	address = {Online},
	title = {Keyphrase {Extraction} from {Scientific} {Articles} via {Extractive} {Summarization}},
	url = {https://www.aclweb.org/anthology/2021.sdp-1.6},
	abstract = {Automatically extracting keyphrases from scholarly documents leads to a valuable concise representation that humans can understand and machines can process for tasks, such as information retrieval, article clustering and article classification. This paper is concerned with the parts of a scientific article that should be given as input to keyphrase extraction methods. Recent deep learning methods take titles and abstracts as input due to the increased computational complexity in processing long sequences, whereas traditional approaches can also work with full-texts. Titles and abstracts are dense in keyphrases, but often miss important aspects of the articles, while full-texts on the other hand are richer in keyphrases but much noisier. To address this trade-off, we propose the use of extractive summarization models on the full-texts of scholarly documents. Our empirical study on 3 article collections using 3 keyphrase extraction methods shows promising results.},
	urldate = {2021-06-02},
	booktitle = {Proceedings of the {Second} {Workshop} on {Scholarly} {Document} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kontoulis, Chrysovalantis Giorgos and Papagiannopoulou, Eirini and Tsoumakas, Grigorios},
	month = jun,
	year = {2021},
	keywords = {meta},
	pages = {49--55},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/ISQTH2DT/Kontoulis et al. - 2021 - Keyphrase Extraction from Scientific Articles via .pdf:application/pdf},
}

@inproceedings{mork_nlm_2013,
	address = {Valencia, Spain},
	title = {The nlm medical text indexer system for indexing biomedical literature},
	abstract = {Abstract. In the face of a growing workload and dwindling resources, the US National Library of Medicine (NLM) created the Indexing Initiative project in the mid-1990s. This cross-library team’s mission is to explore indexing methodologies that can help ensure that MEDLINE and other NLM document collections maintain their quality and currency and thereby contribute to NLM’s mission of maintaining quality access to the biomedical literature. The NLM Medical Text Indexer (MTI) is the main product of this project and has been providing indexing recommendations based on the Medical Subject Headings (MeSH) vocabulary since 2002. In 2011, NLM expanded MTI’s role by designating it as the first-line indexer (MTIFL) for a few journals; today the MTIFL workflow includes about 100 journals and continues to increase. Due to a close collaboration with the Index Section at NLM, MTI continues to grow and expand its ability to provide assistance to the indexers. This paper provides an overview of MTI’s functionality, performance, and its evolution over the years.},
	booktitle = {Proceedings of the first {Workshop} on {Bio}-{Medical} {Semantic} {Indexing} and {Question} {Answering},},
	author = {Mork, James G. and Yepes, Antonio J. Jimeno and Aronson, Alan R.},
	year = {2013},
	file = {Citeseer - Full Text PDF:/home/gallina-y/Zotero/storage/XN2622DM/Mork et al. - 2013 - The nlm medical text indexer system for indexing b.pdf:application/pdf;Citeseer - Snapshot:/home/gallina-y/Zotero/storage/7JERZCDV/summary.html:text/html},
}

@article{hartley_how_2003,
	title = {How useful are 'key words' in scientific journals?},
	volume = {29},
	issn = {0165-5515},
	number = {5},
	journal = {How useful are 'key words' in scientific journals?},
	author = {Hartley, James and Kostoff, Ronald N.},
	year = {2003},
	note = {Num Pages: 6
Place: East Grinstead
Publisher: Bowker-Saur},
	pages = {433--438},
	file = {HARTLEY et KOSTOFF - 2003 - How useful are 'key words' in scientific journals.pdf:/home/gallina-y/Zotero/storage/2N6TIZNB/HARTLEY et KOSTOFF - 2003 - How useful are 'key words' in scientific journals.pdf:application/pdf},
}

@article{gutwin_improving_1999,
	title = {Improving browsing in digital libraries with keyphrase indexes},
	volume = {27},
	issn = {0167-9236},
	url = {https://www.sciencedirect.com/science/article/pii/S016792369900038X},
	doi = {10.1016/S0167-9236(99)00038-X},
	abstract = {Browsing accounts for much of people's interaction with digital libraries, but it is poorly supported by standard search engines. Conventional systems often operate at the wrong level, indexing words when people think in terms of topics, and returning documents when people want a broader view. As a result, users cannot easily determine what is in a collection, how well a particular topic is covered, or what kinds of queries will provide useful results. We have built a new kind of search engine, Keyphind, that is explicitly designed to support browsing. Automatically extracted keyphrases form the basic unit of both indexing and presentation, allowing users to interact with the collection at the level of topics and subjects rather than words and documents. The keyphrase index also provides a simple mechanism for clustering documents, refining queries, and previewing results. We compared Keyphind to a traditional query engine in a small usability study. Users reported that certain kinds of browsing tasks were much easier with the new interface, indicating that a keyphrase index would be a useful supplement to existing search tools.},
	language = {en},
	number = {1},
	urldate = {2021-06-03},
	journal = {Decision Support Systems},
	author = {Gutwin, Carl and Paynter, Gordon and Witten, Ian and Nevill-Manning, Craig and Frank, Eibe},
	month = nov,
	year = {1999},
	keywords = {Browsing interfaces, Digital libraries, Keyphrase extraction, Machine learning, Text mining},
	pages = {81--104},
	file = {ScienceDirect Snapshot:/home/gallina-y/Zotero/storage/C2K4D8QF/S016792369900038X.html:text/html;Version soumise:/home/gallina-y/Zotero/storage/VRL5K9PZ/Gutwin et al. - 1999 - Improving browsing in digital libraries with keyph.pdf:application/pdf},
}

@article{gbur_key_1995,
	title = {Key words and phrases: the key to scholarly visibility and efficiency in an information explosion},
	volume = {49},
	issn = {0003-1305},
	shorttitle = {Key words and phrases},
	number = {1},
	journal = {Key words and phrases: the key to scholarly visibility and efficiency in an information explosion},
	author = {Gbur, E. E and Trumbo, B. E.},
	year = {1995},
	note = {Place: Alexandria, VA
Publisher: American Statistical Association},
	pages = {29--33},
	file = {Gbur et Trumbo - 2021 - Key Words and Phrases-The Key to Scholarly Visibil.pdf:/home/gallina-y/Zotero/storage/HHNFAVY4/Gbur et Trumbo - 2021 - Key Words and Phrases-The Key to Scholarly Visibil.pdf:application/pdf;Key words and phrases\: the key to scholarly visibility and efficiency in an information explosion:/home/gallina-y/Zotero/storage/PSNQDVBR/index.html:text/html},
}

@inproceedings{ritchie_how_2006,
	address = {Sydney, Australia},
	title = {How to {Find} {Better} {Index} {Terms} {Through} {Citations}},
	url = {https://www.aclweb.org/anthology/W06-0804},
	urldate = {2021-06-08},
	booktitle = {Proceedings of the {Workshop} on {How} {Can} {Computational} {Linguistics} {Improve} {Information} {Retrieval}?},
	publisher = {Association for Computational Linguistics},
	author = {Ritchie, Anna and Teufel, Simone and Robertson, Stephen},
	month = jul,
	year = {2006},
	pages = {25--32},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/N9YDRI8Q/Ritchie et al. - 2006 - How to Find Better Index Terms Through Citations.pdf:application/pdf},
}

@inproceedings{zhao_sgg_2021,
	address = {Online},
	title = {{SGG}: {Learning} to {Select}, {Guide}, and {Generate} for {Keyphrase} {Generation}},
	shorttitle = {{SGG}},
	url = {https://www.aclweb.org/anthology/2021.naacl-main.455},
	abstract = {Keyphrases, that concisely summarize the high-level topics discussed in a document, can be categorized into present keyphrase which explicitly appears in the source text and absent keyphrase which does not match any contiguous subsequence but is highly semantically related to the source. Most existing keyphrase generation approaches synchronously generate present and absent keyphrases without explicitly distinguishing these two categories. In this paper, a Select-Guide-Generate (SGG) approach is proposed to deal with present and absent keyphrases generation separately with different mechanisms. Specifically, SGG is a hierarchical neural network which consists of a pointing-based selector at low layer concentrated on present keyphrase generation, a selection-guided generator at high layer dedicated to absent keyphrase generation, and a guider in the middle to transfer information from selector to generator. Experimental results on four keyphrase generation benchmarks demonstrate the effectiveness of our model, which significantly outperforms the strong baselines for both present and absent keyphrases generation. Furthermore, we extend SGG to a title generation task which indicates its extensibility in natural language generation tasks.},
	urldate = {2021-06-10},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jing and Bao, Junwei and Wang, Yifan and Wu, Youzheng and He, Xiaodong and Zhou, Bowen},
	month = jun,
	year = {2021},
	keywords = {generation},
	pages = {5717--5726},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/448NN9SL/Zhao et al. - 2021 - SGG Learning to Select, Guide, and Generate for K.pdf:application/pdf},
}

@inproceedings{ritchie_comparing_2008,
	address = {New York, NY, USA},
	series = {{CIKM} '08},
	title = {Comparing citation contexts for information retrieval},
	isbn = {978-1-59593-991-3},
	url = {https://doi.org/10.1145/1458082.1458113},
	doi = {10.1145/1458082.1458113},
	abstract = {In previous work, we have shown that using terms from around citations in citing papers to index the cited paper, in addition to the cited paper's own terms, can improve retrieval effectiveness. Now, we investigate how to select text from around the citations in order to extract good index terms. We compare the retrieval effectiveness that results from a range of contexts around the citations, including no context, the entire citing paper, some fixed windows and several variations with linguistic motivations. We conclude with an analysis of the benefits of more complex, linguistically motivated methods for extracting citation index terms, over using a fixed window of terms. We speculate that there might be some advantage to using computational linguistic techniques for this task.},
	urldate = {2021-06-11},
	booktitle = {Proceedings of the 17th {ACM} conference on {Information} and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Ritchie, Anna and Robertson, Stephen and Teufel, Simone},
	month = oct,
	year = {2008},
	keywords = {citation context analysis, ir evaluation},
	pages = {213--222},
	file = {Ritchie et al. - 2008 - Comparing citation contexts for information retrie.pdf:/home/gallina-y/Zotero/storage/VUBZVNJ2/Ritchie et al. - 2008 - Comparing citation contexts for information retrie.pdf:application/pdf},
}

@inproceedings{xiong_open_2019,
	address = {Hong Kong, China},
	title = {Open {Domain} {Web} {Keyphrase} {Extraction} {Beyond} {Language} {Modeling}},
	url = {https://www.aclweb.org/anthology/D19-1521},
	doi = {10.18653/v1/D19-1521},
	abstract = {This paper studies keyphrase extraction in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond language understanding using visual presentations of documents and weak supervision from search queries. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of learning from the open domain data compared to a specific domain.},
	urldate = {2021-06-11},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Xiong, Lee and Hu, Chuan and Xiong, Chenyan and Campos, Daniel and Overwijk, Arnold},
	month = nov,
	year = {2019},
	keywords = {dataset},
	pages = {5175--5184},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/MBEA6G74/Xiong et al. - 2019 - Open Domain Web Keyphrase Extraction Beyond Langua.pdf:application/pdf},
}

@inproceedings{doostmohammadi_perkey_2018,
	title = {{PerKey}: {A} {Persian} {News} {Corpus} for {Keyphrase} {Extraction} and {Generation}},
	shorttitle = {{PerKey}},
	doi = {10.1109/ISTEL.2018.8661095},
	abstract = {Keyphrases provide an extremely dense summary of a text. Such information can be used in many Natural Language Processing tasks, such as information retrieval and text summarization. Since previous studies on Persian keyword or keyphrase extraction have not published their data, the field suffers from the lack of a human extracted keyphrase dataset. In this paper, we introduce PerKey1, a corpus of 553k news articles from six Persian news websites and agencies with relatively high quality author extracted keyphrases, which is then filtered and cleaned to achieve higher quality keyphrases. The resulted data was put into human assessment to ensure the quality of the keyphrases. We also measured the performance of different supervised and unsupervised techniques, e.g. TFIDF, MultipartiteRank, KEA, etc. on the dataset using precision, recall, and F1-score.},
	booktitle = {2018 9th {International} {Symposium} on {Telecommunications} ({IST})},
	author = {Doostmohammadi, Ehsan and Bokaei, Mohammad Hadi and Sameti, Hossein},
	month = dec,
	year = {2018},
	keywords = {Cleaning, Data mining, Feature extraction, Information retrieval, Keyphrase Extraction, Keyword Extraction, Natural language processing, Persian news corpus, supervised Keyphrase Extraction, Task analysis, Training, unsupervised Keyphrase Extraction},
	pages = {460--465},
	file = {IEEE Xplore Abstract Record:/home/gallina-y/Zotero/storage/MI7Q353T/8661095.html:text/html;Version soumise:/home/gallina-y/Zotero/storage/9QRYCA3N/Doostmohammadi et al. - 2018 - PerKey A Persian News Corpus for Keyphrase Extrac.pdf:application/pdf},
}

@inproceedings{boudin_taln_2013,
	address = {Les Sables d'Olonne, France},
	title = {{TALN} {Archives} : a digital archive of {French} research articles in {Natural} {Language} {Processing} ({TALN} {Archives} : une archive numérique francophone des articles de recherche en {Traitement} {Automatique} de la {Langue}) [in {French}]},
	shorttitle = {{TALN} {Archives}},
	url = {https://www.aclweb.org/anthology/F13-2001},
	urldate = {2021-06-18},
	booktitle = {Proceedings of {TALN} 2013 ({Volume} 2: {Short} {Papers})},
	publisher = {ATALA},
	author = {Boudin, Florian},
	month = jun,
	year = {2013},
	pages = {507--514},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/CVRUHI9C/Boudin - 2013 - TALN Archives  a digital archive of French resear.pdf:application/pdf},
}

@article{medelyan_domain-independent_2008,
	title = {Domain-independent automatic keyphrase indexing with small training sets},
	volume = {59},
	issn = {15322882, 15322890},
	url = {http://doi.wiley.com/10.1002/asi.20790},
	doi = {10.1002/asi.20790},
	abstract = {Keyphrases are widely used in both physical and digital libraries as a brief but precise summary of documents. They help organize material based on content, provide thematic access, represent search results, and assist with navigation. Manual assignment is expensive, because trained human indexers must reach an understanding of the document and select appropriate descriptors according to defined cataloguing rules. We propose a new method that enhances automatic keyphrase extraction by using semantic information about terms and phrases gleaned from a domain-specific thesaurus. The key advantage of the new approach is that it performs well with very little training data. We evaluate it on a large set of manually indexed documents in the domain of agriculture, compare its consistency with a group of six professional indexers, and explore its performance on smaller collections of documents in other domains and of French and Spanish documents.},
	language = {en},
	number = {7},
	urldate = {2021-06-18},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Medelyan, Olena and Witten, Ian H.},
	month = may,
	year = {2008},
	pages = {1026--1040},
	file = {Medelyan et Witten - 2008 - Domain-independent automatic keyphrase indexing wi.pdf:/home/gallina-y/Zotero/storage/HNWFQTAK/Medelyan et Witten - 2008 - Domain-independent automatic keyphrase indexing wi.pdf:application/pdf},
}

@book{guinchat_introduction_1990,
	address = {Paris},
	title = {Introduction générale aux sciences et techniques de l'information et de la documentation},
	isbn = {978-92-3-202540-1},
	language = {français},
	publisher = {Unesco},
	author = {Guinchat, Claire},
	year = {1990},
	file = {Exemplaires\: Introduction générale aux sciences et techniques de l'information et de la documentation:/home/gallina-y/Zotero/storage/JVF3DSHB/PPN002388936.html:text/html},
}

@incollection{guinchat_indexation_1990,
	address = {Paris},
	title = {L'indexation},
	isbn = {978-92-3-202540-1},
	language = {français},
	booktitle = {Introduction générale aux sciences et techniques de l'information et de la documentation},
	publisher = {Unesco},
	author = {Guinchat, Claire},
	year = {1990},
	pages = {171--181},
}

@article{turney_learning_2000,
	title = {Learning {Algorithms} for {Keyphrase} {Extraction}},
	volume = {2},
	issn = {1573-7659},
	url = {https://doi.org/10.1023/A:1009976227802},
	doi = {10.1023/A:1009976227802},
	abstract = {Many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. We approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. We evaluate the performance of nine different configurations of C4.5. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for automatically extracting keyphrases from text. The experimental results support the claim that a custom-designed algorithm (GenEx), incorporating specialized procedural domain knowledge, can generate better keyphrases than a general-purpose algorithm (C4.5). Subjective human evaluation of the keyphrases generated by GenEx suggests that about 80\% of the keyphrases are acceptable to human readers. This level of performance should be satisfactory for a wide variety of applications.},
	language = {en},
	number = {4},
	urldate = {2021-06-23},
	journal = {Information Retrieval},
	author = {Turney, Peter D.},
	month = may,
	year = {2000},
	keywords = {kpe},
	pages = {303--336},
	file = {Springer Full Text PDF:/home/gallina-y/Zotero/storage/M3BBTBR4/Turney - 2000 - Learning Algorithms for Keyphrase Extraction.pdf:application/pdf},
}

@article{porter_algorithm_1980,
	title = {An algorithm for suffix stripping},
	volume = {14},
	issn = {0033-0337},
	url = {https://doi.org/10.1108/eb046814},
	doi = {10.1108/eb046814},
	abstract = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length.},
	number = {3},
	urldate = {2021-06-25},
	journal = {Program},
	author = {Porter, M.F.},
	month = jan,
	year = {1980},
	note = {Publisher: MCB UP Ltd},
	pages = {130--137},
	file = {Snapshot:/home/gallina-y/Zotero/storage/66RJK6R6/html.html:text/html},
}

@inproceedings{zhai_fast_1997,
	address = {Washington, DC, USA},
	title = {Fast {Statistical} {Parsing} of {Noun} {Phrases} for {Document} {Indexing}},
	url = {https://aclanthology.org/A97-1046},
	doi = {10.3115/974557.974603},
	urldate = {2021-07-16},
	booktitle = {Fifth {Conference} on {Applied} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhai, Chengxiang},
	month = mar,
	year = {1997},
	pages = {312--319},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/YM9FS86C/Zhai - 1997 - Fast Statistical Parsing of Noun Phrases for Docum.pdf:application/pdf},
}

@article{barker_comparative_1972,
	title = {Comparative {Efficiency} of {Searching} {Titles}, {Abstracts}, and {Index} {Terms} in a {Free}‐{Text} {Data} {Base}},
	volume = {28},
	issn = {0022-0418},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb026527/full/html},
	doi = {10.1108/eb026527},
	language = {en},
	number = {1},
	urldate = {2021-07-16},
	journal = {Journal of Documentation},
	author = {Barker, F.H. and Veal, D.C. and Wyatt, B.K.},
	month = jan,
	year = {1972},
	pages = {22--36},
	file = {Barker et al. - 1972 - COMPARATIVE EFFICIENCY OF SEARCHING TITLES, ABSTRA.pdf:/home/gallina-y/Zotero/storage/4RK752A3/Barker et al. - 1972 - COMPARATIVE EFFICIENCY OF SEARCHING TITLES, ABSTRA.pdf:application/pdf},
}

@inproceedings{berend_opinion_2011,
	address = {Chiang Mai, Thailand},
	title = {Opinion {Expression} {Mining} by {Exploiting} {Keyphrase} {Extraction}},
	url = {https://aclanthology.org/I11-1130},
	urldate = {2021-07-16},
	booktitle = {Proceedings of 5th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Asian Federation of Natural Language Processing},
	author = {Berend, Gábor},
	month = nov,
	year = {2011},
	pages = {1162--1170},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/YCGZNCPK/Berend - 2011 - Opinion Expression Mining by Exploiting Keyphrase .pdf:application/pdf},
}

@article{hussey_comparison_2012,
	title = {A comparison of automated keyphrase extraction techniques and of automatic evaluation vs. human evaluation},
	volume = {4},
	issn = {1942-2660},
	url = {http://www.iariajournals.org/life_sciences/},
	abstract = {Keyphrases are added to documents to help identify the areas of interest they contain. However, in a significant proportion of papers author selected keyphrases are not appropriate for the document they accompany: for instance, they can be classificatory rather than explanatory, or they are not updated when the focus of the paper changes. As such, automated methods for improving the use of keyphrases are
needed, and various methods have been published. However, each method was evaluated using a different corpus, typically one relevant to the field of study of the method’s authors. This not only makes it difficult to incorporate the useful elements of algorithms in future work, but also makes comparing the results of each method inefficient and ineffective. This paper describes the work undertaken to compare five methods across
a common baseline of corpora. The methods chosen were Term Frequency, Inverse Document Frequency, the C-Value, the NC-Value, and a Synonym based approach. These methods were analysed to evaluate performance and quality of results, and to provide a future benchmark. It is shown that Term Frequency and Inverse Document Frequency were the best algorithms, with the Synonym approach following them.
Following these findings, a study was undertaken into the value of using human evaluators to judge the outputs. The Synonym method was compared to the original author keyphrases of the Reuters’ News Corpus. The findings show that authors of Reuters’ news articles provide good keyphrases  but that more often than not they do not provide any keyphrases.},
	number = {3 and 4},
	urldate = {2021-07-16},
	journal = {International Journal on Advances in Life Sciences},
	author = {Hussey, Richard and Williams, Shirley and Mitchell, Richard and Field, Ian},
	year = {2012},
	note = {Number: 3 and 4
Publisher: IARIA},
	pages = {136--153},
	file = {Hussey et al. - 2012 - A comparison of automated keyphrase extraction tec.pdf:/home/gallina-y/Zotero/storage/AWUP95J7/Hussey et al. - 2012 - A comparison of automated keyphrase extraction tec.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/MXSS2IU2/32266.html:text/html},
}

@inproceedings{collins_document_2019,
	address = {Champaign, Illinois},
	series = {{JCDL} '19},
	title = {Document embeddings vs. keyphrases vs. terms for recommender systems: a large-scale online evaluation},
	isbn = {978-1-72811-547-4},
	shorttitle = {Document embeddings vs. keyphrases vs. terms for recommender systems},
	url = {https://doi.org/10.1109/JCDL.2019.00027},
	doi = {10.1109/JCDL.2019.00027},
	abstract = {Many recommendation algorithms are available to digital library recommender system operators. The effectiveness of algorithms is largely unreported by way of online evaluation. We compare a standard term-based recommendation approach to two promising approaches for related-article recommendation in digital libraries: document embeddings, and keyphrases. We evaluate the consistency of their performance across multiple scenarios. Through our recommender-system as-a-service Mr. DLib, we delivered 33.5M recommendations to users of Sowiport and Jabref over the course of 19 months, from March 2017 to October 2018. The effectiveness of the algorithms differs significantly between Sowiport and Jabref (Wilcoxon rank-sum test; p {\textless} 0.05). There is a {\textasciitilde}400\% difference in effectiveness between the best and worst algorithm in both scenarios separately. The best performing algorithm in Sowiport (terms) is the worst performing in Jabref. The best performing algorithm in Jabref (keyphrases) is 70\% worse in Sowiport, than Sowiport's best algorithm (click-through rate; 0.1\% terms, 0.03\% keyphrases).},
	urldate = {2021-07-16},
	booktitle = {Proceedings of the 18th {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {IEEE Press},
	author = {Collins, Andrew and Beel, Joeran},
	month = jun,
	year = {2019},
	pages = {130--133},
}

@inproceedings{boudin_pke_2016,
	address = {Osaka, Japan},
	title = {pke: an open source python-based keyphrase extraction toolkit},
	shorttitle = {pke},
	url = {https://aclanthology.org/C16-2015},
	abstract = {We describe pke, an open source python-based keyphrase extraction toolkit. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extented to develop new approaches. pke also allows for easy benchmarking of state-of-the-art keyphrase extraction approaches, and ships with supervised models trained on the SemEval-2010 dataset.},
	urldate = {2021-07-16},
	booktitle = {Proceedings of {COLING} 2016, the 26th {International} {Conference} on {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {The COLING 2016 Organizing Committee},
	author = {Boudin, Florian},
	month = dec,
	year = {2016},
	pages = {69--73},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/CXQCA68M/Boudin - 2016 - pke an open source python-based keyphrase extract.pdf:application/pdf},
}

@inproceedings{wang_how_2014,
	address = {Berlin, Heidelberg},
	series = {{CICLing} 2014},
	title = {How {Preprocessing} {Affects} {Unsupervised} {Keyphrase} {Extraction}},
	isbn = {978-3-642-54905-2},
	url = {https://doi.org/10.1007/978-3-642-54906-9_14},
	doi = {10.1007/978-3-642-54906-9_14},
	abstract = {Unsupervised keyphrase extraction techniques generally consist of candidate phrase selection and ranking techniques. Previous studies treat the candidate phrase selection and ranking as a whole, while the effectiveness of identifying candidate phrases and the impact on ranking algorithms have remained undiscovered. This paper surveys common candidate selection techniques and analyses the effect on the performance of ranking algorithms from different candidate selection approaches. Our evaluation shows that candidate selection approaches with better coverage and accuracy can boost the performance of the ranking algorithms.},
	urldate = {2021-07-16},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Computational} {Linguistics} and {Intelligent} {Text} {Processing} - {Volume} 8403},
	publisher = {Springer-Verlag},
	author = {Wang, Rui and Liu, Wei and Mcdonald, Chris},
	month = apr,
	year = {2014},
	pages = {163--176},
}

@inproceedings{manning_stanford_2014,
	address = {Baltimore, Maryland},
	title = {The {Stanford} {CoreNLP} {Natural} {Language} {Processing} {Toolkit}},
	url = {https://aclanthology.org/P14-5010},
	doi = {10.3115/v1/P14-5010},
	urldate = {2021-07-16},
	booktitle = {Proceedings of 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
	month = jun,
	year = {2014},
	pages = {55--60},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/8A2FVHIE/Manning et al. - 2014 - The Stanford CoreNLP Natural Language Processing T.pdf:application/pdf},
}

@inproceedings{teneva_salience_2017,
	address = {Vancouver, Canada},
	title = {Salience {Rank}: {Efficient} {Keyphrase} {Extraction} with {Topic} {Modeling}},
	shorttitle = {Salience {Rank}},
	url = {https://aclanthology.org/P17-2084},
	doi = {10.18653/v1/P17-2084},
	abstract = {Topical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents. The ranking procedure consists of running PageRank K times, where K is the number of topics used in the LDA model. In this paper, we propose a modification of TPR, called Salience Rank. Salience Rank only needs to run PageRank once and extracts comparable or better keyphrases on benchmark datasets. In addition to quality and efficiency benefit, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity.},
	urldate = {2021-07-16},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Teneva, Nedelina and Cheng, Weiwei},
	month = jul,
	year = {2017},
	pages = {530--535},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/RP6CVNZC/Teneva et Cheng - 2017 - Salience Rank Efficient Keyphrase Extraction with.pdf:application/pdf},
}

@inproceedings{sood_tagassist_2007,
	title = {{TagAssist}: {Automatic} tag suggestion for blog posts},
	shorttitle = {{TagAssist}},
	url = {https://www.scholars.northwestern.edu/en/publications/tagassist-automatic-tag-suggestion-for-blog-posts},
	language = {English (US)},
	urldate = {2021-07-16},
	booktitle = {Proceedings of the 2007 {International} {Conference} on {Weblogs} and {Social} {Media}},
	author = {Sood, Sanjay C. and Hammond, Kristian J. and Sood, Sara Owsley and Birnbaum, Lawrence A.},
	month = dec,
	year = {2007},
	file = {Snapshot:/home/gallina-y/Zotero/storage/R8T4SFET/tagassist-automatic-tag-suggestion-for-blog-posts.html:text/html},
}

@inproceedings{kedzie_content_2018,
	address = {Brussels, Belgium},
	title = {Content {Selection} in {Deep} {Learning} {Models} of {Summarization}},
	url = {https://aclanthology.org/D18-1208},
	doi = {10.18653/v1/D18-1208},
	abstract = {We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the sumarization task.},
	urldate = {2021-07-16},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kedzie, Chris and McKeown, Kathleen and Daumé III, Hal},
	month = oct,
	year = {2018},
	pages = {1818--1828},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/FQ96K5XP/Kedzie et al. - 2018 - Content Selection in Deep Learning Models of Summa.pdf:application/pdf},
}

@inproceedings{goldstein_summarization_1998,
	address = {Baltimore, Maryland, USA},
	title = {Summarization: (1) {Using} {MMR} for {Diversity}- {Based} {Reranking} and (2) {Evaluating} {Summaries}},
	shorttitle = {Summarization},
	url = {https://aclanthology.org/X98-1025},
	doi = {10.3115/1119089.1119120},
	urldate = {2021-07-16},
	booktitle = {{TIPSTER} {TEXT} {PROGRAM} {PHASE} {III}: {Proceedings} of a {Workshop} held at {Baltimore}, {Maryland}, {October} 13-15, 1998},
	publisher = {Association for Computational Linguistics},
	author = {Goldstein, Jade and Carbonell, Jaime},
	month = oct,
	year = {1998},
	pages = {181--195},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/2XQ7LK7V/Goldstein et Carbonell - 1998 - Summarization (1) Using MMR for Diversity- Based .pdf:application/pdf},
}

@article{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://papers.nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	language = {en},
	urldate = {2021-07-16},
	journal = {Advances in Neural Information Processing Systems},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/IHYQI9DL/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/KQFTQZFY/bdbca288fee7f92f2bfa9f7012727740-Abstract.html:text/html},
}

@inproceedings{gardner_allennlp_2018,
	address = {Melbourne, Australia},
	title = {{AllenNLP}: {A} {Deep} {Semantic} {Natural} {Language} {Processing} {Platform}},
	shorttitle = {{AllenNLP}},
	url = {https://aclanthology.org/W18-2501},
	doi = {10.18653/v1/W18-2501},
	abstract = {Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.},
	urldate = {2021-07-16},
	booktitle = {Proceedings of {Workshop} for {NLP} {Open} {Source} {Software} ({NLP}-{OSS})},
	publisher = {Association for Computational Linguistics},
	author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke},
	month = jul,
	year = {2018},
	pages = {1--6},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/VGBMHXRZ/Gardner et al. - 2018 - AllenNLP A Deep Semantic Natural Language Process.pdf:application/pdf},
}

@inproceedings{zesch_approximate_2009,
	address = {Borovets, Bulgaria},
	title = {Approximate {Matching} for {Evaluating} {Keyphrase} {Extraction}},
	url = {https://aclanthology.org/R09-1086},
	urldate = {2021-07-16},
	booktitle = {Proceedings of the {International} {Conference} {RANLP}-2009},
	publisher = {Association for Computational Linguistics},
	author = {Zesch, Torsten and Gurevych, Iryna},
	month = sep,
	year = {2009},
	pages = {484--489},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/BTYDR7PM/Zesch et Gurevych - 2009 - Approximate Matching for Evaluating Keyphrase Extr.pdf:application/pdf},
}

@inproceedings{yang_anserini_2017,
	address = {New York, NY, USA},
	series = {{SIGIR} '17},
	title = {Anserini: {Enabling} the {Use} of {Lucene} for {Information} {Retrieval} {Research}},
	isbn = {978-1-4503-5022-8},
	shorttitle = {Anserini},
	url = {https://doi.org/10.1145/3077136.3080721},
	doi = {10.1145/3077136.3080721},
	abstract = {Software toolkits play an essential role in information retrieval research. Most open-source toolkits developed by academics are designed to facilitate the evaluation of retrieval models over standard test collections. Efforts are generally directed toward better ranking and less attention is usually given to scalability and other operational considerations. On the other hand, Lucene has become the de facto platform in industry for building search applications (outside a small number of companies that deploy custom infrastructure). Compared to academic IR toolkits, Lucene can handle heterogeneous web collections at scale, but lacks systematic support for evaluation over standard test collections. This paper introduces Anserini, a new information retrieval toolkit that aims to provide the best of both worlds, to better align information retrieval practice and research. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial efforts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web-scale collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for many TREC test collections, providing a convenient way to replicate competitive baselines right out of the box. Experiments verify that our system is both efficient and effective, providing a solid foundation to support future research.},
	urldate = {2021-08-23},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Peilin and Fang, Hui and Lin, Jimmy},
	month = aug,
	year = {2017},
	pages = {1253--1256},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/P6JPPA77/Yang et al. - 2017 - Anserini Enabling the Use of Lucene for Informati.pdf:application/pdf},
}

@techreport{abdul-jaleel_umass_2004,
	address = {Fort Belvoir, VA},
	title = {{UMass} at {TREC} 2004: {Novelty} and {HARD}:},
	shorttitle = {{UMass} at {TREC} 2004},
	url = {http://www.dtic.mil/docs/citations/ADA460118},
	language = {en},
	urldate = {2021-08-23},
	institution = {Defense Technical Information Center},
	author = {Abdul-Jaleel, Nasreen and Allan, James and Croft, W. B. and Diaz, Fernando and Larkey, Leah and Li, Xiaoyan and Smucker, Mark D. and Wade, Courtney},
	month = jan,
	year = {2004},
	doi = {10.21236/ADA460118},
	file = {Abdul-Jaleel et al. - 2004 - UMass at TREC 2004 Novelty and HARD.pdf:/home/gallina-y/Zotero/storage/TDZYV3GD/Abdul-Jaleel et al. - 2004 - UMass at TREC 2004 Novelty and HARD.pdf:application/pdf},
}

@article{lin_neural_2019,
	title = {The {Neural} {Hype} and {Comparisons} {Against} {Weak} {Baselines}},
	volume = {52},
	issn = {0163-5840},
	url = {https://doi.org/10.1145/3308774.3308781},
	doi = {10.1145/3308774.3308781},
	abstract = {Recently, the machine learning community paused in a moment of self-reflection. In a widelydiscussed paper at ICLR 2018, Sculley et al. [13] wrote: "We observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole." Their primary complaint is the development of a "research and publication culture that emphasizes wins" (emphasis in original), which typically means "demonstrating that a new method beats previous methods on a given task or benchmark". An apt description might be "leaderboard chasing"-and for many vision and NLP tasks, this isn't a metaphor. There are literally centralized leaderboards1 that track incremental progress, down to the fifth decimal point, some persisting over years, accumulating dozens of entries. Sculley et al. remind us that "the goal of science is not wins, but knowledge". The structure of the scientific enterprise today (pressure to publish, pace of progress, etc.) means that "winning" and "doing good science" are often not fully aligned. To wit, they cite a number of papers showing that recent advances in neural networks could very well be attributed to mundane issues like better hyperparameter optimization. Many results can't be reproduced, and some observed improvements might just be noise.},
	number = {2},
	urldate = {2021-08-23},
	journal = {ACM SIGIR Forum},
	author = {Lin, Jimmy},
	month = jan,
	year = {2019},
	pages = {40--51},
}

@inproceedings{yang_critically_2019,
	address = {New York, NY, USA},
	series = {{SIGIR}'19},
	title = {Critically {Examining} the "{Neural} {Hype}": {Weak} {Baselines} and the {Additivity} of {Effectiveness} {Gains} from {Neural} {Ranking} {Models}},
	isbn = {978-1-4503-6172-9},
	shorttitle = {Critically {Examining} the "{Neural} {Hype}"},
	url = {https://doi.org/10.1145/3331184.3331340},
	doi = {10.1145/3331184.3331340},
	abstract = {Is neural IR mostly hype? In a recent SIGIR Forum article, Lin expressed skepticism that neural ranking models were actually improving ad hoc retrieval effectiveness in limited data scenarios. He provided anecdotal evidence that authors of neural IR papers demonstrate "wins" by comparing against weak baselines. This paper provides a rigorous evaluation of those claims in two ways: First, we conducted a meta-analysis of papers that have reported experimental results on the TREC Robust04 test collection. We do not find evidence of an upward trend in effectiveness over time. In fact, the best reported results are from a decade ago and no recent neural approach comes close. Second, we applied five recent neural models to rerank the strong baselines that Lin used to make his arguments. A significant improvement was observed for one of the models, demonstrating additivity in gains. While there appears to be merit to neural IR approaches, at least some of the gains reported in the literature appear illusory.},
	urldate = {2021-08-23},
	booktitle = {Proceedings of the 42nd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Wei and Lu, Kuang and Yang, Peilin and Lin, Jimmy},
	month = jul,
	year = {2019},
	pages = {1129--1132},
	file = {Version soumise:/home/gallina-y/Zotero/storage/IF627MJN/Yang et al. - 2019 - Critically Examining the Neural Hype Weak Basel.pdf:application/pdf},
}

@inproceedings{he_context-aware_2010,
	address = {New York, NY, USA},
	series = {{WWW} '10},
	title = {Context-aware citation recommendation},
	isbn = {978-1-60558-799-8},
	url = {https://doi.org/10.1145/1772690.1772734},
	doi = {10.1145/1772690.1772734},
	abstract = {When you write papers, how many times do you want to make some citations at a place but you are not sure which papers to cite? Do you wish to have a recommendation system which can recommend a small number of good candidates for every place that you want to make some citations? In this paper, we present our initiative of building a context-aware citation recommendation system. High quality citation recommendation is challenging: not only should the citations recommended be relevant to the paper under composition, but also should match the local contexts of the places citations are made. Moreover, it is far from trivial to model how the topic of the whole paper and the contexts of the citation places should affect the selection and ranking of citations. To tackle the problem, we develop a context-aware approach. The core idea is to design a novel non-parametric probabilistic model which can measure the context-based relevance between a citation context and a document. Our approach can recommend citations for a context effectively. Moreover, it can recommend a set of citations for a paper with high quality. We implement a prototype system in CiteSeerX. An extensive empirical evaluation in the CiteSeerX digital library against many baselines demonstrates the effectiveness and the scalability of our approach.},
	urldate = {2021-08-23},
	booktitle = {Proceedings of the 19th international conference on {World} wide web},
	publisher = {Association for Computing Machinery},
	author = {He, Qi and Pei, Jian and Kifer, Daniel and Mitra, Prasenjit and Giles, Lee},
	month = apr,
	year = {2010},
	pages = {421--430},
	file = {Version soumise:/home/gallina-y/Zotero/storage/R2AXZNJP/He et al. - 2010 - Context-aware citation recommendation.pdf:application/pdf},
}

@inproceedings{smucker_comparison_2007,
	address = {New York, NY, USA},
	series = {{CIKM} '07},
	title = {A comparison of statistical significance tests for information retrieval evaluation},
	isbn = {978-1-59593-803-9},
	url = {https://doi.org/10.1145/1321440.1321528},
	doi = {10.1145/1321440.1321528},
	abstract = {Information retrieval (IR) researchers commonly use three tests of statistical significance: the Student's paired t-test, the Wilcoxon signed rank test, and the sign test. Other researchers have previously proposed using both the bootstrap and Fisher's randomization (permutation) test as non-parametric significance tests for IR but these tests have seen little use. For each of these five tests, we took the ad-hoc retrieval runs submitted to TRECs 3 and 5-8, and for each pair of runs, we measured the statistical significance of the difference in their mean average precision. We discovered that there is little practical difference between the randomization, bootstrap, and t tests. Both the Wilcoxon and sign test have a poor ability to detect significance and have the potential to lead to false detections of significance. The Wilcoxon and sign tests are simplified variants of the randomization test and their use should be discontinued for measuring the significance of a difference between means.},
	urldate = {2021-08-23},
	booktitle = {Proceedings of the sixteenth {ACM} conference on {Conference} on information and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Smucker, Mark D. and Allan, James and Carterette, Ben},
	month = nov,
	year = {2007},
	pages = {623--632},
}

@article{farber_citation_2020,
	title = {Citation recommendation: approaches and datasets},
	volume = {21},
	issn = {1432-1300},
	shorttitle = {Citation recommendation},
	url = {https://doi.org/10.1007/s00799-020-00288-2},
	doi = {10.1007/s00799-020-00288-2},
	abstract = {Citation recommendation describes the task of recommending citations for a given text. Due to the overload of published scientific works in recent years on the one hand, and the need to cite the most appropriate publications when writing scientific texts on the other hand, citation recommendation has emerged as an important research topic. In recent years, several approaches and evaluation data sets have been presented. However, to the best of our knowledge, no literature survey has been conducted explicitly on citation recommendation. In this article, we give a thorough introduction to automatic citation recommendation research. We then present an overview of the approaches and data sets for citation recommendation and identify differences and commonalities using various dimensions. Last but not least, we shed light on the evaluation methods and outline general challenges in the evaluation and how to meet them. We restrict ourselves to citation recommendation for scientific publications, as this document type has been studied the most in this area. However, many of the observations and discussions included in this survey are also applicable to other types of text, such as news articles and encyclopedic articles.},
	language = {en},
	number = {4},
	urldate = {2021-08-23},
	journal = {International Journal on Digital Libraries},
	author = {Färber, Michael and Jatowt, Adam},
	month = dec,
	year = {2020},
	pages = {375--405},
	file = {Springer Full Text PDF:/home/gallina-y/Zotero/storage/PPHJDKFA/Färber et Jatowt - 2020 - Citation recommendation approaches and datasets.pdf:application/pdf},
}

@article{billerbeck_document_2005,
	title = {Document {Expansion} versus {Query} {Expansion} for {Ad}-hoc {Retrieval}},
	abstract = {In document information retrieval, the terminology given by a user may not match the terminology of a relevant document. Query expansion seeks to address this mismatch; it can signiﬁcantly increase effectiveness, but is slow and resource-intensive. We investigate the use of document expansion as an alternative, in which documents are augmented with related terms extracted from the corpus during indexing, and the overheads at query time are small. We propose and explore a range of corpus-based document expansion techniques and compare them to corpus-based query expansion on TREC data. These experiments show that document expansion delivers at best limited beneﬁts, while query expansion – including standard techniques and efﬁcient approaches described in recent work – delivers consistent gains. We conclude that document expansion is unpromising, but it is likely that the efﬁciency of query expansion can be further improved.},
	language = {en},
	journal = {Proceedings of the 10th Australasian Document Computing Symposium},
	author = {Billerbeck, Bodo and Zobel, Justin},
	year = {2005},
	pages = {34--41},
	file = {Billerbeck et Zobel - Document Expansion versus Query Expansion for Ad-h.pdf:/home/gallina-y/Zotero/storage/HIFNIXDB/Billerbeck et Zobel - Document Expansion versus Query Expansion for Ad-h.pdf:application/pdf},
}

@article{furnas_vocabulary_1987,
	title = {The vocabulary problem in human-system communication},
	volume = {30},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/32206.32212},
	doi = {10.1145/32206.32212},
	abstract = {In almost all computer applications, users must enter correct words for the desired objects or actions. For success without extensive training, or in first-tries for new targets, the system must recognize terms that will be chosen spontaneously. We studied spontaneous word choice for objects in five application-related domains, and found the variability to be surprisingly large. In every case two people favored the same term with probability {\textless}0.20. Simulations show how this fundamental property of language limits the success of various design methodologies for vocabulary-driven interaction. For example, the popular approach in which access is via one designer's favorite single word will result in 80-90 percent failure rates in many common situations. An optimal strategy, unlimited aliasing, is derived and shown to be capable of several-fold improvements.},
	number = {11},
	urldate = {2021-08-23},
	journal = {Communications of the ACM},
	author = {Furnas, G. W. and Landauer, T. K. and Gomez, L. M. and Dumais, S. T.},
	month = nov,
	year = {1987},
	pages = {964--971},
}

@inproceedings{livne_citesight_2014,
	address = {New York, NY, USA},
	series = {{SIGIR} '14},
	title = {{CiteSight}: supporting contextual citation recommendation using differential search},
	isbn = {978-1-4503-2257-7},
	shorttitle = {{CiteSight}},
	url = {https://doi.org/10.1145/2600428.2609585},
	doi = {10.1145/2600428.2609585},
	abstract = {A person often uses a single search engine for very different tasks. For example, an author editing a manuscript may use the same academic search engine to find the latest work on a particular topic or to find the correct citation for a familiar article. The author's tolerance for latency and accuracy may vary according to task. However, search engines typically employ a consistent approach for processing all queries. In this paper we explore how a range of search needs and expectations can be supported within a single search system using differential search. We introduce CiteSight, a system that provides personalized citation recommendations to author groups that vary based on task. CiteSight presents cached recommendations instantaneously for online tasks (e.g., active paper writing), and refines these recommendations in the background for offline tasks (e.g., future literature review). We develop an active cache-warming process to enhance the system as the author works, and context-coupling, a technique for augment sparse citation networks. By evaluating the quality of the recommendations and collecting user feedback, we show that differential search can provide a high level of accuracy for different tasks on different time scales. We believe that differential search can be used in many situations where the user's tolerance for latency and desired response vary dramatically based on use.},
	urldate = {2021-08-23},
	booktitle = {Proceedings of the 37th international {ACM} {SIGIR} conference on {Research} \& development in information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Livne, Avishay and Gokuladas, Vivek and Teevan, Jaime and Dumais, Susan T. and Adar, Eytan},
	month = jul,
	year = {2014},
	pages = {807--816},
}

@incollection{roy_improved_2017,
	address = {New York, NY, USA},
	title = {An {Improved} {Test} {Collection} and {Baselines} for {Bibliographic} {Citation} {Recommendation}},
	isbn = {978-1-4503-4918-5},
	url = {https://doi.org/10.1145/3132847.3133085},
	abstract = {The problem of recommending bibliographic citations to an author who is writing an article has been well-studied. However, different researchers have used different datasets to evaluate proposed techniques, and have sometimes reported contradictory findings regarding the relative effectiveness of various approaches. In addition, these datasets are problematic in one way or another (e.g., in terms of size or availability), precluding the possibility of adopting one (or some) of them as standard benchmarks. A recently created test collection that makes use of data from CiteSeerx is large, heterogenous, and publicly available, but has certain other limitations. In this paper, we propose a way to modify this test collection to address these limitations. We also use the improved test collection to establish a set of baseline results using elementary content-based techniques, as well as reference directed indexing.},
	urldate = {2021-08-23},
	booktitle = {Proceedings of the 2017 {ACM} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Roy, Dwaipayan},
	month = nov,
	year = {2017},
	pages = {2271--2274},
}

@inproceedings{kando_overview_2001,
	title = {Overview of the {Second} {NTCIR} {Workshop}},
	abstract = {Semantic Scholar extracted view of "Overview of the Second NTCIR Workshop" by N. Kando},
	booktitle = {Proceedings of the {Second} {NTCIR} {Workshop} on {Research} in {Chinese} \& {Japanese} {Text} {Retrieval} and {Text} {Summarization}},
	author = {Kando, N.},
	year = {2001},
	file = {ovview-kando.pdf:/home/gallina-y/Zotero/storage/CP4W5RWB/ovview-kando.pdf:application/pdf},
}

@inproceedings{fujita_notes_2001,
	title = {Notes on the {Limits} of {CLIR} {Effectiveness} {NTCIR}-2 {Evaluation} {Experiments} at {Justsystem}},
	abstract = {NTCIR-2 evaluation experiments at the Justsystem site are described with a focus on comparative study of CLIR effectiveness with monolingual retrieval effectiveness of the same retrieval engine. Experiments on the effects of phrasal translation, indexing of translated phrasal terms, pre-translation feedback and parallel documents feedback in diverse retrieval settings, are reported. The results show that pre-translation pseudorelevance feedback is always effective. We confirmed that phrasal translation is effective and crucial even the retrieval system does not use phrasal indexing units.},
	language = {en},
	booktitle = {Proceedings of the {Second} {NTCIR} {Workshop} on {Research} in {Chinese} \& {Japanese} {Text} {Retrieval} and {Text} {Summarization}},
	author = {Fujita, Sumio and Corporation, JUSTSYSTEM},
	year = {2001},
	pages = {8},
	file = {Fujita et Corporation - Notes on the Limits of CLIR Effectiveness NTCIR-2 .pdf:/home/gallina-y/Zotero/storage/HLKMSYFE/Fujita et Corporation - Notes on the Limits of CLIR Effectiveness NTCIR-2 .pdf:application/pdf},
}

@inproceedings{murata_crl_2001,
	title = {{CRL} at {NTCIR2}},
	abstract = {We have developed systems of two types for NTCIR2. One is an enhenced version of the system we developed for NTCIR1 and IREX. It submitted retrieval results for JJ and CC tasks. A variety of parameters were tried with the system. It used such characteristics of newspapers as locational information in the CC tasks. The system got good results for both of the tasks. The other system is a portable system which avoids free parameters as much as possible. The system submitted retrieval results for JJ, JE, EE, EJ, and CC tasks. The system automatically determined the number of top documents and the weight of the original query used in automatic-feedback retrieval. It also determined relevant terms quite robustly. For EJ and JE tasks, it used document expansion to augment the initial queries. It achieved good results, except on the CC tasks.},
	language = {en},
	booktitle = {Proceedings of the {Second} {NTCIR} {Workshop} on {Research} in {Chinese} \& {Japanese} {Text} {Retrieval} and {Text} {Summarization}},
	author = {Murata, Masaki and Utiyama, Masao and Ma, Qing and Ozaku, Hiromi and Isahara, Hitoshi},
	year = {2001},
	pages = {11},
	file = {Murata et al. - murata,mutiyama,qma,romi,isahara¡ @crl.go.jp.pdf:/home/gallina-y/Zotero/storage/L9PCSII7/Murata et al. - murata,mutiyama,qma,romi,isahara¡ @crl.go.jp.pdf:application/pdf},
}

@inproceedings{chen_berkeley_2001,
	title = {Berkeley at {NTCIR}-2: {Chinese}, {Japanese}, and {English} {IR} {Experiments}},
	abstract = {This paper reports on the work of Berkeley group at the second NTCIR workshop on Japanese \& English IR and Chinese IR. A number of runs were submitted on all subtasks in the two main tasks. Our main focus on the Japanese monolingual subtask was on comparing the retrieval effectiveness of different segmentation methods. The experimental results show the bigram indexing outperformed the word-based indexing in Japanese monolingual retrieval. The bigram indexing was also highly effective in Chinese monolingual retrieval. This paper presents an alternative segmentation method that breaks text into one-character terms and two-character terms that do not overlap with each other, which overcomes the disadvantage of producing large index ﬁles by overlapping bigram indexing. This paper describes a technique for building bilingual word lexicons from parallel text by sentence alignment and word association. A purely rank-based document pooling strategy is presented for combining monolingual retrieval results in multilingual retrieval.},
	language = {en},
	booktitle = {Proceedings of the {Second} {NTCIR} {Workshop} on {Research} in {Chinese} \& {Japanese} {Text} {Retrieval} and {Text} {Summarization}},
	author = {Chen, Aitao and Gey, Fredric C and Jiang, Hailing},
	year = {2001},
	pages = {9},
	file = {Chen et al. - Berkeley at NTCIR-2 Chinese, Japanese, and Englis.pdf:/home/gallina-y/Zotero/storage/T27W4WE3/Chen et al. - Berkeley at NTCIR-2 Chinese, Japanese, and Englis.pdf:application/pdf},
}

@inproceedings{ponte_language_1998,
	address = {New York, NY, USA},
	series = {{SIGIR} '98},
	title = {A language modeling approach to information retrieval},
	isbn = {978-1-58113-015-7},
	url = {https://doi.org/10.1145/290941.291008},
	doi = {10.1145/290941.291008},
	urldate = {2021-08-27},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Ponte, Jay M. and Croft, W. Bruce},
	month = aug,
	year = {1998},
	pages = {275--281},
}

@book{manning_introduction_2009,
	address = {Cambridge, England},
	title = {Introduction to {Information} {Retrieval}},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher and Raghavan, Prabhakar and Schuetze, Hinrich},
	year = {2009},
	file = {Manning et al. - 2009 - Introduction to Information Retrieval.pdf:/home/gallina-y/Zotero/storage/M88XYDN8/Manning et al. - 2009 - Introduction to Information Retrieval.pdf:application/pdf},
}

@article{zhai_study_2017,
	title = {A {Study} of {Smoothing} {Methods} for {Language} {Models} {Applied} to {Ad} {Hoc} {Information} {Retrieval}},
	volume = {51},
	issn = {0163-5840},
	url = {https://doi.org/10.1145/3130348.3130377},
	doi = {10.1145/3130348.3130377},
	abstract = {Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collection.},
	number = {2},
	urldate = {2021-09-15},
	journal = {ACM SIGIR Forum},
	author = {Zhai, Chengxiang and Lafferty, John},
	month = aug,
	year = {2017},
	pages = {268--276},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/7N848PGB/Zhai et Lafferty - 2017 - A Study of Smoothing Methods for Language Models A.pdf:application/pdf},
}

@article{el-beltagy_kp-miner_2009,
	title = {{KP}-{Miner}: {A} keyphrase extraction system for {English} and {Arabic} documents},
	volume = {34},
	issn = {0306-4379},
	shorttitle = {{KP}-{Miner}},
	url = {https://doi.org/10.1016/j.is.2008.05.002},
	doi = {10.1016/j.is.2008.05.002},
	abstract = {Automatic keyphrase extraction has many important applications including but not limited to summarization, cataloging/indexing, feature extraction for clustering and classification, and data mining. This paper presents the KP-Miner system, and demonstrates through experimentation and comparison with widely used systems that it is effective and efficient in extracting keyphrases from both English and Arabic documents of varied length. Unlike other existing keyphrase extraction systems, the KP-Miner system does not need to be trained on a particular document set in order to achieve its task. It also has the advantage of being configurable as the rules and heuristics adopted by the system are related to the general nature of documents and keyphrases. This implies that the users of this system can use their understanding of the document(s) being input into the system to fine-tune it to their particular needs.},
	number = {1},
	urldate = {2021-09-28},
	journal = {Information Systems},
	author = {El-Beltagy, Samhaa R. and Rafea, Ahmed},
	month = mar,
	year = {2009},
	pages = {132--144},
	file = {El-Beltagy et Rafea - 2009 - KP-Miner A keyphrase extraction system for Englis.pdf:/home/gallina-y/Zotero/storage/9MGWQCSA/El-Beltagy et Rafea - 2009 - KP-Miner A keyphrase extraction system for Englis.pdf:application/pdf},
}

@article{sun_sifrank_2020,
	title = {{SIFRank}: {A} {New} {Baseline} for {Unsupervised} {Keyphrase} {Extraction} {Based} on {Pre}-{Trained} {Language} {Model}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{SIFRank}},
	doi = {10.1109/ACCESS.2020.2965087},
	abstract = {In the age of social media, faced with a huge amount of knowledge and information, accurate and effective keyphrase extraction methods are needed to be applied in information retrieval and natural language processing. It is difficult for traditional keyphrase extraction models to contain a large amount of external knowledge information, but with the rise of pre-trained language models, there is a new way to solve this problem. Based on the above background, we propose a new baseline for unsupervised keyphrase extraction based on pre-trained language model called SIFRank. SIFRank combines sentence embedding model SIF and autoregressive pre-trained language model ELMo, and it has the best performance in keyphrase extraction for short documents. We speed up SIFRank while maintaining its accuracy by document segmentation and contextual word embeddings alignment. For long documents, we upgrade SIFRank to SIFRank+ by position-biased weight, greatly improve its performance on long documents. Compared to other baseline models, our model achieves state-of-the-art level on three widely used datasets.},
	journal = {IEEE Access},
	author = {Sun, Yi and Qiu, Hangping and Zheng, Yu and Wang, Zhongwei and Zhang, Chaoran},
	year = {2020},
	note = {Conference Name: IEEE Access},
	pages = {10896--10906},
	file = {IEEE Xplore Abstract Record:/home/gallina-y/Zotero/storage/XAXBIKGG/8954611.html:text/html;IEEE Xplore Full Text PDF:/home/gallina-y/Zotero/storage/EIDCUCVR/Sun et al. - 2020 - SIFRank A New Baseline for Unsupervised Keyphrase.pdf:application/pdf},
}

@inproceedings{rigouts_terryn_termeval_2020,
	address = {Marseille, France},
	title = {{TermEval} 2020: {Shared} {Task} on {Automatic} {Term} {Extraction} {Using} the {Annotated} {Corpora} for {Term} {Extraction} {Research} ({ACTER}) {Dataset}},
	isbn = {979-10-95546-57-3},
	shorttitle = {{TermEval} 2020},
	url = {https://aclanthology.org/2020.computerm-1.12},
	abstract = {The TermEval 2020 shared task provided a platform for researchers to work on automatic term extraction (ATE) with the same dataset: the Annotated Corpora for Term Extraction Research (ACTER). The dataset covers three languages (English, French, and Dutch) and four domains, of which the domain of heart failure was kept as a held-out test set on which final f1-scores were calculated. The aim was to provide a large, transparent, qualitatively annotated, and diverse dataset to the ATE research community, with the goal of promoting comparative research and thus identifying strengths and weaknesses of various state-of-the-art methodologies. The results show a lot of variation between different systems and illustrate how some methodologies reach higher precision or recall, how different systems extract different types of terms, how some are exceptionally good at finding rare terms, or are less impacted by term length. The current contribution offers an overview of the shared task with a comparative evaluation, which complements the individual papers by all participants.},
	language = {English},
	urldate = {2021-10-28},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Computational} {Terminology}},
	publisher = {European Language Resources Association},
	author = {Rigouts Terryn, Ayla and Hoste, Veronique and Drouin, Patrick and Lefever, Els},
	month = may,
	year = {2020},
	pages = {85--94},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/5VXLTXJ3/Rigouts Terryn et al. - 2020 - TermEval 2020 Shared Task on Automatic Term Extra.pdf:application/pdf},
}

@inproceedings{rajpurkar_know_2018,
	address = {Melbourne, Australia},
	title = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},
	shorttitle = {Know {What} {You} {Don}'t {Know}},
	url = {https://aclanthology.org/P18-2124},
	doi = {10.18653/v1/P18-2124},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD achieves only 66\% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.},
	urldate = {2021-10-28},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	month = jul,
	year = {2018},
	pages = {784--789},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/XKGPM2MP/Rajpurkar et al. - 2018 - Know What You Don't Know Unanswerable Questions f.pdf:application/pdf},
}

@inproceedings{lanza_terminology_2019,
	address = {Toulouse, France},
	title = {Terminology systematization for {Cybersecurity} domain in {Italian} {Language}},
	url = {https://aclanthology.org/2019.jeptalnrecital-tia.1},
	abstract = {This paper aims at presenting the first steps to improve the quality of the first draft of an Italian thesaurus for Cybersecurity terminology that has been realized for a specific project activity in collaboration with CybersecurityLab at Informatics and Telematics Institute (IIT) of the National Council of Research (CNR) in Italy. In particular, the paper will focus, first, on the terminological knowledge base built to retrieve the most representative candidate terms of Cybersecurity domain in Italian language, giving examples of the main gold standard repositories that have been used to build this semantic tool. Attention will be then given to the methodology and software employed to configure a system of NLP rules to get the desired semantic results and to proceed with the enhancement of the candidate terms selection which are meant to be inserted in the controlled vocabulary.},
	urldate = {2021-10-28},
	booktitle = {Actes de la {Conférence} sur le {Traitement} {Automatique} des {Langues} {Naturelles} ({TALN}) {PFIA} 2019. {Terminologie} et {Intelligence} {Artificielle} (atelier {TALN}-{RECITAL} {\textbackslash}textbackslash\& {IC})},
	publisher = {ATALA},
	author = {Lanza, Claudia and Daille, Béatrice},
	month = jul,
	year = {2019},
	pages = {7--18},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/CXZHABD4/Lanza et Daille - 2019 - Terminology systematization for Cybersecurity doma.pdf:application/pdf},
}

@article{popel_transforming_2020,
	title = {Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	shorttitle = {Transforming machine translation},
	url = {https://www.nature.com/articles/s41467-020-18073-9},
	doi = {10.1038/s41467-020-18073-9},
	abstract = {The quality of human translation was long thought to be unattainable for computer translation systems. In this study, we present a deep-learning system, CUBBITT, which challenges this view. In a context-aware blind evaluation by human judges, CUBBITT significantly outperformed professional-agency English-to-Czech news translation in preserving text meaning (translation adequacy). While human translation is still rated as more fluent, CUBBITT is shown to be substantially more fluent than previous state-of-the-art systems. Moreover, most participants of a Translation Turing test struggle to distinguish CUBBITT translations from human translations. This work approaches the quality of human translation and even surpasses it in adequacy in certain circumstances.This suggests that deep learning may have the potential to replace humans in applications where conservation of meaning is the primary aim.},
	language = {en},
	number = {1},
	urldate = {2021-10-28},
	journal = {Nature Communications},
	author = {Popel, Martin and Tomkova, Marketa and Tomek, Jakub and Kaiser, Łukasz and Uszkoreit, Jakob and Bojar, Ondřej and Žabokrtský, Zdeněk},
	month = sep,
	year = {2020},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Communication;Computer science;Software
Subject\_term\_id: communication;computer-science;software},
	keywords = {Communication, Computer science, Software},
	pages = {4381},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/P2CMD9LT/Popel et al. - 2020 - Transforming machine translation a deep learning .pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/7EHVVV2F/s41467-020-18073-9.html:text/html},
}

@article{barreaux_indexation_2017,
	title = {Indexation automatique en {SHS} : bilan d’une expérimentation},
	volume = {Volume 54},
	issn = {2428-2111},
	shorttitle = {Indexation automatique en {SHS}},
	url = {https://www.cairn.info/revue-i2d-information-donnees-et-documents-2017-1-page-15.htm},
	abstract = {[projet] La masse d\&\#8217;informations en IST ne cesse de cro\&\#238;tre et, avec elle, la recherche de syst\&\#232;mes d\&\#8217;indexation automatique performants. Dans cet objectif, le projet TermITH a mis au point des m\&\#233;thodes originales d\&\#8217;indexation automatique qu\&\#8217;il confronte \&\#224; des m\&\#233;thodes de r\&\#233;f\&\#233;rence pour des disciplines des sciences humaines. Les r\&\#233;sultats sont encourageants.},
	language = {fr},
	number = {1},
	urldate = {2021-10-28},
	journal = {I2D - Information, donnees documents},
	author = {Barreaux, Sabine and Daille, Béatrice and Jacquey, Évelyne},
	month = apr,
	year = {2017},
	note = {Bibliographie\_available: 0
Cairndomain: www.cairn.info
Cite Par\_available: 0
Publisher: A.D.B.S.},
	pages = {15--17},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/NSI7EU85/Barreaux et al. - 2017 - Indexation automatique en SHS  bilan d’une expéri.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/GIAUU8B4/revue-i2d-information-donnees-et-documents-2017-1-page-15.html:text/html},
}

@inproceedings{subramanian_neural_2018-1,
	address = {Melbourne, Australia},
	title = {Neural {Models} for {Key} {Phrase} {Extraction} and {Question} {Generation}},
	url = {https://aclanthology.org/W18-2609},
	doi = {10.18653/v1/W18-2609},
	abstract = {We propose a two-stage neural model to tackle question generation from documents. First, our model estimates the probability that word sequences in a document are ones that a human would pick when selecting candidate answers by training a neural key-phrase extractor on the answers in a question-answering corpus. Predicted key phrases then act as target answers and condition a sequence-to-sequence question-generation model with a copy mechanism. Empirically, our key-phrase extraction model significantly outperforms an entity-tagging baseline and existing rule-based approaches. We further demonstrate that our question generation system formulates fluent, answerable questions from key phrases. This two-stage system could be used to augment or generate reading comprehension datasets, which may be leveraged to improve machine reading systems or in educational settings.},
	urldate = {2021-10-28},
	booktitle = {Proceedings of the {Workshop} on {Machine} {Reading} for {Question} {Answering}},
	publisher = {Association for Computational Linguistics},
	author = {Subramanian, Sandeep and Wang, Tong and Yuan, Xingdi and Zhang, Saizheng and Trischler, Adam and Bengio, Yoshua},
	month = jul,
	year = {2018},
	pages = {78--88},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/TXIQE66I/Subramanian et al. - 2018 - Neural Models for Key Phrase Extraction and Questi.pdf:application/pdf},
}

@incollection{balkan_automatic_2017,
	title = {Automatic indexing of a variable/question bank collection using {KEA}},
	volume = {10},
	isbn = {978-1-78405-214-0},
	language = {English},
	booktitle = {Systèmes d’organisation des connaissances et humanités numériques},
	publisher = {ISTE Group},
	author = {Balkan, Lorna},
	month = apr,
	year = {2017},
	pages = {129--139},
}

@book{pebayle_systemes_2017,
	title = {Systèmes d’organisation des connaissances et humanités numériques: {Actes} du 10ème colloque {ISKO} {France} 2015},
	isbn = {978-1-78405-214-0},
	shorttitle = {Systèmes d’organisation des connaissances et humanités numériques},
	abstract = {Le numérique modifie considérablement les conditions de production et de diffusion du savoir. Quelles sont les mutations subies par les systèmes d’organisation des connaissances à l’ère des humanités numériques ?Systèmes d’organisation des connaissances et humanités numériques examine quatre grandes thématiques : les approches épistémologiques ou historiques des humanités numériques ; l’organisation de leur contenu ; l’activité du chercheur en sciences humaines et sociales ; ainsi que l’évaluation des systèmes et la prise en compte des usagers.Cet ouvrage collectif réunit les contributions d’une cinquantaine de chercheurs présentées lors du 10ème colloque ISKO France. Il apporte un regard essentiel sur les humanités numériques en pleine expansion et sur les mutations subies par les systèmes d’organisation des connaissances.},
	language = {fr},
	publisher = {ISTE Group},
	author = {Pébayle, Emmanuelle Chevry},
	month = apr,
	year = {2017},
	note = {Google-Books-ID: rRZgDwAAQBAJ},
}

@inproceedings{weber_fine_2018,
	address = {New Orleans, Louisiana},
	title = {The {Fine} {Line} between {Linguistic} {Generalization} and {Failure} in {Seq2Seq}-{Attention} {Models}},
	url = {https://aclanthology.org/W18-1004},
	doi = {10.18653/v1/W18-1004},
	abstract = {Seq2Seq based neural architectures have become the go-to architecture to apply to sequence to sequence language tasks. Despite their excellent performance on these tasks, recent work has noted that these models typically do not fully capture the linguistic structure required to generalize beyond the dense sections of the data distribution (Ettinger et al., 2017), and as such, are likely to fail on examples from the tail end of the distribution (such as inputs that are noisy (Belinkov and Bisk, 2018), or of different length (Bentivogli et al., 2016)). In this paper we look at a model's ability to generalize on a simple symbol rewriting task with a clearly defined structure. We find that the model's ability to generalize this structure beyond the training distribution depends greatly on the chosen random seed, even when performance on the test set remains the same. This finding suggests that model's ability to capture generalizable structure is highly sensitive, and more so, this sensitivity may not be apparent when evaluating the model on standard test sets.},
	urldate = {2021-10-29},
	booktitle = {Proceedings of the {Workshop} on {Generalization} in the {Age} of {Deep} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Weber, Noah and Shekhar, Leena and Balasubramanian, Niranjan},
	month = jun,
	year = {2018},
	pages = {24--27},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/AHBJ6ZV2/Weber et al. - 2018 - The Fine Line between Linguistic Generalization an.pdf:application/pdf},
}

@inproceedings{huang_holes_2019,
	address = {New York, NY, USA},
	series = {{CHIIR} '19},
	title = {Holes in the {Outline}: {Subject}-dependent {Abstract} {Quality} and its {Implications} for {Scientific} {Literature} {Search}},
	isbn = {978-1-4503-6025-8},
	shorttitle = {Holes in the {Outline}},
	url = {https://doi.org/10.1145/3295750.3298953},
	doi = {10.1145/3295750.3298953},
	abstract = {Scientific literature search engines typically index abstracts instead of the full-text of publications. The expectation is that the abstract provides a comprehensive summary of the article, enumerating key points for the reader to assess whether their information needs could be satisfied by reading the full-text. Furthermore, from a practical standpoint, obtaining the full-text is more complicated due to licensing issues, in the case of commercial publishers, and resource limitations of public repositories and pre-print servers. In this article, we use topic modelling to represent content in abstracts and full-text articles. Using Computer Science as a case study, we demonstrate that how well the abstract summarises the full-text is subfield-dependent. Indeed, we show that abstract representativeness has a direct impact on retrieval performance, with poorer abstracts leading to degraded performance. Finally, we present evidence that how well an abstract represents the full-text of an article is not random, but is a consequence of style and writing conventions in different subdisciplines and can be used to infer an "evolutionary" tree of subfields within Computer Science.},
	urldate = {2021-10-29},
	booktitle = {Proceedings of the 2019 {Conference} on {Human} {Information} {Interaction} and {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Chien-yu and Casey, Arlene and Głowacka, Dorota and Medlar, Alan},
	month = mar,
	year = {2019},
	pages = {289--293},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/LJQTBXG7/Huang et al. - 2019 - Holes in the Outline Subject-dependent Abstract Q.pdf:application/pdf},
}

@book{peters_advances_2002,
	address = {Rome, Italy},
	title = {Advances in {Cross}-{Language} {Information} {Retrieval}: {Third} {Workshop} of the {Cross}-{Language} {Evaluation} {Forum}, {CLEF} 2002.},
	isbn = {978-3-540-40830-7},
	shorttitle = {Advances in {Cross}-{Language} {Information} {Retrieval}},
	abstract = {The third campaignof the Cross-LanguageEvaluation Forum (CLEF) for Eu- pean languages was held from January to September 2002. Participation in this campaignshowedaslightriseinthenumberofparticipantswith,37groupsfrom both academia and industry and a steep rise in the number of experiments they submitted for oneor moreof the ?ve o?cialtracks. The campaignculminated in atwo-dayworkshopheldinRome, Italy,19-20September, immediatelyfollowing the Sixth European Conference on Digital Libraries (ECDL 2002), attended by nearly 70 researchersand system developers. The objective of the workshop was to bring together the groups that had participated in CLEF 2002 so that they could report on the results of their experiments. Attendance at the workshop was thus limited to participants in the campaignplus severalinvited guests with recognized expertise in the multilingual information access ?eld. This volume contains thoroughly revised and expanded versions of the preliminary papers presented at the workshop accompanied by a complete run-down and detailed analysis of the results, and it thus provides an exhaustive record of the CLEF 2002 campaign. CLEF2002 wasconducted within the frameworkof a projectof the Infor- tion Society Technologies programme of the European Commission (IST-2000- 31002). The campaign was organized in collaboration with the US National - stitute ofStandardsandTechnology(NIST)andwiththe supportoftheDELOS Network of Excellence for Digital Libraries. The support of NIST and DELOS in the running of the evaluation campaign is gratefully acknowledged. We would also like to thank the other members of the Workshop Steering Committee for their assistance in the coordination of this event.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Peters, Carol},
	month = sep,
	year = {2002},
	note = {Google-Books-ID: \_Zx55edO2uQC},
}

@book{gonzalo_advances_2003,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Advances in {Cross}-{Language} {Information} {Retrieval}: {Third} {Workshop} of the {Cross}-{Language} {Evaluation} {Forum}, {CLEF} 2002, {Rome}, {Italy}, {September} 2002. {Revised} {Papers}},
	isbn = {978-3-540-45237-9},
	shorttitle = {Advances in {Cross}-{Language} {Information} {Retrieval}},
	language = {en},
	number = {2785},
	publisher = {Springer-Verlag Berlin Heidelberg Springer e-books},
	author = {Gonzalo, Julio and Braschler, Martin and Peters, Carol},
	year = {2003},
}

@article{fleiss_measuring_1971,
	title = {Measuring nominal scale agreement among many raters},
	volume = {76},
	issn = {1939-1455},
	doi = {10.1037/h0031619},
	abstract = {Introduced the statistic kappa to measure nominal scale agreement between a fixed pair of raters. Kappa was generalized to the case where each of a sample of 30 patients was rated on a nominal scale by the same number of psychiatrist raters (n = 6), but where the raters rating 1 s were not necessarily the same as those rating another. Large sample standard errors were derived. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {5},
	journal = {Psychological Bulletin},
	author = {Fleiss, Joseph L.},
	year = {1971},
	note = {Place: US
Publisher: American Psychological Association},
	pages = {378--382},
	file = {Snapshot:/home/gallina-y/Zotero/storage/QB5T8B9H/1972-05083-001.html:text/html},
}

@book{centre_national_rameau_guide_2017,
	address = {Paris},
	title = {Guide d'indexation {RAMEAU}},
	url = {https://rameau.bnf.fr/sites/default/files/docs_reference/pdf/guide_rameau_2017.pdf},
	language = {français},
	publisher = {Bibliothèque nationale de France},
	author = {{Centre National RAMEAU}},
	year = {2017},
	file = {Exemplaires\: Guide d'indexation RAMEAU:/home/gallina-y/Zotero/storage/ML2A277T/PPN079727131.html:text/html},
}

@article{menon_les_2007,
	title = {Les langages documentaires},
	volume = {Vol. 44},
	issn = {0012-4508},
	url = {https://www.cairn.info/revue-documentaliste-sciences-de-l-information-2007-1-page-18.htm},
	abstract = {{\textless}titre{\textgreater}Résumé{\textless}/titre{\textgreater}La première contribution s’ouvre par un panorama critique d’un siècle de développement et d’usages des langages documentaires : Bruno Menon en étudie la formation et l’évolution, il en montre les mérites et les limites, il en analyse les pratiques. Dans la deuxième partie de cet article, il examine les taxonomies et les ontologies, nouveaux outils de recherche d’information apparus avec l’explosion des technologies Internet et les mutations induites des pratiques informationnelles. Il suggère enfin un certain nombre de réflexions et de travaux qui permettront aux langages documentaires – dont le concept pourrait être étendu à celui de systèmes d’organisation des connaissances – de s’imposer demain dans l’économie numérique.},
	language = {fr},
	number = {1},
	urldate = {2021-11-05},
	journal = {Documentaliste-Sciences de l'Information},
	author = {Menon, Bruno},
	year = {2007},
	note = {Bibliographie\_available: 1
Cairndomain: www.cairn.info
Cite Par\_available: 1
Publisher: A.D.B.S.},
	pages = {18--28},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/DHSVMV69/Menon - 2007 - Les langages documentaires.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/AR3J66E6/revue-documentaliste-sciences-de-l-information-2007-1-page-18.html:text/html},
}

@article{francis_indexation_2007,
	title = {Indexation collaborative et folksonomies},
	volume = {Vol. 44},
	issn = {0012-4508},
	url = {https://www.cairn.info/revue-documentaliste-sciences-de-l-information-2007-1-page-58.htm},
	abstract = {{\textless}titre{\textgreater}Etude{\textless}/titre{\textgreater} À l’heure où, de multiples façons, les consommateurs d’information deviennent acteurs sur le Web, ils s’approprient peu à peu les techniques et les outils qui étaient auparavant l’apanage des professionnels de l’I-D. Élie Francis et Odile Quesnel présentent ici quatre modes d’indexation et de classification sur la Toile : la classification personnelle, l’indexation par l’auteur, l’indexation par l’utilisateur et la classification globale. Ils précisent ensuite les propriétés, le fonctionnement et les raisons du succès des folksonomies, systèmes d’indexation collaborative libre, décentralisée et spontanée qui peuvent apporter le meilleur (qualité d’information) mais dont on peut redouter le pire (désinformation).},
	language = {fr},
	number = {1},
	urldate = {2021-11-08},
	journal = {Documentaliste-Sciences de l'Information},
	author = {Francis, Élie and Quesnel, Odile},
	year = {2007},
	note = {Bibliographie\_available: 0
Cairndomain: www.cairn.info
Cite Par\_available: 1
Publisher: A.D.B.S.},
	pages = {58--63},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/T43TSAJT/Francis et Quesnel - 2007 - Indexation collaborative et folksonomies.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/F2DCSQKC/revue-documentaliste-sciences-de-l-information-2007-1-page-58.html:text/html},
}

@misc{over_introduction_2001,
	title = {Introduction to {DUC}-2001:  an {Intrinsic} {Evaluation} of {Generic} {News} {Text} {Summarization} {Systems}},
	url = {https://www-nlpir.nist.gov/projects/duc/pubs/2001slides/pauls_slides/},
	urldate = {2021-11-09},
	author = {Over, Paul},
	year = {2001},
	file = {Introduction to DUC-2001\:  an Intrinsic Evaluation of Generic News Text Summarization Systems:/home/gallina-y/Zotero/storage/U2UUN8IK/pauls_slides.html:text/html},
}

@article{gay_semi-automatic_2005,
	title = {Semi-{Automatic} {Indexing} of {Full} {Text} {Biomedical} {Articles}},
	volume = {2005},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1560666/},
	abstract = {The main application of U.S. National Library of Medicine’s Medical
Text Indexer (MTI) is to provide indexing recommendations to the
Library’s indexing staff. The current input to MTI consists of
the titles and abstracts of articles to be indexed. This study reports
on an extension of MTI to the full text of articles appearing in online
medical journals that are indexed for Medline®. Using a collection of 17 journal issues containing 500 articles, we report
on the effectiveness of the contribution of terms by the whole article
and also by each section. We obtain the best results using a model
consisting of the sections Results, Results and Discussion, and Conclusions
together with the article’s title and abstract, the
captions of tables and figures, and sections that have no titles. The
resulting model provides indexing significantly better (7.4\%) than
what is currently achieved using only titles and abstracts.},
	urldate = {2021-11-09},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Gay, Clifford W. and Kayaalp, Mehmet and Aronson, Alan R.},
	year = {2005},
	pmid = {16779044},
	pmcid = {PMC1560666},
	pages = {271--275},
	file = {PubMed Central Full Text PDF:/home/gallina-y/Zotero/storage/7QI5AEZ6/Gay et al. - 2005 - Semi-Automatic Indexing of Full Text Biomedical Ar.pdf:application/pdf},
}

@book{daille_term_2017,
	title = {Term {Variation} in {Specialised} {Corpora}: {Characterisation}, automatic discovery and applications},
	isbn = {978-90-272-6535-7},
	shorttitle = {Term {Variation} in {Specialised} {Corpora}},
	url = {https://www.jbe-platform.com/content/books/9789027265357},
	abstract = {This book addresses term variation which has been a very important topic in terminology, computational terminology and natural language processing for up to twenty years. This book presents the first complete inventory of term variants and the linguistic procedures that lead to their formation. It also takes into account issues raised by multilingual applications and presents ways to detect variants in five different languages: French, English, German, Spanish and Russian.The book provides insights into the following issues: What is a variant? What are the main linguistic mechanisms involved in the transformation of base terms into variants? How can variants be automatically detected in texts? Should variation be taken into account in natural language processing applications?This book is targeted at terminologists and linguists interested in term variation as well as researchers in natural language processing and computer science that must handle term variants in different kinds of applications.},
	language = {en},
	urldate = {2021-11-12},
	publisher = {John Benjamins},
	author = {Daille, Béatrice},
	month = aug,
	year = {2017},
	doi = {10.1075/tlrp.19},
	file = {Snapshot:/home/gallina-y/Zotero/storage/6NP4XZ7R/9789027265357.html:text/html},
}

@inproceedings{cram_terminology_2016,
	address = {Berlin, Germany},
	title = {Terminology {Extraction} with {Term} {Variant} {Detection}},
	url = {https://aclanthology.org/P16-4003},
	doi = {10.18653/v1/P16-4003},
	urldate = {2021-11-19},
	booktitle = {Proceedings of {ACL}-2016 {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Cram, Damien and Daille, Béatrice},
	month = aug,
	year = {2016},
	pages = {13--18},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/EC88DXKZ/Cram et Daille - 2016 - Terminology Extraction with Term Variant Detection.pdf:application/pdf},
}

@techreport{el_hachani_indexation_1997,
	title = {L'indexation automatique},
	language = {fr},
	institution = {Ecole Nationale Supérieure des Sciences de l’Information et des Bibliothèques},
	author = {El Hachani, Mabrouka},
	year = {1997},
	file = {elhachani.pdf:/home/gallina-y/Zotero/storage/695IXSAC/elhachani.pdf:application/pdf},
}

@article{beall_weaknesses_2008,
	title = {The {Weaknesses} of {Full}-{Text} {Searching}},
	volume = {34},
	issn = {0099-1333},
	url = {https://www.sciencedirect.com/science/article/pii/S0099133308001067},
	doi = {10.1016/j.acalib.2008.06.007},
	abstract = {This paper provides a theoretical critique of the deficiencies of full-text searching in academic library databases. Because full-text searching relies on matching words in a search query with words in online resources, it is an inefficient method of finding information in a database. This matching fails to retrieve synonyms, and it also retrieves unwanted homonyms. Numerous other problems also make full-text searching an ineffective information retrieval tool. Academic libraries purchase and subscribe to numerous proprietary databases, many of which rely on full-text searching for access and discovery. An understanding of the weaknesses of full-text searching is needed to evaluate the search and discovery capabilities of academic library databases.},
	language = {en},
	number = {5},
	urldate = {2021-11-25},
	journal = {The Journal of Academic Librarianship},
	author = {Beall, Jeffrey},
	month = sep,
	year = {2008},
	pages = {438--444},
	file = {ScienceDirect Full Text PDF:/home/gallina-y/Zotero/storage/ZUD267CU/Beall - 2008 - The Weaknesses of Full-Text Searching.pdf:application/pdf},
}

@article{peng_deepmesh_2016,
	title = {{DeepMeSH}: deep semantic representation for improving large-scale {MeSH} indexing},
	volume = {32},
	issn = {1367-4803},
	shorttitle = {{DeepMeSH}},
	url = {https://doi.org/10.1093/bioinformatics/btw294},
	doi = {10.1093/bioinformatics/btw294},
	abstract = {Motivation: Medical Subject Headings (MeSH) indexing, which is to assign a set of MeSH main headings to citations, is crucial for many important tasks in biomedical text mining and information retrieval. Large-scale MeSH indexing has two challenging aspects: the citation side and MeSH side. For the citation side, all existing methods, including Medical Text Indexer (MTI) by National Library of Medicine and the state-of-the-art method, MeSHLabeler, deal with text by bag-of-words, which cannot capture semantic and context-dependent information well. Methods: We propose DeepMeSH that incorporates deep semantic information for large-scale MeSH indexing. It addresses the two challenges in both citation and MeSH sides. The citation side challenge is solved by a new deep semantic representation, D2V-TFIDF, which concatenates both sparse and dense semantic representations. The MeSH side challenge is solved by using the ‘learning to rank’ framework of MeSHLabeler, which integrates various types of evidence generated from the new semantic representation. Results: DeepMeSH achieved a Micro F-measure of 0.6323, 2\% higher than 0.6218 of MeSHLabeler and 12\% higher than 0.5637 of MTI, for BioASQ3 challenge data with 6000 citations. Availability and Implementation: The software is available upon request. Contact:zhusf@fudan.edu.cnSupplementary information:Supplementary data are available at Bioinformatics online.},
	number = {12},
	urldate = {2021-11-25},
	journal = {Bioinformatics},
	author = {Peng, Shengwen and You, Ronghui and Wang, Hongning and Zhai, Chengxiang and Mamitsuka, Hiroshi and Zhu, Shanfeng},
	month = jun,
	year = {2016},
	pages = {i70--i79},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/P6TLRV34/Peng et al. - 2016 - DeepMeSH deep semantic representation for improvi.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/B8ZFHDJC/2289069.html:text/html},
}

@inproceedings{khemiri_manual_2020,
	address = {Tunis, Tunisia},
	series = {{ISKO}-{Maghreb} {Proceedings}},
	title = {From manual indexing to automatic indexing in the era of {Big} {Data} and {Open} {Data}: a state of the art},
	volume = {2},
	shorttitle = {From manual indexing to automatic indexing in the era of {Big} {Data} and {Open} {Data}},
	url = {https://hal.archives-ouvertes.fr/hal-02933709},
	abstract = {In the era of Big Data and Open Data, a massive and heterogeneous collections of documents (from text to multimedia) are created, managed and stored electronically. to make these documents more usable, a manual and/or automatic indexing process allows to create a representation of documents by a set of metadata, descriptors and social tags. These representations then make it easier to find information in a massive and scalable collection of documents from different sources (social networks, open data, …) to respond to user information needs (user requests). Numerous research studies have been carried out to propose indexing approaches depending on the type of indexed documents. Also, the evolution of indexing Methods, documents representation, electronic content, Big Data and Open Data. This paper presents a state of the art of approaches and methodologies ranging from manual and automatic indexing to algorithmic methods in the era of Big Data and Open Data.},
	urldate = {2021-11-25},
	booktitle = {Multi-{Conference} {OCTA}'2019 on: {Organization} of {Knowledge} and {Advanced} {Technologies}},
	publisher = {Université de Tunis and ISKO-Maghreb Chapter},
	author = {Khemiri, Nabil and Sidhom, Sahbi},
	month = feb,
	year = {2020},
	note = {Issue: 1},
	pages = {171--175},
	file = {HAL PDF Full Text:/home/gallina-y/Zotero/storage/BWTZJKZZ/Khemiri et Sidhom - 2020 - From manual indexing to automatic indexing in the .pdf:application/pdf},
}

@inproceedings{salabert_de_vers_2020,
	address = {Nancy, France},
	title = {Vers un corpus optimal pour la fouille de textes : stratégie de constitution de corpus spécialisés à partir d'{ISTEX}},
	shorttitle = {Vers un corpus optimal pour la fouille de textes},
	url = {https://hal.archives-ouvertes.fr/hal-02768520},
	abstract = {Préalable indispensable à de nombreuses activités de TAL et de fouille de textes, l’élaboration d’un corpus peut nécessiter plusieurs phases de traitement pour améliorer sa qualité et ainsi obtenir les meilleurs résultats d’analyse automatique. Les post-traitements appliqués à un tel corpus, notamment pour garantir la pertinence de son contenu et l’homogénéité de son format, pourront s’avérer d’autant plus coûteux et fastidieux que la construction du corpus de travail aura été imprécise. Cette démonstration se proposera de tirer parti de la plateforme ISTEX et de ses services associés pour constituer, au travers d’un cycle itératif, un corpus homogène de publications scientifiquement pertinentes pour une utilisation simplifiée par des outils de fouille.},
	urldate = {2021-11-26},
	booktitle = {6e conférence conjointe {Journées} d'Études sur la {Parole} ({JEP}, 33e édition), {Traitement} {Automatique} des {Langues} {Naturelles} ({TALN}, 27e édition), {Rencontre} des Étudiants {Chercheurs} en {Informatique} pour le {Traitement} {Automatique} des {Langues} ({RÉCITAL}, 22e édition)},
	publisher = {ATALA},
	author = {Salabert de, Camille and Barreaux, Sabine},
	editor = {Benzitoun, Christophe and Braud, Chloé and Huber, Laurine and Langlois, David and Ouni, Slim and Pogodalla, Sylvain and Schneider, Stéphane},
	year = {2020},
	pages = {66--69},
	file = {HAL PDF Full Text:/home/gallina-y/Zotero/storage/AAGHFIIM/de Salabert et Barreaux - 2020 - Vers un corpus optimal pour la fouille de textes .pdf:application/pdf},
}

@article{kosovac_use_2002,
	title = {Use of {Keyphrase} {Extraction} {Software} for {Creation} of an {AEC}/{FM} {Thesaurus}},
	volume = {5},
	url = {http://itcon.org/paper/2000/2},
	abstract = {The paper describes a method used to collect terms needed for the development of a thesaurus in the roofing domain. This work is part of a larger effort to investigate the potential of thesauri as an aid in product modeling and as a tool for information management in model-based systems. Extractor, a software module that extracts keyphrases from documents, was used for collecting candidate thesaurus terms from Internet sources. The principal advantage of the Internet as a source of candidate terms is that it reflects the language that is actually used in communications concerning buildings and that it covers the widest range of different views on the domain. The advantage of using Extractor or similar software is that it allows processing huge text corpora available on the Internet while eliminating irrelevant terms. The methodology used was found to be highly useful, although it was not sufficient by itself for constructing a thesaurus for the architecture, engineering, construction and facilities management industries, as considerable human intervention was required. Some possibilities for customizing the software and for partially automating a thesaurus construction process are suggested.},
	language = {en},
	number = {2},
	urldate = {2021-11-26},
	journal = {Journal of Information Technology in Construction (ITcon)},
	author = {Kosovac, Branka and Vanier, Dana J. and Froese, Thomas M.},
	month = jul,
	year = {2002},
	pages = {25--36},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/3KS7R3RT/Kosovac et al. - 2002 - Use of Keyphrase Extraction Software for Creation .pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/FH4S92KD/2.html:text/html},
}

@inproceedings{qazvinian_citation_2010,
	address = {Beijing, China},
	title = {Citation {Summarization} {Through} {Keyphrase} {Extraction}},
	url = {https://aclanthology.org/C10-1101},
	urldate = {2021-11-26},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Computational} {Linguistics} ({Coling} 2010)},
	publisher = {Coling 2010 Organizing Committee},
	author = {Qazvinian, Vahed and Radev, Dragomir R. and Özgür, Arzucan},
	month = aug,
	year = {2010},
	pages = {895--903},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/IPN63F6X/Qazvinian et al. - 2010 - Citation Summarization Through Keyphrase Extractio.pdf:application/pdf},
}

@article{zhang_automatic_2008,
	title = {Automatic {Keyword} {Extraction} from {Documents} {Using} {Conditional} {Random} {Fields}},
	abstract = {In process of manual assignment keyword to a document, the content of the document will be analyzed and comprehended firstly. Keywords which can express the meaning of document are then determined. Content analysis is the process that most of the units of a document, such as the title, abstract, full text, references and so on, be analyzed and comprehended. Usually, we can extract 3{\textasciitilde}5 keywords from a document in process of manual assignment keyword.},
	language = {en},
	author = {Zhang, Chengzhi and Wang, Huilin and Liu, Yao and Wu, Dan and Liao, Yi and Wang, Bo},
	year = {2008},
	pages = {11},
	file = {Zhang et al. - 2008 - Automatic Keyword Extraction from Documents Using .pdf:/home/gallina-y/Zotero/storage/KVHSSHBP/Zhang et al. - 2008 - Automatic Keyword Extraction from Documents Using .pdf:application/pdf},
}

@article{willis_random_2013,
	title = {A random walk on an ontology: {Using} thesaurus structure for automatic subject indexing},
	volume = {64},
	issn = {1532-2890},
	shorttitle = {A random walk on an ontology},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.22853},
	doi = {10.1002/asi.22853},
	abstract = {Relationships between terms and features are an essential component of thesauri, ontologies, and a range of controlled vocabularies. In this article, we describe ways to identify important concepts in documents using the relationships in a thesaurus or other vocabulary structures. We introduce a methodology for the analysis and modeling of the indexing process based on a weighted random walk algorithm. The primary goal of this research is the analysis of the contribution of thesaurus structure to the indexing process. The resulting models are evaluated in the context of automatic subject indexing using four collections of documents pre-indexed with 4 different thesauri (AGROVOC [UN Food and Agriculture Organization], high-energy physics taxonomy [HEP], National Agricultural Library Thesaurus [NALT], and medical subject headings [MeSH]). We also introduce a thesaurus-centric matching algorithm intended to improve the quality of candidate concepts. In all cases, the weighted random walk improves automatic indexing performance over matching alone with an increase in average precision (AP) of 9\% for HEP, 11\% for MeSH, 35\% for NALT, and 37\% for AGROVOC. The results of the analysis support our hypothesis that subject indexing is in part a browsing process, and that using the vocabulary and its structure in a thesaurus contributes to the indexing process. The amount that the vocabulary structure contributes was found to differ among the 4 thesauri, possibly due to the vocabulary used in the corresponding thesauri and the structural relationships between the terms. Each of the thesauri and the manual indexing associated with it is characterized using the methods developed here.},
	language = {en},
	number = {7},
	urldate = {2021-12-17},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Willis, Craig and Losee, Robert M.},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.22853},
	pages = {1330--1344},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/6G829PXK/Willis et Losee - 2013 - A random walk on an ontology Using thesaurus stru.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/J93I7TUI/asi.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2021-12-28},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {download.pdf:/home/gallina-y/Zotero/storage/J7UK89P5/download.pdf:application/pdf},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2021-12-23},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:/home/gallina-y/Zotero/storage/KVUAIJJD/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@article{bojanowski_enriching_2017,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	volume = {5},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/43387},
	doi = {10.1162/tacl_a_00051},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	language = {en},
	urldate = {2021-12-23},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = dec,
	year = {2017},
	pages = {135--146},
	file = {Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:/home/gallina-y/Zotero/storage/CEFDMAVT/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:application/pdf},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	urldate = {2021-12-23},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg S. and Dean, Jeffrey},
	year = {2013},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/27RMLEIA/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf},
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992696},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	language = {en},
	number = {3},
	urldate = {2022-01-04},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	pages = {229--256},
	file = {Springer Full Text PDF:/home/gallina-y/Zotero/storage/KHVDYLMS/Williams - 1992 - Simple statistical gradient-following algorithms f.pdf:application/pdf},
}

@inproceedings{cuxac_archives_2017,
	address = {Grenoble, France},
	title = {Archives numériques et fouille de textes : le projet {ISTEX}},
	url = {https://www.egc.asso.fr/wp-content/uploads/egc2017_atelier_TextMine.pdf},
	language = {fr},
	booktitle = {Actes de l'{Atelier} sur la {Fouille} de {Textes} {TextMine} organisé conjointement a la conference {EGC} {Extraction} et {Gestion} des {Connaissances}},
	author = {Cuxac, Pascal and Thouvenin, Nicolas},
	month = jan,
	year = {2017},
	pages = {10},
	file = {Cuxac et Thouvenin - Archives numériques et fouille de textes  le proj.pdf:/home/gallina-y/Zotero/storage/DRTPLB4J/Cuxac et Thouvenin - Archives numériques et fouille de textes  le proj.pdf:application/pdf},
}

@article{xu_show_2016,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
	shorttitle = {Show, {Attend} and {Tell}},
	url = {http://arxiv.org/abs/1502.03044},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to ﬁx its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	language = {en},
	urldate = {2022-01-08},
	journal = {arXiv:1502.03044 [cs]},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv: 1502.03044},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Xu et al. - 2016 - Show, Attend and Tell Neural Image Caption Genera.pdf:/home/gallina-y/Zotero/storage/RMLG8V8P/Xu et al. - 2016 - Show, Attend and Tell Neural Image Caption Genera.pdf:application/pdf},
}

@inproceedings{rello_keyword_2014,
	address = {Gothenburg, Sweden},
	title = {Keyword {Highlighting} {Improves} {Comprehension} for {People} with {Dyslexia}},
	url = {http://aclweb.org/anthology/W14-1204},
	doi = {10.3115/v1/W14-1204},
	abstract = {The use of certain font types and sizes improve the reading performance of people with dyslexia. However, the impact of combining such features with the semantics of the text has not yet been studied. In this eye-tracking study with 62 people (31 with dyslexia), we explore whether highlighting the main ideas of the text in boldface has an impact on readability and comprehensibility. We found that highlighting keywords improved the comprehension of participants with dyslexia. To the best of our knowledge, this is the ﬁrst result of this kind for people with dyslexia.},
	language = {en},
	urldate = {2022-01-13},
	booktitle = {Proceedings of the 3rd {Workshop} on {Predicting} and {Improving} {Text} {Readability} for {Target} {Reader} {Populations} ({PITR})},
	publisher = {Association for Computational Linguistics},
	author = {Rello, Luz and Saggion, Horacio and Baeza-Yates, Ricardo},
	year = {2014},
	pages = {30--37},
	file = {Rello et al. - 2014 - Keyword Highlighting Improves Comprehension for Pe.pdf:/home/gallina-y/Zotero/storage/VBLHHFAP/Rello et al. - 2014 - Keyword Highlighting Improves Comprehension for Pe.pdf:application/pdf},
}

@article{schluter_critical_nodate,
	title = {A critical survey on measuring success in rank-based keyword assignment to documents},
	abstract = {Evaluation approaches for unsupervised rank-based keyword assignment are nearly as numerous as are the existing systems. The proliﬁc production of each newly used metric (or metric twist) seems to stem from general dissatisfaction with the previous one and the source of that dissatisfaction has not previously been discussed in the literature. The difﬁculty may stem from a poor speciﬁcation of the keyword assignment task in view of the rank-based approach. With a more complete speciﬁcation of this task, we aim to show why the previous evaluation metrics fail to satisfy researchers’ goals to distinguish and detect good rank-based keyword assignment systems. We put forward a characterisation of an ideal evaluation metric, and discuss the consistency of the evaluation metrics with this ideal, ﬁnding that the average standard normalised cumulative gain metric is most consistent with this ideal.},
	language = {en},
	author = {Schluter, Natalie},
	pages = {6},
	file = {Schluter - A critical survey on measuring success in rank-bas.pdf:/home/gallina-y/Zotero/storage/QENDPLUL/Schluter - A critical survey on measuring success in rank-bas.pdf:application/pdf},
}

@inproceedings{ahmad_select_2021,
	address = {Online},
	title = {Select, {Extract} and {Generate}: {Neural} {Keyphrase} {Generation} with {Layer}-wise {Coverage} {Attention}},
	shorttitle = {Select, {Extract} and {Generate}},
	url = {https://aclanthology.org/2021.acl-long.111},
	doi = {10.18653/v1/2021.acl-long.111},
	abstract = {Natural language processing techniques have demonstrated promising results in keyphrase generation. However, one of the major challenges in neural keyphrase generation is processing long documents using deep neural networks. Generally, documents are truncated before given as inputs to neural networks. Consequently, the models may miss essential points conveyed in the target document. To overcome this limitation, we propose SEG-Net, a neural keyphrase generation model that is composed of two major components, (1) a selector that selects the salient sentences in a document and (2) an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses Transformer, a self-attentive architecture, as the basic building block with a novel layer-wise coverage attention to summarize most of the points discussed in the document. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin.},
	urldate = {2022-01-13},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ahmad, Wasi and Bai, Xiao and Lee, Soomin and Chang, Kai-Wei},
	month = aug,
	year = {2021},
	keywords = {generation},
	pages = {1389--1404},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/FJ8G88NA/Ahmad et al. - 2021 - Select, Extract and Generate Neural Keyphrase Gen.pdf:application/pdf},
}

@inproceedings{kim_structure-augmented_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Structure-{Augmented} {Keyphrase} {Generation}},
	url = {https://aclanthology.org/2021.emnlp-main.209},
	doi = {10.18653/v1/2021.emnlp-main.209},
	abstract = {This paper studies the keyphrase generation (KG) task for scenarios where structure plays an important role. For example, a scientific publication consists of a short title and a long body, where the title can be used for de-emphasizing unimportant details in the body. Similarly, for short social media posts (, tweets), scarce context can be augmented from titles, though often missing. Our contribution is generating/augmenting structure then injecting these information in the encoding, using existing keyphrases of other documents, complementing missing/incomplete titles. We propose novel structure-augmented document encoding approaches that consist of the following two phases: The first phase, generating structure, extends the given document with related but absent keyphrases, augmenting missing context. The second phase, encoding structure, builds a graph of keyphrases and the given document to obtain the structure-aware representation of the augmented text. Our empirical results validate that our proposed structure augmentation and augmentation-aware encoding/decoding can improve KG for both scenarios, outperforming the state-of-the-art.},
	urldate = {2022-01-13},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Jihyuk and Jeong, Myeongho and Choi, Seungtaek and Hwang, Seung-won},
	month = nov,
	year = {2021},
	keywords = {generation},
	pages = {2657--2667},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/CINSFI5A/Kim et al. - 2021 - Structure-Augmented Keyphrase Generation.pdf:application/pdf},
}

@inproceedings{santosh_hicova_2021,
	address = {Virtual Event Queensland Australia},
	title = {{HiCoVA}: {Hierarchical} {Conditional} {Variational} {Autoencoder} for {Keyphrase} {Generation}},
	isbn = {978-1-4503-8446-9},
	shorttitle = {{HiCoVA}},
	url = {https://dl.acm.org/doi/10.1145/3459637.3482119},
	doi = {10.1145/3459637.3482119},
	abstract = {The task of keyphrase generation, unlike extraction, aims to generate the phrases which succinctly capture the key information of the source text, that are even absent in the document (i.e., do not match any contiguous sub-sequence of source text). Despite the significant progress achieved by sequence-to-sequence (seq2seq) models in modelling such a high entropy task, they are limited by their deterministic modelling capability which limits the generation of a diverse set of keyphrases. To address the above limitation, in this paper, we propose to incorporate Conditional Variational Autoencoder (CoVA) into seq2seq models for its ability to represent a set of keyphrases as a probabilistic distribution, which improves the diversity of the generated keyphrases. We model the probabilistic distribution using a hierarchical latent structure where a global latent variable tries to model the diversity among the keyphrases and local latent variables control the generation of each keyphrase to make them coherent. Experimental results on four benchmark datasets of research papers demonstrate the effectiveness of our proposed approach in achieving a large improvement in diversity along with modest gains in quality with respect to previous models.},
	language = {en},
	urldate = {2022-01-13},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Santosh, Tokala Yaswanth Sri Sai and Varimalla, Nikhil Reddy and Vallabhajosyula, Anoop and Sanyal, Debarshi Kumar and Das, Partha Pratim},
	month = oct,
	year = {2021},
	keywords = {generation},
	pages = {3448--3452},
	file = {Santosh et al. - 2021 - HiCoVA Hierarchical Conditional Variational Autoe.pdf:/home/gallina-y/Zotero/storage/5XKW7HT5/Santosh et al. - 2021 - HiCoVA Hierarchical Conditional Variational Autoe.pdf:application/pdf},
}

@inproceedings{ye_heterogeneous_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Heterogeneous {Graph} {Neural} {Networks} for {Keyphrase} {Generation}},
	url = {https://aclanthology.org/2021.emnlp-main.213},
	doi = {10.18653/v1/2021.emnlp-main.213},
	language = {en},
	urldate = {2022-01-13},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ye, Jiacheng and Cai, Ruijian and Gui, Tao and Zhang, Qi},
	year = {2021},
	keywords = {generation},
	pages = {2705--2715},
	file = {Ye et al. - 2021 - Heterogeneous Graph Neural Networks for Keyphrase .pdf:/home/gallina-y/Zotero/storage/PBK9Y9C6/Ye et al. - 2021 - Heterogeneous Graph Neural Networks for Keyphrase .pdf:application/pdf},
}

@inproceedings{knittel_elske_2021,
	address = {Limerick Ireland},
	title = {{ELSKE}: efficient large-scale keyphrase extraction},
	isbn = {978-1-4503-8596-1},
	shorttitle = {{ELSKE}},
	url = {https://dl.acm.org/doi/10.1145/3469096.3474930},
	doi = {10.1145/3469096.3474930},
	abstract = {Keyphrase extraction methods can provide insights into large collections of documents such as social media posts. Existing methods, however, are less suited for the real-time analysis of streaming data, because they are computationally too expensive or require restrictive constraints regarding the structure of keyphrases. We propose an efficient approach to extract keyphrases from large document collections and show that the method also performs competitively on individual documents.},
	language = {en},
	urldate = {2022-01-13},
	booktitle = {Proceedings of the 21st {ACM} {Symposium} on {Document} {Engineering}},
	publisher = {ACM},
	author = {Knittel, Johannes and Koch, Steffen and Ertl, Thomas},
	month = aug,
	year = {2021},
	pages = {1--4},
	file = {Knittel et al. - 2021 - ELSKE efficient large-scale keyphrase extraction.pdf:/home/gallina-y/Zotero/storage/AQAWJ2JD/Knittel et al. - 2021 - ELSKE efficient large-scale keyphrase extraction.pdf:application/pdf},
}

@inproceedings{song_importance_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Importance {Estimation} from {Multiple} {Perspectives} for {Keyphrase} {Extraction}},
	url = {https://aclanthology.org/2021.emnlp-main.215},
	doi = {10.18653/v1/2021.emnlp-main.215},
	language = {en},
	urldate = {2022-01-13},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Song, Mingyang and Jing, Liping and Xiao, Lin},
	year = {2021},
	keywords = {extraction},
	pages = {2726--2736},
	file = {Song et al. - 2021 - Importance Estimation from Multiple Perspectives f.pdf:/home/gallina-y/Zotero/storage/4EI4WCVJ/Song et al. - 2021 - Importance Estimation from Multiple Perspectives f.pdf:application/pdf},
}

@article{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
	pages = {24},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/home/gallina-y/Zotero/storage/3KJEFMEI/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@inproceedings{wang_glue_2018,
	address = {Brussels, Belgium},
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://aclweb.org/anthology/W18-5446},
	doi = {10.18653/v1/W18-5446},
	language = {en},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	year = {2018},
	pages = {353--355},
	file = {Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:/home/gallina-y/Zotero/storage/UFCGIAAC/Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:application/pdf},
}

@inproceedings{boudin_redefining_2021,
	address = {Online},
	title = {Redefining {Absent} {Keyphrases} and their {Effect} on {Retrieval} {Effectiveness}},
	url = {https://aclanthology.org/2021.naacl-main.330},
	doi = {10.18653/v1/2021.naacl-main.330},
	language = {en},
	urldate = {2022-01-19},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Boudin, Florian and Gallina, Ygor},
	year = {2021},
	keywords = {meta},
	pages = {4185--4193},
	file = {Boudin et Gallina - 2021 - Redefining Absent Keyphrases and their Effect on R.pdf:/home/gallina-y/Zotero/storage/BX3V3C4X/Boudin et Gallina - 2021 - Redefining Absent Keyphrases and their Effect on R.pdf:application/pdf},
}

@article{martinc_tnt-kid_2021,
	title = {{TNT}-{KID}: {Transformer}-based neural tagger for keyword identification},
	issn = {1351-3249, 1469-8110},
	shorttitle = {{TNT}-{KID}},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/tntkid-transformerbased-neural-tagger-for-keyword-identification/A41C8B12C1F3F4F02BF839FCAFA1A695},
	doi = {10.1017/S1351324921000127},
	abstract = {With growing amounts of available textual data, development of algorithms capable of automatic analysis, categorization, and summarization of these data has become a necessity. In this research, we present a novel algorithm for keyword identification, that is, an extraction of one or multiword phrases representing key aspects of a given document, called Transformer-Based Neural Tagger for Keyword IDentification (TNT-KID). By adapting the transformer architecture for a specific task at hand and leveraging language model pretraining on a domain-specific corpus, the model is capable of overcoming deficiencies of both supervised and unsupervised state-of-the-art approaches to keyword extraction by offering competitive and robust performance on a variety of different datasets while requiring only a fraction of manually labeled data required by the best-performing systems. This study also offers thorough error analysis with valuable insights into the inner workings of the model and an ablation study measuring the influence of specific components of the keyword identification workflow on the overall performance.},
	language = {en},
	urldate = {2022-01-19},
	journal = {Natural Language Engineering},
	author = {Martinc, Matej and Škrlj, Blaž and Pollak, Senja},
	month = jun,
	year = {2021},
	note = {Publisher: Cambridge University Press},
	keywords = {extraction},
	pages = {1--40},
	file = {Full Text PDF:/home/gallina-y/Zotero/storage/M8IUX848/Martinc et al. - 2021 - TNT-KID Transformer-based neural tagger for keywo.pdf:application/pdf;Snapshot:/home/gallina-y/Zotero/storage/WK2KKBSB/A41C8B12C1F3F4F02BF839FCAFA1A695.html:text/html},
}

@inproceedings{meng_empirical_2021,
	address = {Online},
	title = {An {Empirical} {Study} on {Neural} {Keyphrase} {Generation}},
	url = {https://aclanthology.org/2021.naacl-main.396},
	doi = {10.18653/v1/2021.naacl-main.396},
	language = {en},
	urldate = {2022-01-19},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Meng, Rui and Yuan, Xingdi and Wang, Tong and Zhao, Sanqiang and Trischler, Adam and He, Daqing},
	year = {2021},
	keywords = {meta},
	pages = {4985--5007},
	file = {Meng et al. - 2021 - An Empirical Study on Neural Keyphrase Generation.pdf:/home/gallina-y/Zotero/storage/6RNT4DS8/Meng et al. - 2021 - An Empirical Study on Neural Keyphrase Generation.pdf:application/pdf},
}
@inproceedings{boudin_acm-cr_2021,
	title = {{ACM}-{CR}: {A} {Manually} {Annotated} {Test} {Collection} for {Citation} {Recommendation}},
	shorttitle = {{ACM}-{CR}},
	doi = {10.1109/JCDL52503.2021.00035},
	booktitle = {2021 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL})},
	author = {Boudin, Florian},
	month = sep,
	year = {2021},
	pages = {280--281},
}

@inproceedings{mothe_automatic_2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {Automatic keyphrase extraction using graph-based methods},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167392},
	doi = {10.1145/3167132.3167392},
	urldate = {2022-03-17},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Mothe, Josiane and Ramiandrisoa, Faneva and Rasolomanana, Michael},
	month = apr,
	year = {2018},
	pages = {728--730},
}
